{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Tutorial 1: Preprocessing Conversational Transcripts\n",
    "\n",
    "This tutorial demonstrates how to use the ALIGN package to preprocess conversational data for linguistic alignment analysis.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to prepare raw conversational transcripts for alignment analysis\n",
    "- Using different POS taggers (NLTK, spaCy, Stanford)\n",
    "- Understanding the preprocessing output format\n",
    "- Validating that outputs are ready for Phase 2 (alignment analysis)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You should have already:\n",
    "1. Cloned this repository\n",
    "2. Installed the package: `pip install -e .`\n",
    "3. Sample data is available in `src/align_test/data/CHILDES/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Import and Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Import the preprocessing function\n",
    "from align_test.prepare_transcripts import prepare_transcripts\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "# Input: Sample CHILDES data (included in the package)\n",
    "INPUT_DIR = '../src/align_test/data/CHILDES/'\n",
    "\n",
    "# Output: Where to save preprocessed files\n",
    "OUTPUT_DIR_BASIC = './tutorial_output/preprocessed_nltk'\n",
    "OUTPUT_DIR_SPACY = './tutorial_output/preprocessed_spacy'\n",
    "OUTPUT_DIR_STANFORD = './tutorial_output/preprocessed_stanford'\n",
    "\n",
    "# Create output directories\n",
    "for dir_path in [OUTPUT_DIR_BASIC, OUTPUT_DIR_SPACY, OUTPUT_DIR_STANFORD]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Output directories created ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Inspect Sample Input Data\n",
    "\n",
    "Let's look at what raw conversation data looks like before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available files\n",
    "files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.txt')]\n",
    "print(f\"Found {len(files)} conversation files\\n\")\n",
    "\n",
    "# Load and display a sample file\n",
    "sample_file = os.path.join(INPUT_DIR, files[0])\n",
    "df_sample = pd.read_csv(sample_file, sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(f\"Sample file: {files[0]}\")\n",
    "print(f\"Columns: {df_sample.columns.tolist()}\")\n",
    "print(f\"Rows: {len(df_sample)}\\n\")\n",
    "print(\"First 5 utterances:\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess1",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Basic Preprocessing (NLTK Only)\n",
    "\n",
    "This is the **fastest option** and requires no additional setup. NLTK tags are automatically downloaded if needed.\n",
    "\n",
    "### What This Does:\n",
    "1. Cleans text (removes non-letters, fillers like \"um\", \"uh\")\n",
    "2. Merges adjacent turns by the same speaker\n",
    "3. Tokenizes and lemmatizes words\n",
    "4. Adds POS (Part-of-Speech) tags using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess_nltk",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting NLTK-only preprocessing...\\n\")\n",
    "\n",
    "results_nltk = prepare_transcripts(\n",
    "    input_files=INPUT_DIR,\n",
    "    output_file_directory=OUTPUT_DIR_BASIC,\n",
    "    run_spell_check=False,          # Disable for speed (optional)\n",
    "    minwords=2,                      # Minimum words per utterance\n",
    "    add_additional_tags=False        # NLTK only\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete!\")\n",
    "print(f\"Total utterances processed: {len(results_nltk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "examine1",
   "metadata": {},
   "source": [
    "### Examine Preprocessed Output\n",
    "\n",
    "Let's see what the preprocessed data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examine_nltk",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output columns:\")\n",
    "for i, col in enumerate(results_nltk.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(f\"\\nSample row:\")\n",
    "sample = results_nltk.iloc[0]\n",
    "print(f\"Participant: {sample['participant']}\")\n",
    "print(f\"Content: {sample['content']}\")\n",
    "print(f\"Tokens: {sample['token']}\")\n",
    "print(f\"Lemmas: {sample['lemma']}\")\n",
    "print(f\"Tagged: {sample['tagged_token'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate",
   "metadata": {},
   "source": [
    "### Validate Output Format\n",
    "\n",
    "The preprocessing creates strings that can be parsed back to Python objects. This is important for Phase 2 (alignment analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate_format",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parsing\n",
    "sample_row = results_nltk.iloc[0]\n",
    "\n",
    "# Parse token string back to list\n",
    "tokens = ast.literal_eval(sample_row['token'])\n",
    "print(f\"Token type after parsing: {type(tokens)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Parse tagged tokens\n",
    "tagged = ast.literal_eval(sample_row['tagged_token'])\n",
    "print(f\"\\nTagged token type: {type(tagged)}\")\n",
    "print(f\"Tagged tokens (first 3): {tagged[:3]}\")\n",
    "print(f\"\\n‚úì Format validation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spacy_intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Preprocessing with spaCy (Optional)\n",
    "\n",
    "spaCy provides **100-200x faster** POS tagging than Stanford with minimal accuracy differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_spacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if spaCy is available\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"‚úì spaCy is installed\")\n",
    "    spacy_available = True\n",
    "except ImportError:\n",
    "    print(\"‚úó spaCy not installed\")\n",
    "    print(\"\\nTo install: pip install spacy\")\n",
    "    print(\"Then run: python -m spacy download en_core_web_sm\")\n",
    "    spacy_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess_spacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacy_available:\n",
    "    print(\"Starting preprocessing with spaCy...\\n\")\n",
    "    \n",
    "    results_spacy = prepare_transcripts(\n",
    "        input_files=INPUT_DIR,\n",
    "        output_file_directory=OUTPUT_DIR_SPACY,\n",
    "        run_spell_check=False,\n",
    "        minwords=2,\n",
    "        add_additional_tags=True,       # Add spaCy tags\n",
    "        tagger_type='spacy'             # Specify spaCy\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì spaCy preprocessing complete!\")\n",
    "    \n",
    "    # Show additional columns\n",
    "    spacy_cols = [c for c in results_spacy.columns if c not in results_nltk.columns]\n",
    "    print(f\"\\nAdditional columns with spaCy:\")\n",
    "    for col in spacy_cols:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"‚äò Skipping spaCy preprocessing (not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stanford_intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Preprocessing with Stanford (Optional)\n",
    "\n",
    "Stanford CoreNLP provides the **highest accuracy** but is **~100x slower** than spaCy.\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "#### 1. Install Java\n",
    "```bash\n",
    "# macOS\n",
    "brew install openjdk\n",
    "\n",
    "# Linux\n",
    "sudo apt-get install default-jdk\n",
    "\n",
    "# Windows\n",
    "# Download from: https://www.java.com/en/download/\n",
    "```\n",
    "\n",
    "#### 2. Download Stanford POS Tagger\n",
    "- Download from: https://nlp.stanford.edu/software/tagger.shtml#Download\n",
    "- Extract to a known location (e.g., `~/stanford-postagger/`)\n",
    "- Update the paths in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Check Java\n",
    "try:\n",
    "    result = subprocess.run(['java', '-version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        version = result.stderr.split('\\n')[0]\n",
    "        print(f\"‚úì Java installed: {version}\")\n",
    "        java_available = True\n",
    "    else:\n",
    "        print(\"‚úó Java not working properly\")\n",
    "        java_available = False\n",
    "except:\n",
    "    print(\"‚úó Java not found\")\n",
    "    print(\"\\nPlease install Java first (see instructions above)\")\n",
    "    java_available = False\n",
    "\n",
    "# Configure STANFORD_PATH (UPDATE THESE FOR YOUR SYSTEM)\n",
    "if java_available:\n",
    "    STANFORD_PATH = os.path.expanduser('~/stanford-postagger-full-2020-11-17') # Update this path\n",
    "    STANFORD_MODEL = 'models/english-left3words-distsim.tagger'\n",
    "    \n",
    "    # Check if Stanford tagger exists\n",
    "    jar_path = os.path.join(STANFORD_PATH, 'stanford-postagger.jar')\n",
    "    model_path = os.path.join(STANFORD_PATH, STANFORD_MODEL)\n",
    "    \n",
    "    if os.path.exists(jar_path) and os.path.exists(model_path):\n",
    "        print(f\"‚úì Stanford tagger found at: {STANFORD_PATH}\")\n",
    "        stanford_available = True\n",
    "    else:\n",
    "        print(f\"‚úó Stanford tagger not found at: {STANFORD_PATH}\")\n",
    "        print(\"\\nPlease:\")\n",
    "        print(\"  1. Download from: https://nlp.stanford.edu/software/tagger.shtml#Download\")\n",
    "        print(\"  2. Extract and update STANFORD_PATH above\")\n",
    "        stanford_available = False\n",
    "else:\n",
    "    stanford_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess_stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stanford_available:\n",
    "    print(\"Starting preprocessing with Stanford...\")\n",
    "    print(\"‚ö†Ô∏è  This may take several minutes...\\n\")\n",
    "    \n",
    "    results_stanford = prepare_transcripts(\n",
    "        input_files=INPUT_DIR,\n",
    "        output_file_directory=OUTPUT_DIR_STANFORD,\n",
    "        run_spell_check=False,\n",
    "        minwords=2,\n",
    "        add_additional_tags=True,\n",
    "        tagger_type='stanford',\n",
    "        stanford_pos_path=STANFORD_PATH,\n",
    "        stanford_language_path=STANFORD_MODEL,\n",
    "        stanford_batch_size=50  # Process in batches for speed\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Stanford preprocessing complete!\")\n",
    "    \n",
    "    # Show additional columns\n",
    "    stanford_cols = [c for c in results_stanford.columns if c not in results_nltk.columns]\n",
    "    print(f\"\\nAdditional columns with Stanford:\")\n",
    "    for col in stanford_cols:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"‚äò Skipping Stanford preprocessing (not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Output Files\n",
    "\n",
    "Your preprocessed files are now ready for alignment analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÅ Preprocessed Files Created:\\n\")\n",
    "\n",
    "for label, path in [(\"NLTK-only\", OUTPUT_DIR_BASIC),\n",
    "                    (\"NLTK + spaCy\", OUTPUT_DIR_SPACY),\n",
    "                    (\"NLTK + Stanford\", OUTPUT_DIR_STANFORD)]:\n",
    "    if os.path.exists(path):\n",
    "        files = [f for f in os.listdir(path) if f.endswith('.txt')]\n",
    "        if files:\n",
    "            print(f\"‚úì {label}: {len(files)} files in {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TUTORIAL 1 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext Step: Run Tutorial 2 (Alignment Analysis)\")\n",
    "print(\"This will analyze the preprocessed files to compute alignment metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding the Output Format\n",
    "\n",
    "Each preprocessed file contains:\n",
    "\n",
    "### Core Columns (always present):\n",
    "- `participant`: Speaker identifier\n",
    "- `content`: Original utterance text (cleaned)\n",
    "- `token`: List of tokenized words (as string)\n",
    "- `lemma`: List of lemmatized words (as string)\n",
    "- `tagged_token`: List of (word, POS) tuples from NLTK (as string)\n",
    "- `tagged_lemma`: List of (lemma, POS) tuples from NLTK (as string)\n",
    "- `file`: Source filename\n",
    "\n",
    "### Additional Columns (when using extra taggers):\n",
    "- `tagged_spacy_token`: POS tags from spaCy\n",
    "- `tagged_spacy_lemma`: POS tags for lemmas from spaCy\n",
    "- `tagged_stan_token`: POS tags from Stanford\n",
    "- `tagged_stan_lemma`: POS tags for lemmas from Stanford\n",
    "\n",
    "### Important Notes:\n",
    "- All list/tuple columns are stored as **strings**\n",
    "- Use `ast.literal_eval()` to convert strings back to Python objects\n",
    "- This format ensures compatibility with the alignment analysis phase\n",
    "- A concatenated file with all conversations is also saved for batch processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (align-venv)",
   "language": "python",
   "name": "align-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
