{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Tutorial 1: Preprocessing Conversational Transcripts\n",
    "\n",
    "This tutorial demonstrates how to use the ALIGN package to preprocess conversational data for linguistic alignment analysis.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to prepare raw conversational transcripts for alignment analysis\n",
    "- Using different POS taggers (NLTK, spaCy, Stanford)\n",
    "- Understanding the preprocessing output format\n",
    "- Validating that outputs are ready for Phase 2 (alignment analysis)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You should have already:\n",
    "1. Cloned this repository\n",
    "2. Installed the package: `pip install -e .`\n",
    "3. Sample data is available in `src/align_test/data/CHILDES/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Import and Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ndd697/.pyenv/versions/3.13.5/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Import the preprocessing function\n",
    "from align_test.prepare_transcripts import prepare_transcripts\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: ../src/align_test/data/CHILDES/\n",
      "Output directories created ✓\n"
     ]
    }
   ],
   "source": [
    "# Configure paths\n",
    "# Input: Sample CHILDES data (included in the package)\n",
    "INPUT_DIR = '../src/align_test/data/CHILDES/'\n",
    "\n",
    "# Output: Where to save preprocessed files\n",
    "OUTPUT_DIR_BASIC = './tutorial_output/preprocessed_nltk'\n",
    "OUTPUT_DIR_SPACY = './tutorial_output/preprocessed_spacy'\n",
    "OUTPUT_DIR_STANFORD = './tutorial_output/preprocessed_stanford'\n",
    "\n",
    "# Create output directories\n",
    "for dir_path in [OUTPUT_DIR_BASIC, OUTPUT_DIR_SPACY, OUTPUT_DIR_STANFORD]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Output directories created ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Inspect Sample Input Data\n",
    "\n",
    "Let's look at what raw conversation data looks like before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inspect_input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 conversation files\n",
      "\n",
      "Sample file: time197-cond1.txt\n",
      "Columns: ['participant', 'content']\n",
      "Rows: 127\n",
      "\n",
      "First 5 utterances:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cgv</td>\n",
       "      <td>that was fun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kid</td>\n",
       "      <td>Dad you should have climbed the cliffs with us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cgv</td>\n",
       "      <td>next time I will.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kid</td>\n",
       "      <td>did you have fun fishing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cgv</td>\n",
       "      <td>yeah.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant                                          content\n",
       "0         cgv                                    that was fun.\n",
       "1         kid  Dad you should have climbed the cliffs with us.\n",
       "2         cgv                                next time I will.\n",
       "3         kid                        did you have fun fishing.\n",
       "4         cgv                                            yeah."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available files\n",
    "files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.txt')]\n",
    "print(f\"Found {len(files)} conversation files\\n\")\n",
    "\n",
    "# Load and display a sample file\n",
    "sample_file = os.path.join(INPUT_DIR, files[0])\n",
    "df_sample = pd.read_csv(sample_file, sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(f\"Sample file: {files[0]}\")\n",
    "print(f\"Columns: {df_sample.columns.tolist()}\")\n",
    "print(f\"Rows: {len(df_sample)}\\n\")\n",
    "print(\"First 5 utterances:\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess1",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Basic Preprocessing (NLTK Only)\n",
    "\n",
    "This is the **fastest option** and requires no additional setup. NLTK tags are automatically downloaded if needed.\n",
    "\n",
    "### What This Does:\n",
    "1. Cleans text (removes non-letters, fillers like \"um\", \"uh\")\n",
    "2. Merges adjacent turns by the same speaker\n",
    "3. Tokenizes and lemmatizes words\n",
    "4. Adds POS (Part-of-Speech) tags using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "preprocess_nltk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NLTK-only preprocessing...\n",
      "\n",
      "Downloading required NLTK resources...\n",
      "  - Downloading wordnet...\n",
      "    ✓ wordnet downloaded successfully\n",
      "  - Downloading omw-1.4...\n",
      "    ✓ omw-1.4 downloaded successfully\n",
      "NLTK resources ready!\n",
      "\n",
      "\n",
      "Found 20 files to process\n",
      "Output directory: ./tutorial_output/preprocessed_nltk\n",
      "\n",
      "============================================================\n",
      "Processing: time197-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time197-cond1.txt\n",
      "     Rows: 76\n",
      "============================================================\n",
      "Processing: time202-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time202-cond1.txt\n",
      "     Rows: 92\n",
      "============================================================\n",
      "Processing: time191-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time191-cond1.txt\n",
      "     Rows: 99\n",
      "============================================================\n",
      "Processing: time209-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time209-cond1.txt\n",
      "     Rows: 98\n",
      "============================================================\n",
      "Processing: time210-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time210-cond1.txt\n",
      "     Rows: 100\n",
      "============================================================\n",
      "Processing: time204-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time204-cond1.txt\n",
      "     Rows: 143\n",
      "============================================================\n",
      "Processing: time196-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time196-cond1.txt\n",
      "     Rows: 66\n",
      "============================================================\n",
      "Processing: time203-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time203-cond1.txt\n",
      "     Rows: 90\n",
      "============================================================\n",
      "Processing: time208-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time208-cond1.txt\n",
      "     Rows: 86\n",
      "============================================================\n",
      "Processing: time205-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time205-cond1.txt\n",
      "     Rows: 106\n",
      "============================================================\n",
      "Processing: time195-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time195-cond1.txt\n",
      "     Rows: 90\n",
      "============================================================\n",
      "Processing: time198-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time198-cond1.txt\n",
      "     Rows: 89\n",
      "============================================================\n",
      "Processing: time200-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time200-cond1.txt\n",
      "     Rows: 78\n",
      "============================================================\n",
      "Processing: time193-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time193-cond1.txt\n",
      "     Rows: 95\n",
      "============================================================\n",
      "Processing: time206-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time206-cond1.txt\n",
      "     Rows: 97\n",
      "============================================================\n",
      "Processing: time194-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time194-cond1.txt\n",
      "     Rows: 77\n",
      "============================================================\n",
      "Processing: time199-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time199-cond1.txt\n",
      "     Rows: 87\n",
      "============================================================\n",
      "Processing: time201-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time201-cond1.txt\n",
      "     Rows: 90\n",
      "============================================================\n",
      "Processing: time192-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time192-cond1.txt\n",
      "     Rows: 67\n",
      "============================================================\n",
      "Processing: time207-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "  6. Saved: time207-cond1.txt\n",
      "     Rows: 106\n",
      "\n",
      "============================================================\n",
      "Saved concatenated dataframe: align_concatenated_dataframe.txt\n",
      "Total rows: 1832\n",
      "============================================================\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Summary:\n",
      "  - Files processed: 20\n",
      "  - Total utterances: 1832\n",
      "  - Output directory: ./tutorial_output/preprocessed_nltk\n",
      "\n",
      "✓ Preprocessing complete!\n",
      "Total utterances processed: 1832\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting NLTK-only preprocessing...\\n\")\n",
    "\n",
    "results_nltk = prepare_transcripts(\n",
    "    input_files=INPUT_DIR,\n",
    "    output_file_directory=OUTPUT_DIR_BASIC,\n",
    "    run_spell_check=False,          # Disable for speed (optional)\n",
    "    minwords=2,                      # Minimum words per utterance\n",
    "    add_additional_tags=False        # NLTK only\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Preprocessing complete!\")\n",
    "print(f\"Total utterances processed: {len(results_nltk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "examine1",
   "metadata": {},
   "source": [
    "### Examine Preprocessed Output\n",
    "\n",
    "Let's see what the preprocessed data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "examine_nltk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output columns:\n",
      "  1. participant\n",
      "  2. content\n",
      "  3. token\n",
      "  4. lemma\n",
      "  5. tagged_token\n",
      "  6. tagged_lemma\n",
      "  7. file\n",
      "\n",
      "Sample row:\n",
      "Participant: cgv\n",
      "Content: that was fun\n",
      "Tokens: ['that', 'was', 'fun']\n",
      "Lemmas: ['that', 'be', 'fun']\n",
      "Tagged: [('that', 'DT'), ('was', 'VBD'), ('fun', 'NN')]...\n"
     ]
    }
   ],
   "source": [
    "print(\"Output columns:\")\n",
    "for i, col in enumerate(results_nltk.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(f\"\\nSample row:\")\n",
    "sample = results_nltk.iloc[0]\n",
    "print(f\"Participant: {sample['participant']}\")\n",
    "print(f\"Content: {sample['content']}\")\n",
    "print(f\"Tokens: {sample['token']}\")\n",
    "print(f\"Lemmas: {sample['lemma']}\")\n",
    "print(f\"Tagged: {sample['tagged_token'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate",
   "metadata": {},
   "source": [
    "### Validate Output Format\n",
    "\n",
    "The preprocessing creates strings that can be parsed back to Python objects. This is important for Phase 2 (alignment analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "validate_format",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token type after parsing: <class 'list'>\n",
      "Tokens: ['that', 'was', 'fun']\n",
      "\n",
      "Tagged token type: <class 'list'>\n",
      "Tagged tokens (first 3): [('that', 'DT'), ('was', 'VBD'), ('fun', 'NN')]\n",
      "\n",
      "✓ Format validation successful!\n"
     ]
    }
   ],
   "source": [
    "# Test parsing\n",
    "sample_row = results_nltk.iloc[0]\n",
    "\n",
    "# Parse token string back to list\n",
    "tokens = ast.literal_eval(sample_row['token'])\n",
    "print(f\"Token type after parsing: {type(tokens)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Parse tagged tokens\n",
    "tagged = ast.literal_eval(sample_row['tagged_token'])\n",
    "print(f\"\\nTagged token type: {type(tagged)}\")\n",
    "print(f\"Tagged tokens (first 3): {tagged[:3]}\")\n",
    "print(f\"\\n✓ Format validation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spacy_intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Preprocessing with spaCy (Optional)\n",
    "\n",
    "spaCy provides **100-200x faster** POS tagging than Stanford with minimal accuracy differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "check_spacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ spaCy is installed\n"
     ]
    }
   ],
   "source": [
    "# Check if spaCy is available\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"✓ spaCy is installed\")\n",
    "    spacy_available = True\n",
    "except ImportError:\n",
    "    print(\"✗ spaCy not installed\")\n",
    "    print(\"\\nTo install: pip install spacy\")\n",
    "    print(\"Then run: python -m spacy download en_core_web_sm\")\n",
    "    spacy_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "preprocess_spacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with spaCy...\n",
      "\n",
      "Downloading required NLTK resources...\n",
      "  - Downloading wordnet...\n",
      "    ✓ wordnet downloaded successfully\n",
      "  - Downloading omw-1.4...\n",
      "    ✓ omw-1.4 downloaded successfully\n",
      "NLTK resources ready!\n",
      "\n",
      "\n",
      "Found 20 files to process\n",
      "Output directory: ./tutorial_output/preprocessed_spacy\n",
      "\n",
      "============================================================\n",
      "Processing: time197-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "spaCy model 'en_core_web_sm' not found. Downloading...\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded and loaded: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 76 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 76/76 [00:00<00:00, 1011.18it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 76/76 [00:00<00:00, 1184.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time197-cond1.txt\n",
      "     Rows: 76\n",
      "============================================================\n",
      "Processing: time202-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 92 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 92/92 [00:00<00:00, 1029.55it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 92/92 [00:00<00:00, 1065.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time202-cond1.txt\n",
      "     Rows: 92\n",
      "============================================================\n",
      "Processing: time191-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 99 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 99/99 [00:00<00:00, 1092.81it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 99/99 [00:00<00:00, 1183.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6. Saved: time191-cond1.txt\n",
      "     Rows: 99\n",
      "============================================================\n",
      "Processing: time209-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 98 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 98/98 [00:00<00:00, 1072.91it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 98/98 [00:00<00:00, 1102.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time209-cond1.txt\n",
      "     Rows: 98\n",
      "============================================================\n",
      "Processing: time210-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 100 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 100/100 [00:00<00:00, 1076.66it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 100/100 [00:00<00:00, 1182.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time210-cond1.txt\n",
      "     Rows: 100\n",
      "============================================================\n",
      "Processing: time204-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 143 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 143/143 [00:00<00:00, 1011.82it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 143/143 [00:00<00:00, 1050.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time204-cond1.txt\n",
      "     Rows: 143\n",
      "============================================================\n",
      "Processing: time196-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 66 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 66/66 [00:00<00:00, 1060.09it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 66/66 [00:00<00:00, 1005.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time196-cond1.txt\n",
      "     Rows: 66\n",
      "============================================================\n",
      "Processing: time203-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 90 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 90/90 [00:00<00:00, 1057.23it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 90/90 [00:00<00:00, 1093.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time203-cond1.txt\n",
      "     Rows: 90\n",
      "============================================================\n",
      "Processing: time208-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 86 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 86/86 [00:00<00:00, 970.50it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 86/86 [00:00<00:00, 1008.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time208-cond1.txt\n",
      "     Rows: 86\n",
      "============================================================\n",
      "Processing: time205-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 106 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 106/106 [00:00<00:00, 984.82it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 106/106 [00:00<00:00, 1037.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time205-cond1.txt\n",
      "     Rows: 106\n",
      "============================================================\n",
      "Processing: time195-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 90 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 90/90 [00:00<00:00, 1037.61it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 90/90 [00:00<00:00, 1049.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time195-cond1.txt\n",
      "     Rows: 90\n",
      "============================================================\n",
      "Processing: time198-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 89 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 89/89 [00:00<00:00, 1056.83it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 89/89 [00:00<00:00, 1100.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time198-cond1.txt\n",
      "     Rows: 89\n",
      "============================================================\n",
      "Processing: time200-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 78 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 78/78 [00:00<00:00, 912.38it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 78/78 [00:00<00:00, 926.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time200-cond1.txt\n",
      "     Rows: 78\n",
      "============================================================\n",
      "Processing: time193-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 95 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 95/95 [00:00<00:00, 1028.02it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 95/95 [00:00<00:00, 1034.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time193-cond1.txt\n",
      "     Rows: 95\n",
      "============================================================\n",
      "Processing: time206-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 97 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 97/97 [00:00<00:00, 921.72it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 97/97 [00:00<00:00, 1007.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time206-cond1.txt\n",
      "     Rows: 97\n",
      "============================================================\n",
      "Processing: time194-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 77 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 77/77 [00:00<00:00, 953.76it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 77/77 [00:00<00:00, 975.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time194-cond1.txt\n",
      "     Rows: 77\n",
      "============================================================\n",
      "Processing: time199-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 87 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 87/87 [00:00<00:00, 974.28it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 87/87 [00:00<00:00, 1008.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time199-cond1.txt\n",
      "     Rows: 87\n",
      "============================================================\n",
      "Processing: time201-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 90 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 90/90 [00:00<00:00, 1027.05it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 90/90 [00:00<00:00, 1030.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time201-cond1.txt\n",
      "     Rows: 90\n",
      "============================================================\n",
      "Processing: time192-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 67 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 67/67 [00:00<00:00, 963.93it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 67/67 [00:00<00:00, 1014.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time192-cond1.txt\n",
      "     Rows: 67\n",
      "============================================================\n",
      "Processing: time207-cond1.txt\n",
      "============================================================\n",
      "  1. Cleaning text...\n",
      "  2. Merging adjacent turns...\n",
      "  3. Tokenizing...\n",
      "  4. Lemmatizing...\n",
      "  5. Applying POS tagging...\n",
      "Initializing spaCy POS tagger...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "spaCy tagger initialized (disabled: ['parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
      "Converting tokens and lemmas to string representations...\n",
      "Applying NLTK POS tagging...\n",
      "NLTK POS tagging complete\n",
      "Applying spaCy POS tagging to 106 utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy tagging tokens: 100%|██████████| 106/106 [00:00<00:00, 1020.95it/s]\n",
      "spaCy tagging lemmas: 100%|██████████| 106/106 [00:00<00:00, 1001.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS tagging complete\n",
      "  6. Saved: time207-cond1.txt\n",
      "     Rows: 106\n",
      "\n",
      "============================================================\n",
      "Saved concatenated dataframe: align_concatenated_dataframe.txt\n",
      "Total rows: 1832\n",
      "============================================================\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Summary:\n",
      "  - Files processed: 20\n",
      "  - Total utterances: 1832\n",
      "  - Output directory: ./tutorial_output/preprocessed_spacy\n",
      "\n",
      "✓ spaCy preprocessing complete!\n",
      "\n",
      "Additional columns with spaCy:\n",
      "  - tagged_spacy_token\n",
      "  - tagged_spacy_lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if spacy_available:\n",
    "    print(\"Starting preprocessing with spaCy...\\n\")\n",
    "    \n",
    "    results_spacy = prepare_transcripts(\n",
    "        input_files=INPUT_DIR,\n",
    "        output_file_directory=OUTPUT_DIR_SPACY,\n",
    "        run_spell_check=False,\n",
    "        minwords=2,\n",
    "        add_additional_tags=True,       # Add spaCy tags\n",
    "        tagger_type='spacy'             # Specify spaCy\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ spaCy preprocessing complete!\")\n",
    "    \n",
    "    # Show additional columns\n",
    "    spacy_cols = [c for c in results_spacy.columns if c not in results_nltk.columns]\n",
    "    print(f\"\\nAdditional columns with spaCy:\")\n",
    "    for col in spacy_cols:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"⊘ Skipping spaCy preprocessing (not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stanford_intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Preprocessing with Stanford (Optional)\n",
    "\n",
    "Stanford CoreNLP provides the **highest accuracy** but is **~100x slower** than spaCy.\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "#### 1. Install Java\n",
    "```bash\n",
    "# macOS\n",
    "brew install openjdk\n",
    "\n",
    "# Linux\n",
    "sudo apt-get install default-jdk\n",
    "\n",
    "# Windows\n",
    "# Download from: https://www.java.com/en/download/\n",
    "```\n",
    "\n",
    "#### 2. Download Stanford POS Tagger\n",
    "- Download from: https://nlp.stanford.edu/software/tagger.shtml#Download\n",
    "- Extract to a known location (e.g., `~/stanford-postagger/`)\n",
    "- Update the paths in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_stanford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Java installed: java version \"1.8.0_471\"\n",
      "✗ Stanford tagger not found at: /Users/ndd697/stanford-postagger-full-2020-11-17\n",
      "\n",
      "Please:\n",
      "  1. Download from: https://nlp.stanford.edu/software/tagger.shtml#Download\n",
      "  2. Extract and update STANFORD_PATH above\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Check Java\n",
    "try:\n",
    "    result = subprocess.run(['java', '-version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        version = result.stderr.split('\\n')[0]\n",
    "        print(f\"✓ Java installed: {version}\")\n",
    "        java_available = True\n",
    "    else:\n",
    "        print(\"✗ Java not working properly\")\n",
    "        java_available = False\n",
    "except:\n",
    "    print(\"✗ Java not found\")\n",
    "    print(\"\\nPlease install Java first (see instructions above)\")\n",
    "    java_available = False\n",
    "\n",
    "# Configure STANFORD_PATH (UPDATE THESE FOR YOUR SYSTEM)\n",
    "if java_available:\n",
    "    STANFORD_PATH = os.path.expanduser('~/stanford-postagger-full-2020-11-17') # Update this path\n",
    "    STANFORD_MODEL = 'models/english-left3words-distsim.tagger'\n",
    "    \n",
    "    # Check if Stanford tagger exists\n",
    "    jar_path = os.path.join(STANFORD_PATH, 'stanford-postagger.jar')\n",
    "    model_path = os.path.join(STANFORD_PATH, STANFORD_MODEL)\n",
    "    \n",
    "    if os.path.exists(jar_path) and os.path.exists(model_path):\n",
    "        print(f\"✓ Stanford tagger found at: {STANFORD_PATH}\")\n",
    "        stanford_available = True\n",
    "    else:\n",
    "        print(f\"✗ Stanford tagger not found at: {STANFORD_PATH}\")\n",
    "        print(\"\\nPlease:\")\n",
    "        print(\"  1. Download from: https://nlp.stanford.edu/software/tagger.shtml#Download\")\n",
    "        print(\"  2. Extract and update STANFORD_PATH above\")\n",
    "        stanford_available = False\n",
    "else:\n",
    "    stanford_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess_stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stanford_available:\n",
    "    print(\"Starting preprocessing with Stanford...\")\n",
    "    print(\"⚠️  This may take several minutes...\\n\")\n",
    "    \n",
    "    results_stanford = prepare_transcripts(\n",
    "        input_files=INPUT_DIR,\n",
    "        output_file_directory=OUTPUT_DIR_STANFORD,\n",
    "        run_spell_check=False,\n",
    "        minwords=2,\n",
    "        add_additional_tags=True,\n",
    "        tagger_type='stanford',\n",
    "        stanford_pos_path=STANFORD_PATH,\n",
    "        stanford_language_path=STANFORD_MODEL,\n",
    "        stanford_batch_size=50  # Process in batches for speed\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Stanford preprocessing complete!\")\n",
    "    \n",
    "    # Show additional columns\n",
    "    stanford_cols = [c for c in results_stanford.columns if c not in results_nltk.columns]\n",
    "    print(f\"\\nAdditional columns with Stanford:\")\n",
    "    for col in stanford_cols:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"⊘ Skipping Stanford preprocessing (not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Output Files\n",
    "\n",
    "Your preprocessed files are now ready for alignment analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📁 Preprocessed Files Created:\\n\")\n",
    "\n",
    "for label, path in [(\"NLTK-only\", OUTPUT_DIR_BASIC),\n",
    "                    (\"NLTK + spaCy\", OUTPUT_DIR_SPACY),\n",
    "                    (\"NLTK + Stanford\", OUTPUT_DIR_STANFORD)]:\n",
    "    if os.path.exists(path):\n",
    "        files = [f for f in os.listdir(path) if f.endswith('.txt')]\n",
    "        if files:\n",
    "            print(f\"✓ {label}: {len(files)} files in {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ TUTORIAL 1 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext Step: Run Tutorial 2 (Alignment Analysis)\")\n",
    "print(\"This will analyze the preprocessed files to compute alignment metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding the Output Format\n",
    "\n",
    "Each preprocessed file contains:\n",
    "\n",
    "### Core Columns (always present):\n",
    "- `participant`: Speaker identifier\n",
    "- `content`: Original utterance text (cleaned)\n",
    "- `token`: List of tokenized words (as string)\n",
    "- `lemma`: List of lemmatized words (as string)\n",
    "- `tagged_token`: List of (word, POS) tuples from NLTK (as string)\n",
    "- `tagged_lemma`: List of (lemma, POS) tuples from NLTK (as string)\n",
    "- `file`: Source filename\n",
    "\n",
    "### Additional Columns (when using extra taggers):\n",
    "- `tagged_spacy_token`: POS tags from spaCy\n",
    "- `tagged_spacy_lemma`: POS tags for lemmas from spaCy\n",
    "- `tagged_stan_token`: POS tags from Stanford\n",
    "- `tagged_stan_lemma`: POS tags for lemmas from Stanford\n",
    "\n",
    "### Important Notes:\n",
    "- All list/tuple columns are stored as **strings**\n",
    "- Use `ast.literal_eval()` to convert strings back to Python objects\n",
    "- This format ensures compatibility with the alignment analysis phase\n",
    "- A concatenated file with all conversations is also saved for batch processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (align-venv)",
   "language": "python",
   "name": "align-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
