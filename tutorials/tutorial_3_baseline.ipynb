{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Tutorial 3: Baseline Alignment with Surrogate Conversations\n",
    "\n",
    "This tutorial demonstrates how to establish **baseline alignment levels** using surrogate conversation pairs.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- What surrogate conversations are and why they matter\n",
    "- How to generate surrogate pairs from real conversations\n",
    "- Computing baseline alignment in surrogate data\n",
    "- Comparing real vs. baseline alignment\n",
    "- Interpreting statistical significance of alignment\n",
    "\n",
    "## Why Baseline Analysis Matters\n",
    "\n",
    "When you find alignment in real conversations, you need to ask: **\"Is this alignment meaningful, or could it occur by chance?\"**\n",
    "\n",
    "### The Problem:\n",
    "Some alignment occurs naturally just from:\n",
    "- Speaking the same language\n",
    "- Discussing the same topic\n",
    "- Using common grammatical structures\n",
    "\n",
    "### The Solution: Surrogate Pairs\n",
    "Create \"fake\" conversations by pairing speakers who **never actually talked to each other**:\n",
    "- Same experimental condition\n",
    "- Same number of turns\n",
    "- But different dyads (pairs of people)\n",
    "\n",
    "**Example**:\n",
    "- Real conversation: Person A talks with Person B\n",
    "- Surrogate: Person A's turns paired with Person C's turns (who never met)\n",
    "\n",
    "### The Test:\n",
    "If **real alignment > baseline alignment**, the alignment is likely due to genuine interaction, not chance!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You should have already:\n",
    "1. Completed Tutorial 1 (Preprocessing)\n",
    "2. Completed Tutorial 2 (Alignment Analysis)\n",
    "3. Have real alignment results saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Import and Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Import the alignment analyzer\n",
    "from align_test.alignment import LinguisticAlignment\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configure Paths\n",
    "# ============================================================\n",
    "\n",
    "# INPUT: Choose which preprocessed output to use\n",
    "# This should match what you used in Tutorial 2 for real alignment!\n",
    "#\n",
    "# Options:\n",
    "#   './tutorial_output/preprocessed_nltk'     - NLTK tags only (fastest)\n",
    "#   './tutorial_output/preprocessed_spacy'    - NLTK + spaCy tags\n",
    "#   './tutorial_output/preprocessed_stanford' - NLTK + Stanford tags\n",
    "#\n",
    "# ‚ö†Ô∏è IMPORTANT: Use the SAME preprocessing output you used in Tutorial 2\n",
    "# so that baseline results are directly comparable to real results!\n",
    "\n",
    "INPUT_DIR = './tutorial_output/preprocessed_nltk'  # ‚Üê Change this if needed\n",
    "\n",
    "# OUTPUT: Where to save baseline results\n",
    "OUTPUT_DIR = './tutorial_output/baseline_results'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Using preprocessed files from: {INPUT_DIR}\")\n",
    "print(f\"Baseline results will be saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Verify input data exists\n",
    "if os.path.exists(INPUT_DIR):\n",
    "    files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.txt') and 'concatenated' not in f]\n",
    "    print(f\"\\n‚úì Found {len(files)} preprocessed conversation files\")\n",
    "    print(f\"\\nSample filenames:\")\n",
    "    for f in files[:3]:\n",
    "        print(f\"  - {f}\")\n",
    "else:\n",
    "    print(\"\\n‚úó Preprocessed data not found!\")\n",
    "    print(\"Please run Tutorial 1 (Preprocessing) first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filename_pattern",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Identify Filename Patterns\n",
    "\n",
    "The surrogate algorithm needs to identify **which dyad** and **which condition** each file belongs to. It does this by looking at the **filename patterns** from your preprocessing output.\n",
    "\n",
    "**Important**: You're **NOT** changing your filenames! The filenames come from Tutorial 1 preprocessing and are already set. Your job is to **tell the algorithm how to read them**.\n",
    "\n",
    "### What the Algorithm Needs to Know:\n",
    "\n",
    "Looking at the CHILDES sample filenames (`time197-cond1.txt`, `time202-cond1.txt`, etc.):\n",
    "- **Dyad identifier**: `time197`, `time202` (the part that identifies which pair of people)\n",
    "- **Condition identifier**: `cond1` (the experimental condition)\n",
    "- **Separator**: `-` (the character between components)\n",
    "\n",
    "### Your Task: Configure These Parameters\n",
    "\n",
    "In the code below, you'll specify:\n",
    "- `DYAD_LABEL`: The text that **precedes** the dyad ID (e.g., `'time'` in `time197`)\n",
    "- `CONDITION_LABEL`: The text that **precedes** the condition ID (e.g., `'cond'` in `cond1`)\n",
    "- `ID_SEPARATOR`: The character that separates parts (e.g., `'-'` in `time197-cond1`)\n",
    "\n",
    "### More Examples:\n",
    "\n",
    "| Filename | DYAD_LABEL | CONDITION_LABEL | ID_SEPARATOR |\n",
    "|----------|------------|-----------------|---------------|\n",
    "| `time197-cond1.txt` | `'time'` | `'cond'` | `'-'` |\n",
    "| `dyad05-condition2.txt` | `'dyad'` | `'condition'` | `'-'` |\n",
    "| `pair_A_exp_1.txt` | `'pair'` | `'exp'` | `'_'` |\n",
    "\n",
    "**The algorithm will then:**\n",
    "1. Extract dyad and condition IDs from each filename\n",
    "2. Group files by condition (e.g., all `cond1` files together)\n",
    "3. Within each condition, pair different dyads to create surrogates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Identify the Filename Pattern in YOUR Data\n",
    "# ============================================================\n",
    "# Look at your actual filenames and configure these parameters\n",
    "# to match YOUR naming convention:\n",
    "\n",
    "ID_SEPARATOR = '-'        # ‚Üê Character separating parts (e.g., '-' in 'time197-cond1')\n",
    "DYAD_LABEL = 'time'       # ‚Üê Text before dyad ID (e.g., 'time' in 'time197-cond1')\n",
    "CONDITION_LABEL = 'cond'  # ‚Üê Text before condition ID (e.g., 'cond' in 'time197-cond1')\n",
    "\n",
    "print(f\"Your Filename Pattern Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Dyad label: '{DYAD_LABEL}'\")\n",
    "print(f\"  Condition label: '{CONDITION_LABEL}'\")\n",
    "print(f\"  Separator: '{ID_SEPARATOR}'\")\n",
    "print(f\"\\nüí° These parameters tell the algorithm how to READ your existing filenames.\")\n",
    "print(f\"   You are NOT renaming files - just identifying the pattern!\")\n",
    "\n",
    "# Validate that your filenames match this pattern\n",
    "print(f\"\\nValidating filenames against your configuration:\\n\")\n",
    "\n",
    "for filename in files[:5]:\n",
    "    has_dyad = DYAD_LABEL in filename\n",
    "    has_cond = CONDITION_LABEL in filename\n",
    "    has_sep = ID_SEPARATOR in filename\n",
    "    \n",
    "    status = \"‚úì\" if (has_dyad and has_cond and has_sep) else \"‚úó\"\n",
    "    print(f\"{status} {filename}\")\n",
    "    \n",
    "    if not (has_dyad and has_cond and has_sep):\n",
    "        missing = []\n",
    "        if not has_dyad: missing.append(f\"'{DYAD_LABEL}'\")\n",
    "        if not has_cond: missing.append(f\"'{CONDITION_LABEL}'\")\n",
    "        if not has_sep: missing.append(f\"'{ID_SEPARATOR}'\")\n",
    "        print(f\"   ‚ö†Ô∏è  Missing: {', '.join(missing)}\")\n",
    "        print(f\"   ‚Üí Update the configuration parameters above to match your filenames\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"If all files show ‚úì, you're ready to proceed!\")\n",
    "print(\"If any show ‚úó, update DYAD_LABEL, CONDITION_LABEL, or ID_SEPARATOR above.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrogate_gen",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Generate Surrogate Conversation Pairs\n",
    "\n",
    "The algorithm will:\n",
    "1. Group files by condition (e.g., all `cond1` files together)\n",
    "2. Create all possible pairings of different dyads within each condition\n",
    "3. For each pairing, create 2 surrogate conversations by interleaving turns\n",
    "\n",
    "### Example:\n",
    "**Original conversations:**\n",
    "- File 1: Dyad A (Person 1 + Person 2)\n",
    "- File 2: Dyad B (Person 3 + Person 4)\n",
    "\n",
    "**Surrogates created:**\n",
    "- Surrogate 1: Person 1's turns + Person 3's turns\n",
    "- Surrogate 2: Person 2's turns + Person 4's turns\n",
    "\n",
    "**Result**: You'll typically generate many surrogate pairs (e.g., with 20 files ‚Üí ~190 surrogate pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer for baseline analysis\n",
    "print(\"Initializing analyzer for baseline analysis...\\n\")\n",
    "\n",
    "analyzer_baseline = LinguisticAlignment(\n",
    "    alignment_type=\"lexsyn\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Analyzer ready for baseline computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate surrogates and analyze baseline alignment\n",
    "print(\"Generating surrogate pairs and computing baseline...\\n\")\n",
    "print(\"‚ö†Ô∏è  This may take several minutes depending on the number of files...\\n\")\n",
    "\n",
    "baseline_results = analyzer_baseline.analyze_baseline(\n",
    "    input_files=INPUT_DIR,\n",
    "    output_directory=OUTPUT_DIR,\n",
    "    lag=1,\n",
    "    max_ngram=2,\n",
    "    ignore_duplicates=True,\n",
    "    all_surrogates=True,              # Generate all possible pairings\n",
    "    keep_original_turn_order=True,    # Maintain temporal order\n",
    "    id_separator=ID_SEPARATOR,\n",
    "    dyad_label=DYAD_LABEL,\n",
    "    condition_label=CONDITION_LABEL\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Baseline analysis complete!\")\n",
    "print(f\"Surrogate pairs analyzed: {len(baseline_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examine_surrogates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine what was created\n",
    "import glob\n",
    "\n",
    "surrogate_dir = os.path.join(OUTPUT_DIR, 'surrogates')\n",
    "surrogate_runs = [d for d in os.listdir(surrogate_dir) if d.startswith('surrogate_run-')]\n",
    "\n",
    "if surrogate_runs:\n",
    "    latest_run = sorted(surrogate_runs)[-1]\n",
    "    surrogate_files = glob.glob(os.path.join(surrogate_dir, latest_run, '*.txt'))\n",
    "    \n",
    "    print(f\"Surrogate Generation Summary:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Original conversations: {len(files)}\")\n",
    "    print(f\"  Surrogate pairs created: {len(surrogate_files)}\")\n",
    "    print(f\"  Location: {surrogate_dir}/{latest_run}/\")\n",
    "    \n",
    "    print(f\"\\n  Sample surrogate filenames:\")\n",
    "    for f in surrogate_files[:3]:\n",
    "        print(f\"    - {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Load Real Alignment Results\n",
    "\n",
    "Load the real conversation alignment computed in Tutorial 2 for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_real",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real alignment results from Tutorial 2\n",
    "real_results_path = './tutorial_output/alignment_results/lexsyn/lexsyn_alignment_ngram2_lag1_noDups_noAdd.csv'\n",
    "\n",
    "if os.path.exists(real_results_path):\n",
    "    real_results = pd.read_csv(real_results_path)\n",
    "    print(f\"‚úì Loaded real alignment results\")\n",
    "    print(f\"  Utterance pairs: {len(real_results)}\")\n",
    "else:\n",
    "    print(\"‚úó Real alignment results not found!\")\n",
    "    print(\"Please run Tutorial 2 first to generate real alignment results.\")\n",
    "    print(f\"Expected file: {real_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_viz",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Compare Real vs. Baseline Alignment\n",
    "\n",
    "Now we can see if real conversations show more alignment than surrogate pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistics\n",
    "print(\"Alignment Comparison: Real vs. Baseline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics = ['lexical_master_cosine', 'syntactic_master_cosine']\n",
    "\n",
    "for metric in metrics:\n",
    "    real_mean = real_results[metric].mean()\n",
    "    baseline_mean = baseline_results[metric].mean()\n",
    "    difference = real_mean - baseline_mean\n",
    "    percent_increase = (difference / baseline_mean * 100) if baseline_mean > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Real conversations:  {real_mean:.4f}\")\n",
    "    print(f\"  Baseline (surrogates): {baseline_mean:.4f}\")\n",
    "    print(f\"  Difference: {difference:.4f} ({percent_increase:+.1f}%)\")\n",
    "    \n",
    "    if difference > 0:\n",
    "        print(f\"  ‚Üí Real conversations show MORE alignment ‚úì\")\n",
    "    else:\n",
    "        print(f\"  ‚Üí No additional alignment in real conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_compare",
   "metadata": {},
   "source": [
    "### Visualize Real vs. Baseline Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_distributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side boxplots for clearer comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Lexical alignment comparison\n",
    "data_lexical = [\n",
    "    real_results['lexical_master_cosine'].dropna(),\n",
    "    baseline_results['lexical_master_cosine'].dropna()\n",
    "]\n",
    "bp1 = axes[0].boxplot(data_lexical, labels=['Real', 'Baseline'], patch_artist=True)\n",
    "bp1['boxes'][0].set_facecolor('steelblue')\n",
    "bp1['boxes'][1].set_facecolor('lightgray')\n",
    "axes[0].set_title('Lexical Alignment: Real vs. Baseline', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Alignment Score', fontsize=12)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add mean markers\n",
    "means_lex = [d.mean() for d in data_lexical]\n",
    "axes[0].scatter([1, 2], means_lex, color='red', s=100, zorder=3, label='Mean', marker='D')\n",
    "axes[0].legend()\n",
    "\n",
    "# Syntactic alignment comparison\n",
    "data_syntactic = [\n",
    "    real_results['syntactic_master_cosine'].dropna(),\n",
    "    baseline_results['syntactic_master_cosine'].dropna()\n",
    "]\n",
    "bp2 = axes[1].boxplot(data_syntactic, labels=['Real', 'Baseline'], patch_artist=True)\n",
    "bp2['boxes'][0].set_facecolor('coral')\n",
    "bp2['boxes'][1].set_facecolor('lightgray')\n",
    "axes[1].set_title('Syntactic Alignment: Real vs. Baseline', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Alignment Score', fontsize=12)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add mean markers\n",
    "means_syn = [d.mean() for d in data_syntactic]\n",
    "axes[1].scatter([1, 2], means_syn, color='red', s=100, zorder=3, label='Mean', marker='D')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation Guide:\")\n",
    "print(\"  - Box: Middle 50% of data (25th-75th percentile)\")\n",
    "print(\"  - Line in box: Median\")\n",
    "print(\"  - Red diamond: Mean\")\n",
    "print(\"  - Whiskers: Range of data (excluding outliers)\")\n",
    "print(\"  - Circles: Outliers\\n\")\n",
    "print(\"If Real box/mean is higher than Baseline ‚Üí alignment is above chance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical_test",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Statistical Testing\n",
    "\n",
    "Perform statistical tests to determine if the difference between real and baseline is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Significance Testing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for metric in metrics:\n",
    "    # Independent samples t-test\n",
    "    real_values = real_results[metric].dropna()\n",
    "    baseline_values = baseline_results[metric].dropna()\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(real_values, baseline_values)\n",
    "    \n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.001:\n",
    "        print(f\"  ‚Üí Highly significant (p < 0.001) ***\")\n",
    "    elif p_value < 0.01:\n",
    "        print(f\"  ‚Üí Very significant (p < 0.01) **\")\n",
    "    elif p_value < 0.05:\n",
    "        print(f\"  ‚Üí Significant (p < 0.05) *\")\n",
    "    else:\n",
    "        print(f\"  ‚Üí Not significant (p >= 0.05)\")\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((real_values.std()**2 + baseline_values.std()**2) / 2)\n",
    "    cohens_d = (real_values.mean() - baseline_values.mean()) / pooled_std\n",
    "    print(f\"  Cohen's d: {cohens_d:.4f}\", end=\"\")\n",
    "    \n",
    "    if abs(cohens_d) > 0.8:\n",
    "        print(\" (large effect)\")\n",
    "    elif abs(cohens_d) > 0.5:\n",
    "        print(\" (medium effect)\")\n",
    "    elif abs(cohens_d) > 0.2:\n",
    "        print(\" (small effect)\")\n",
    "    else:\n",
    "        print(\" (negligible effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Interpretation Guide\n",
    "\n",
    "### Understanding the Results:\n",
    "\n",
    "#### If Real > Baseline (Statistically Significant):\n",
    "- ‚úÖ **Alignment is meaningful**: Speakers genuinely adapt to each other\n",
    "- ‚úÖ **Not just chance**: The alignment exceeds what random pairing would produce\n",
    "\n",
    "#### If Real ‚âà Baseline (Not Significant):\n",
    "- ‚ö†Ô∏è **Alignment may be spurious**: Could be due to topic/language constraints\n",
    "- ‚ö†Ô∏è **Need more data**: Or the effect is too subtle to detect\n",
    "- ‚ö†Ô∏è **Reconsider analysis**: Try different parameters or alignment types\n",
    "\n",
    "#### If Real < Baseline (Rare):\n",
    "- ü§î **Anti-alignment?**: Speakers may be deliberately differentiating\n",
    "- ü§î **Check your data**: Ensure preprocessing was correct\n",
    "- ü§î **Unusual pattern**: Worth investigating further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_comparison",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Save Comparison Data\n",
    "\n",
    "Create a summary dataframe for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "comparison_data = []\n",
    "\n",
    "for metric in metrics:\n",
    "    real_values = real_results[metric].dropna()\n",
    "    baseline_values = baseline_results[metric].dropna()\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(real_values, baseline_values)\n",
    "    pooled_std = np.sqrt((real_values.std()**2 + baseline_values.std()**2) / 2)\n",
    "    cohens_d = (real_values.mean() - baseline_values.mean()) / pooled_std\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'metric': metric,\n",
    "        'real_mean': real_values.mean(),\n",
    "        'real_std': real_values.std(),\n",
    "        'baseline_mean': baseline_values.mean(),\n",
    "        'baseline_std': baseline_values.std(),\n",
    "        'difference': real_values.mean() - baseline_values.mean(),\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'real_n': len(real_values),\n",
    "        'baseline_n': len(baseline_values)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Save comparison\n",
    "comparison_dir = os.path.join(OUTPUT_DIR, 'comparison')\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "comparison_path = os.path.join(comparison_dir, 'alignment_comparison_lexsyn.csv')\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "\n",
    "print(\"Comparison Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n‚úì Comparison saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_files",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Review All Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "review_outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÅ Baseline Analysis Output Files:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show directory structure\n",
    "for root, dirs, files_list in os.walk(OUTPUT_DIR):\n",
    "    level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    \n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files_list[:5]:  # Show first 5 files per directory\n",
    "        size_kb = os.path.getsize(os.path.join(root, file)) / 1024\n",
    "        print(f\"{subindent}{file} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "    if len(files_list) > 5:\n",
    "        print(f\"{subindent}... and {len(files_list) - 5} more files\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've completed the baseline analysis tutorial.\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. ‚úì **Surrogate Concept**: Why baseline comparison matters for research\n",
    "2. ‚úì **Surrogate Generation**: Creating fake conversation pairs from real data\n",
    "3. ‚úì **Baseline Computation**: Analyzing alignment in surrogate data\n",
    "4. ‚úì **Statistical Testing**: Testing significance of real vs. baseline\n",
    "5. ‚úì **Interpretation**: Understanding what your results mean\n",
    "\n",
    "### Key Findings to Report:\n",
    "\n",
    "For each alignment metric, you now have:\n",
    "- **Real alignment mean and SD**\n",
    "- **Baseline alignment mean and SD**\n",
    "- **Statistical test results** (t-test, p-value)\n",
    "- **Effect size** (Cohen's d)\n",
    "- **Interpretation** (is the alignment meaningful?)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Export results**: Load comparison CSV into R, SPSS, or Excel for publication\n",
    "- **Visualize in papers**: Use the plots generated here\n",
    "- **Try different analyzers**: Run baseline with FastText or BERT\n",
    "- **Adjust parameters**: Test different lag values, n-gram sizes\n",
    "- **Use your own data**: Apply to your research conversations\n",
    "\n",
    "### Advanced Options:\n",
    "\n",
    "#### Use Existing Surrogates:\n",
    "If you've already generated surrogates, reuse them:\n",
    "```python\n",
    "baseline_results = analyzer.analyze_baseline(\n",
    "    input_files=INPUT_DIR,\n",
    "    use_existing_surrogates='./path/to/surrogates/surrogate_run-123456/'\n",
    ")\n",
    "```\n",
    "\n",
    "#### Sample Fewer Surrogates:\n",
    "For faster testing, generate fewer pairs:\n",
    "```python\n",
    "baseline_results = analyzer.analyze_baseline(\n",
    "    input_files=INPUT_DIR,\n",
    "    all_surrogates=False  # Generate ~50% of possible pairs\n",
    ")\n",
    "```\n",
    "\n",
    "#### Randomize Turn Order:\n",
    "Break temporal structure in surrogates:\n",
    "```python\n",
    "baseline_results = analyzer.analyze_baseline(\n",
    "    input_files=INPUT_DIR,\n",
    "    keep_original_turn_order=False  # Shuffle turns\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Tutorial 3 Complete!\n",
    "\n",
    "You now have the complete workflow for rigorous alignment analysis:\n",
    "\n",
    "1. **Tutorial 1**: Preprocess raw conversations\n",
    "2. **Tutorial 2**: Compute alignment metrics\n",
    "3. **Tutorial 3**: Establish baseline and test significance ‚Üê You are here!\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Congratulations!\n",
    "\n",
    "You've mastered the complete ALIGN package workflow and are ready to conduct publication-quality linguistic alignment research.\n",
    "\n",
    "For questions or support, please visit the GitHub repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (align-venv)",
   "language": "python",
   "name": "align-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
