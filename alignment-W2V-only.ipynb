{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import gensim \n",
    "import gensim.downloader as api \n",
    "from gensim.models import KeyedVectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading in the gensim w2v models\n",
    "MODEL_w2v_google = 'word2vec-google-news-300'\n",
    "MODEL_w2v_twitter = 'glove-twitter-200'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How necessary is the below set of codes. It is every single conversation, utterance by utterance, with tokens and lemmas.... is this not equivalent to... \n",
    "\n",
    "taking the folder with all the files and concatenating them and that is that... and from there, easily running the `build_filtered_vocab` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set standards for which words will be evaluated in word2vec analysis\n",
    "TRANSCRIPTS_CONCAT_VOCAB_FILE = \"/Users/nduran4/Dropbox (ASU)/Mac/Desktop/GitProjects/align-linguistic-alignment/sandbox/transformers/align_concatenated_dataframe.txt\"\n",
    "HIGH_SD_CUTOFF = None\n",
    "LOW_N_CUTOFF = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulting the above processed list of filtered words, below gets the embeddings stored in word2vec and then aggregates them... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filtered_vocab(data: pd.DataFrame,\n",
    "                         output_file_directory: str,\n",
    "                         high_sd_cutoff: float = 3,\n",
    "                         low_n_cutoff: int = 1):\n",
    "    \n",
    "    # Tokenize the lemmas\n",
    "    all_sentences = [re.sub('[^\\w\\s]+','', str(row)).split() for row in data['lemma']]\n",
    "    all_words = [word for sentence in all_sentences for word in sentence]\n",
    "    \n",
    "    # Frequency count using Counter\n",
    "    frequency = Counter(all_words)\n",
    "    \n",
    "    # Filter out one-letter words and those below low_n_cutoff\n",
    "    frequency_filt = {word: freq for word, freq in frequency.items() if len(word) > 1 and freq > low_n_cutoff}\n",
    "    \n",
    "    # Remove high-frequency words if high_sd_cutoff is specified\n",
    "    if high_sd_cutoff is not None:\n",
    "        mean_freq = np.mean(list(frequency_filt.values()))\n",
    "        std_freq = np.std(list(frequency_filt.values()))\n",
    "        cutoff_freq = mean_freq + (std_freq * high_sd_cutoff)\n",
    "        filteredWords = {word: freq for word, freq in frequency_filt.items() if freq < cutoff_freq}\n",
    "    else:\n",
    "        filteredWords = frequency_filt\n",
    "    \n",
    "    # Convert to DataFrames for exporting\n",
    "    vocabfreq_all = pd.DataFrame(frequency.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
    "    vocabfreq_filt = pd.DataFrame(filteredWords.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
    "    \n",
    "    # Save to files\n",
    "    vocabfreq_all.to_csv(os.path.join(output_file_directory, 'vocab_unfilt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
    "    vocabfreq_filt.to_csv(os.path.join(output_file_directory, 'vocab_filt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
    "    \n",
    "    return list(frequency.keys()), list(filteredWords.keys())\n",
    "\n",
    "def aggregate_conversations(folder_path: str) -> pd.DataFrame:\n",
    "    text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "    \n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    for file_name in text_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"data/prepped_stan_small\"\n",
    "    output_file_directory = \"output\"\n",
    "    \n",
    "    # Aggregate individual conversation files\n",
    "    concatenated_df = aggregate_conversations(folder_path)\n",
    "    \n",
    "    # Build filtered vocabulary from aggregated data\n",
    "    vocab_all, vocab_filtered = build_filtered_vocab(concatenated_df, output_file_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
