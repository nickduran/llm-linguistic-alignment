{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import ast\n",
    "\n",
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import gensim \n",
    "import gensim.downloader as api \n",
    "from gensim.models import KeyedVectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 0: Download and Cache the Models Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cache directory: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data\n",
      "Gensim BASE_DIR set to: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data\n",
      "Model word2vec-google-news-300 already exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/word2vec-google-news-300\n",
      "Model glove-twitter-200 already exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/glove-twitter-200\n"
     ]
    }
   ],
   "source": [
    "# Determine the current working directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Define the local cache directory relative to the current working directory\n",
    "local_cache_dir = os.path.join(script_dir, \"gensim-data\")\n",
    "os.makedirs(local_cache_dir, exist_ok=True)\n",
    "print(f\"Local cache directory: {local_cache_dir}\")\n",
    "\n",
    "# Set the BASE_DIR for gensim data\n",
    "api.BASE_DIR = local_cache_dir\n",
    "print(f\"Gensim BASE_DIR set to: {api.BASE_DIR}\")\n",
    "\n",
    "# Function to download and cache models\n",
    "def download_and_cache_models(models, cache_dir):\n",
    "    api.BASE_DIR = cache_dir\n",
    "    for model_name in models:\n",
    "        model_path = os.path.join(cache_dir, model_name)\n",
    "        if not os.path.exists(model_path):\n",
    "            try:\n",
    "                print(f\"Downloading model: {model_name}\")\n",
    "                model = api.load(model_name)\n",
    "                print(f\"Downloaded and cached model: {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {model_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Model {model_name} already exists at: {model_path}\")\n",
    "\n",
    "# List of models to download and cache\n",
    "models_to_cache = ['word2vec-google-news-300', 'glove-twitter-200']\n",
    "download_and_cache_models(models_to_cache, local_cache_dir)\n",
    "\n",
    "# note, is it more efficient to use gensim.downloader.load(model_name)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_conversations(folder_path: str) -> pd.DataFrame:\n",
    "    text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "    \n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    for file_name in text_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "    \n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filtered_vocab(data: pd.DataFrame,\n",
    "                         output_file_directory: str,\n",
    "                         high_sd_cutoff: float = 3,\n",
    "                         low_n_cutoff: int = 1):\n",
    "    \n",
    "    # Tokenize the lemmas\n",
    "    all_sentences = [re.sub(r'[^\\w\\s]+', '', str(row)).split() for row in data['lemma']]\n",
    "    all_words = [word for sentence in all_sentences for word in sentence]\n",
    "    \n",
    "    # Frequency count using Counter\n",
    "    frequency = Counter(all_words)\n",
    "    \n",
    "    # Filter out one-letter words and those below low_n_cutoff\n",
    "    frequency_filt = {word: freq for word, freq in frequency.items() if len(word) > 1 and freq > low_n_cutoff}\n",
    "    \n",
    "    # Remove high-frequency words if high_sd_cutoff is specified\n",
    "    if high_sd_cutoff is not None:\n",
    "        mean_freq = np.mean(list(frequency_filt.values()))\n",
    "        std_freq = np.std(list(frequency_filt.values()))\n",
    "        cutoff_freq = mean_freq + (std_freq * high_sd_cutoff)\n",
    "        filteredWords = {word: freq for word, freq in frequency_filt.items() if freq < cutoff_freq}\n",
    "    else:\n",
    "        filteredWords = frequency_filt\n",
    "    \n",
    "    # Convert to DataFrames for exporting\n",
    "    vocabfreq_all = pd.DataFrame(frequency.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
    "    vocabfreq_filt = pd.DataFrame(filteredWords.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
    "    \n",
    "    # Save to files\n",
    "    vocabfreq_all.to_csv(os.path.join(output_file_directory, 'vocab_unfilt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
    "    vocabfreq_filt.to_csv(os.path.join(output_file_directory, 'vocab_filt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
    "    \n",
    "    return list(frequency.keys()), list(filteredWords.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get lagged conversational turns, restructure dataframe\n",
    "def process_input_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['utter1'] = df['content']\n",
    "    df['utter2'] = df['content'].shift(-1)\n",
    "    df['utter_order'] = df['participant'] + ' ' + df['participant'].shift(-1)\n",
    "    return df\n",
    "\n",
    "### turn the following into functions:\n",
    "\n",
    "# Load the models from the local cache when needed\n",
    "try:\n",
    "    w2v_google_model_path = os.path.join(local_cache_dir, 'word2vec-google-news-300', 'word2vec-google-news-300.gz')\n",
    "    w2v_google_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_google_model_path, binary=True)\n",
    "    print(\"Word2Vec Google News model loaded from local cache successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading word2vec-google-news-300: {e}\")\n",
    "\n",
    "# Load the models from the local cache when needed\n",
    "try:\n",
    "    w2v_twitter_model_path = os.path.join(local_cache_dir, 'glove-twitter-200', 'glove-twitter-200.gz')\n",
    "    w2v_twitter_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_twitter_model_path, binary=False)\n",
    "    print(\"Word2Vec Twitter model loaded from local cache successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading glove-twitter-200: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: TODO NEXT!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Might need to set up a function here to do checks if word exists in each model?\n",
    "\n",
    "# Function to process and get embeddings/cosines for a single file\n",
    "# def process_file(file_path, embedding_cache, default_embedding_engine):       \n",
    "def process_file(file_path, large_list: pd.DataFrame):       \n",
    "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "    df = process_input_data(df)\n",
    "    ######### ABOVE IS FROM MAIN CODE\n",
    "\n",
    "    filter_model = [word for word in large_list if w2v_google_model.has_index_for(word)]\n",
    "\n",
    "\n",
    "    df[\"token\"] = df[\"token\"].apply(ast.literal_eval)    \n",
    "\n",
    "    filter_df = [word for token_list in df[\"token\"] for word in token_list if word in large_list]\n",
    "    print(filter_df)\n",
    "\n",
    "\n",
    "    # all_words = [word for sentence in all_sentences for word in sentence]\n",
    "\n",
    "    # Convert string representations of lists to actual lists\n",
    "    # concatenated_df[\"token\"] = concatenated_df[\"token\"].apply(ast.literal_eval)\n",
    "\n",
    "    # Print the first few entries in the \"token\" column to verify conversion\n",
    "    # print(\"First few entries in 'lemma' column after conversion:\")\n",
    "    # print(filtered_words[\"lemma\"].head())\n",
    "\n",
    "    # # Print the type of the first few entries to ensure they are now lists\n",
    "    # print(\"\\nTypes of the first few entries in 'token' column after conversion:\")\n",
    "    # print(words[\"lemma\"].apply(type).head())\n",
    "\n",
    "    # # Flatten the lists of tokens in the \"token\" column and filter the tokens\n",
    "    # filter_vocablist = [word for token_list in words[\"token\"] for word in token_list if word in large_list]\n",
    "\n",
    "    # print(\"Filtered vocabulary\")\n",
    "\n",
    "\n",
    "\n",
    "    # return df ## <<< this needs to be updated, this assumes a dataframe to be processed over\n",
    "    # Create column of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_model = [word for word in filter_vocablist if w2v_google_model.has_index_for(word)]\n",
    "\n",
    "word_vectors = [w2v_google_model[word] for word in filter_model if w2v_google_model.has_index_for(word)]\n",
    "vector_avg = np.mean(word_vectors, axis=0)\n",
    "vector_sum = np.sum(word_vectors, axis=0)\n",
    "vector1_norm = vector_sum / np.linalg.norm(vector_sum)\n",
    "similarity = cosine_similarity([vector1_norm], [vector1_norm])\n",
    "similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN EVERYTHING BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2/2 [00:00<00:00, 417.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'maybe', 'that', 'would', 'do', 'something', 'think', 'you', 'have', 'maybe', 'close', 'and', 'will', 'you', 'not', 'close', 'the', 'all', 'the', 'start', 'this', 'like', 'down', 'there', 'yeah', 'would', 'just', 'start', 'okay', 'now', 'what', 'you', 'not', 'close', 'yeah', 'but', 'what', 'if', 'close', 'we', 'could', 'make', 'another', 'and', 'could', 'swing', 'up', 'okay', 'okay', 'you', 'on', 'na', 'have', 'swing', 'up', 'well', 'do', 'not', 'really', 'know', 'how', 'we', 'on', 'na', 'get', 'swing', 'though', 'because', 'on', 'na', 'heavy', 'maybe', 'pin', 'like', 'that', 'blue', 'wave', 'this', 'one', 'no', 'the', 'blue', 'wave', 'the', 'balloon', 'well', 'there', 'blue', 'wave', 'or', 'mean', 'there', 'pin', 'on', 'the', 'blue', 'wave', 'not', 'though', 'yeah', 'look', 'this', 'one', 'on', 'not', 'the', 'blow', 'okay', 'think', 'you', 'need', 'what', 'you', 'say', 'pin', 'the', 'blow', 'the', 'blue', 'wave', 'this', 'do', 'not', 'think', 'can', 'attach', 'pin', 'like', 'do', 'not', 'think', 'on', 'na', 'do', 'well', 'we', 'need', 'get', 'swing', 'so', 'now', 'we', 'have', 'ball', 'drop', 'something', 'heavy', 'there', 'we', 'go', 'no', 'okay', 'now', 'that', 'there', 'okay', 'what', 'if', 'we', 'do', 'do', 'not', 'really', 'know', 'what', 'do', 'would', 'would', 'just', 'try', 'and', 'draw', 'something', 'drop', 'something', 'really', 'really', 'really', 'heavy', 'the', 'thing', 'there', 'no', 'like', 'on', 'right', 'make', 'like', 'on', 'the', 'on', 'the', 'like', 'that', 'like', 'like', 'like', 'what', 'you', 'yeah', 'and', 'then', 'drop', 'on', 'na', 'have', 'okay', 'just', 'if', 'you', 'can', 'move', 'the', 'balloon', 'you', 'drop', 'something', 'like', 'on', 'the', 'balloon', 'and', 'make', 'move', 'we', 'can', 'try', 'that', 'maybe', 'that', 'that', 'like', 'good', 'okay', 'let', 'draw', 'something', 'balloon', 'can', 'not', 'maybe', 'if', 'we', 'just', 'go', 'and', 'okay', 'that', 'not', 'really', 'what', 'but', 'maybe', 'dolphin', 'because', 'you', 'know', 'how', 'dolphin', 'like', 'bounce', 'each', 'other', 'maybe', 'we', 'bounce', 'from', 'like', 'one', 'dolphin', 'another', 'and', 'then', 'get', 'the', 'balloon', 'how', 'would', 'we', 'even', 'do', 'that', 'like', 'from', 'think', 'we', 'will', 'have', 'make', 'or', 'something', 'the', 'one', 'the', 'one', 'do', 'not', 'know', 'yeah', 'something', 'like', 'that', 'okay', 'would', 'not', 'each', 'other', 'wait', 'but', 'even', 'if', 'we', 'that', 'that', 'might', 'okay', 'and', 'then', 'if', 'we', 'could', 'jump', 'if', 'we', 'another', 'thing', 'like', 'that', 'if', 'we', 'could', 'jump', 'up', 'the', 'one', 'okay', 'so']\n",
      "['yeah', 'let', 'try', 'that', 'one', 'this', 'one', 'yeah', 'that', 'one', 'not', 'look', 'something', 'like', 'that', 'the', 'all', 'yeah', 'just', 'make', 'thing', 'like', 'like', 'last', 'time', 'yeah', 'how', 'this', 'from', 'last', 'time', 'do', 'not', 'know', 'wait', 'now', 'try', 'and', 'the', 'thing', 'and', 'if', 'just', 'down', 'think', 'swing', 'the', 'right', 'like', 'up', 'or', 'down', 'yeah', 'but', 'think', 'might', 'have', 'now', 'if', 'you', 'the', 'think', 'we', 'gold', 'one', 'on', 'do', 'not', 'know', 'what', 'that', 'but', 'gold', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the folder containing the text files\n",
    "folder_path = \"./data/prepped_stan_small\"\n",
    "output_file_directory = \"output\"\n",
    "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "\n",
    "# Aggregate individual conversation files\n",
    "concatenated_text_files = aggregate_conversations(folder_path)\n",
    "    \n",
    "# Build filtered vocabulary from aggregated data\n",
    "vocab_all, vocab_filtered = build_filtered_vocab(concatenated_text_files, output_file_directory)\n",
    "\n",
    "# Process each file and update the cache\n",
    "concatenated_df = pd.DataFrame()\n",
    "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = process_file(file_path, vocab_filtered)\n",
    "    # concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   participant                                            content  \\\n",
      "0          PC:                            yeah let's try that one   \n",
      "1          PB:                                           this one   \n",
      "2          PA:                 yeah that one doesn't look too bad   \n",
      "3          PB:                                  dropped something   \n",
      "4          PA:    i feel like that's the solution to all of these   \n",
      "5          PB:   boy yeah just make a thing like a weight like...   \n",
      "6          PC:      yeah how is this any different from last time   \n",
      "7          PA:                                         don't know   \n",
      "8          PC:  k wait now try and delete the red thing and se...   \n",
      "9          PA:  i think it needs to swing out to the right sid...   \n",
      "10         PC:    yeah but i think it might have gotten stuck now   \n",
      "11         PA:    if you hit the space bar i think it restarts it   \n",
      "12         PC:                                  we got a gold one   \n",
      "13         PB:                                      sweet come on   \n",
      "14         PA:      i don't know what that means but gold is good   \n",
      "\n",
      "                                                token  \\\n",
      "0         ['yeah', 'let', 'us', 'try', 'that', 'one']   \n",
      "1                                     ['this', 'one']   \n",
      "2   ['yeah', 'that', 'one', 'does', 'not', 'look',...   \n",
      "3                            ['dropped', 'something']   \n",
      "4   ['i', 'feel', 'like', 'that', 'is', 'the', 'so...   \n",
      "5   ['boy', 'yeah', 'just', 'make', 'a', 'thing', ...   \n",
      "6   ['yeah', 'how', 'is', 'this', 'any', 'differen...   \n",
      "7                               ['do', 'not', 'know']   \n",
      "8   ['k', 'wait', 'now', 'try', 'and', 'delete', '...   \n",
      "9   ['i', 'think', 'it', 'needs', 'to', 'swing', '...   \n",
      "10  ['yeah', 'but', 'i', 'think', 'it', 'might', '...   \n",
      "11  ['if', 'you', 'hit', 'the', 'space', 'bar', 'i...   \n",
      "12                  ['we', 'got', 'a', 'gold', 'one']   \n",
      "13                            ['sweet', 'come', 'on']   \n",
      "14  ['i', 'do', 'not', 'know', 'what', 'that', 'me...   \n",
      "\n",
      "                                                lemma  \\\n",
      "0                         [yeah, let, try, that, one]   \n",
      "1                                         [this, one]   \n",
      "2                    [yeah, that, one, do, not, look]   \n",
      "3                                   [drop, something]   \n",
      "4                              [like, that, the, all]   \n",
      "5   [yeah, just, make, thing, like, like, last, time]   \n",
      "6                 [yeah, how, this, from, last, time]   \n",
      "7                                     [do, not, know]   \n",
      "8   [wait, now, try, and, the, thing, and, if, jus...   \n",
      "9   [think, need, swing, the, right, like, go, up,...   \n",
      "10          [yeah, but, think, might, have, get, now]   \n",
      "11                       [if, you, the, think, start]   \n",
      "12                               [we, get, gold, one]   \n",
      "13                                               [on]   \n",
      "14  [do, not, know, what, that, mean, but, gold, g...   \n",
      "\n",
      "                                         tagged_token  \\\n",
      "0   [('yeah', 'NN'), ('let', 'VBD'), ('us', 'PRP')...   \n",
      "1                     [('this', 'DT'), ('one', 'NN')]   \n",
      "2   [('yeah', 'NN'), ('that', 'WDT'), ('one', 'CD'...   \n",
      "3           [('dropped', 'VBD'), ('something', 'NN')]   \n",
      "4   [('i', 'JJ'), ('feel', 'VBP'), ('like', 'IN'),...   \n",
      "5   [('boy', 'NN'), ('yeah', 'NN'), ('just', 'RB')...   \n",
      "6   [('yeah', 'VB'), ('how', 'WRB'), ('is', 'VBZ')...   \n",
      "7       [('do', 'VB'), ('not', 'RB'), ('know', 'VB')]   \n",
      "8   [('k', 'NNS'), ('wait', 'VBP'), ('now', 'RB'),...   \n",
      "9   [('i', 'NN'), ('think', 'VBP'), ('it', 'PRP'),...   \n",
      "10  [('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...   \n",
      "11  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
      "12  [('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...   \n",
      "13    [('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]   \n",
      "14  [('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...   \n",
      "\n",
      "                                         tagged_lemma  \\\n",
      "0   [('yeah', 'NNS'), ('let', 'VBP'), ('u', 'JJ'),...   \n",
      "1                     [('this', 'DT'), ('one', 'NN')]   \n",
      "2   [('yeah', 'NN'), ('that', 'WDT'), ('one', 'CD'...   \n",
      "3               [('drop', 'NN'), ('something', 'NN')]   \n",
      "4   [('i', 'JJ'), ('feel', 'VBP'), ('like', 'IN'),...   \n",
      "5   [('boy', 'NN'), ('yeah', 'NN'), ('just', 'RB')...   \n",
      "6   [('yeah', 'VB'), ('how', 'WRB'), ('be', 'VB'),...   \n",
      "7       [('do', 'VB'), ('not', 'RB'), ('know', 'VB')]   \n",
      "8   [('k', 'NNS'), ('wait', 'VBP'), ('now', 'RB'),...   \n",
      "9   [('i', 'NN'), ('think', 'VBP'), ('it', 'PRP'),...   \n",
      "10  [('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...   \n",
      "11  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
      "12  [('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...   \n",
      "13    [('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]   \n",
      "14  [('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...   \n",
      "\n",
      "                                    tagged_stan_token  \\\n",
      "0   [('yeah', 'JJ'), ('let', 'VB'), ('us', 'PRP'),...   \n",
      "1                     [('this', 'DT'), ('one', 'CD')]   \n",
      "2   [('yeah', 'NN'), ('that', 'IN'), ('one', 'CD')...   \n",
      "3           [('dropped', 'VBD'), ('something', 'NN')]   \n",
      "4   [('i', 'LS'), ('feel', 'VB'), ('like', 'IN'), ...   \n",
      "5   [('boy', 'NN'), ('yeah', 'RB'), ('just', 'RB')...   \n",
      "6   [('yeah', 'VB'), ('how', 'WRB'), ('is', 'VBZ')...   \n",
      "7       [('do', 'VB'), ('not', 'RB'), ('know', 'VB')]   \n",
      "8   [('k', 'NN'), ('wait', 'NN'), ('now', 'RB'), (...   \n",
      "9   [('i', 'LS'), ('think', 'VB'), ('it', 'PRP'), ...   \n",
      "10  [('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...   \n",
      "11  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
      "12  [('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...   \n",
      "13   [('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]   \n",
      "14  [('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...   \n",
      "\n",
      "                                    tagged_stan_lemma  \\\n",
      "0   [('yeah', 'JJ'), ('let', 'VB'), ('u', 'FW'), (...   \n",
      "1                     [('this', 'DT'), ('one', 'CD')]   \n",
      "2   [('yeah', 'NN'), ('that', 'IN'), ('one', 'CD')...   \n",
      "3               [('drop', 'NN'), ('something', 'NN')]   \n",
      "4   [('i', 'LS'), ('feel', 'VB'), ('like', 'IN'), ...   \n",
      "5   [('boy', 'NN'), ('yeah', 'RB'), ('just', 'RB')...   \n",
      "6   [('yeah', 'VB'), ('how', 'WRB'), ('be', 'VB'),...   \n",
      "7       [('do', 'VB'), ('not', 'RB'), ('know', 'VB')]   \n",
      "8   [('k', 'NN'), ('wait', 'NN'), ('now', 'RB'), (...   \n",
      "9   [('i', 'LS'), ('think', 'VB'), ('it', 'PRP'), ...   \n",
      "10  [('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...   \n",
      "11  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
      "12  [('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...   \n",
      "13   [('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]   \n",
      "14  [('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...   \n",
      "\n",
      "                                file  \\\n",
      "0   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "1   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "2   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "3   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "4   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "5   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "6   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "7   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "8   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "9   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "10  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "11  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "12  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "13  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "14  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "\n",
      "                                               utter1  \\\n",
      "0                             yeah let's try that one   \n",
      "1                                            this one   \n",
      "2                  yeah that one doesn't look too bad   \n",
      "3                                   dropped something   \n",
      "4     i feel like that's the solution to all of these   \n",
      "5    boy yeah just make a thing like a weight like...   \n",
      "6       yeah how is this any different from last time   \n",
      "7                                          don't know   \n",
      "8   k wait now try and delete the red thing and se...   \n",
      "9   i think it needs to swing out to the right sid...   \n",
      "10    yeah but i think it might have gotten stuck now   \n",
      "11    if you hit the space bar i think it restarts it   \n",
      "12                                  we got a gold one   \n",
      "13                                      sweet come on   \n",
      "14      i don't know what that means but gold is good   \n",
      "\n",
      "                                               utter2 utter_order  \n",
      "0                                            this one     PC: PB:  \n",
      "1                  yeah that one doesn't look too bad     PB: PA:  \n",
      "2                                   dropped something     PA: PB:  \n",
      "3     i feel like that's the solution to all of these     PB: PA:  \n",
      "4    boy yeah just make a thing like a weight like...     PA: PB:  \n",
      "5       yeah how is this any different from last time     PB: PC:  \n",
      "6                                          don't know     PC: PA:  \n",
      "7   k wait now try and delete the red thing and se...     PA: PC:  \n",
      "8   i think it needs to swing out to the right sid...     PC: PA:  \n",
      "9     yeah but i think it might have gotten stuck now     PA: PC:  \n",
      "10    if you hit the space bar i think it restarts it     PC: PA:  \n",
      "11                                  we got a gold one     PA: PC:  \n",
      "12                                      sweet come on     PC: PB:  \n",
      "13      i don't know what that means but gold is good     PB: PA:  \n",
      "14                                               None         NaN  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Retrieve the Word2Vec vectors for each word in the sentence; ignores any words not in the pre-trained model vocabulary\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m [w2v_google_model[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mw2v_google_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_index_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# word_vectors = [w2v_google_model[word] for word in filter_model if w2v_google_model.has_index_for(word)]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:646\u001b[0m, in \u001b[0;36mKeyedVectors.has_index_for\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_index_for\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can this model return a single index for this key?\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    Subclasses that synthesize vectors for out-of-vocabulary words (like\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m \n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:412\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the integer index (slot/position) where the given key's vector is stored in the\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m    backing vectors array.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_to_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "df = process_input_data(df)\n",
    "\n",
    "# Convert 'token' column to list of words\n",
    "df['lemma'] = df['lemma'].apply(ast.literal_eval)\n",
    "\n",
    "# Large list of words to check against \n",
    "vocab_filtered # Use a set for faster look-up\n",
    "\n",
    "def filter_words(token_list):\n",
    "    return [word for word in token_list if word in vocab_filtered]\n",
    "\n",
    "# Apply the filtering function to the 'token' column\n",
    "df['lemma'] = df['lemma'].apply(filter_words)\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "    \n",
    "# Retrieve the Word2Vec vectors for each word in the sentence; ignores any words not in the pre-trained model vocabulary\n",
    "word_vectors = [w2v_google_model[word] for word in df['lemma'] if w2v_google_model.has_index_for(word)]\n",
    "\n",
    "\n",
    "\n",
    "# word_vectors = [w2v_google_model[word] for word in filter_model if w2v_google_model.has_index_for(word)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   participant                                            content  \\\n",
      "0          PC:                            yeah let's try that one   \n",
      "1          PB:                                           this one   \n",
      "2          PA:                 yeah that one doesn't look too bad   \n",
      "3          PB:                                  dropped something   \n",
      "4          PA:    i feel like that's the solution to all of these   \n",
      "5          PB:   boy yeah just make a thing like a weight like...   \n",
      "6          PC:      yeah how is this any different from last time   \n",
      "7          PA:                                         don't know   \n",
      "8          PC:  k wait now try and delete the red thing and se...   \n",
      "9          PA:  i think it needs to swing out to the right sid...   \n",
      "10         PC:    yeah but i think it might have gotten stuck now   \n",
      "11         PA:    if you hit the space bar i think it restarts it   \n",
      "12         PC:                                  we got a gold one   \n",
      "13         PB:                                      sweet come on   \n",
      "14         PA:      i don't know what that means but gold is good   \n",
      "\n",
      "                                                token  \\\n",
      "0         ['yeah', 'let', 'us', 'try', 'that', 'one']   \n",
      "1                                     ['this', 'one']   \n",
      "2   ['yeah', 'that', 'one', 'does', 'not', 'look',...   \n",
      "3                            ['dropped', 'something']   \n",
      "4   ['i', 'feel', 'like', 'that', 'is', 'the', 'so...   \n",
      "5   ['boy', 'yeah', 'just', 'make', 'a', 'thing', ...   \n",
      "6   ['yeah', 'how', 'is', 'this', 'any', 'differen...   \n",
      "7                               ['do', 'not', 'know']   \n",
      "8   ['k', 'wait', 'now', 'try', 'and', 'delete', '...   \n",
      "9   ['i', 'think', 'it', 'needs', 'to', 'swing', '...   \n",
      "10  ['yeah', 'but', 'i', 'think', 'it', 'might', '...   \n",
      "11  ['if', 'you', 'hit', 'the', 'space', 'bar', 'i...   \n",
      "12                  ['we', 'got', 'a', 'gold', 'one']   \n",
      "13                            ['sweet', 'come', 'on']   \n",
      "14  ['i', 'do', 'not', 'know', 'what', 'that', 'me...   \n",
      "\n",
      "                                                lemma  \\\n",
      "0                         [yeah, let, try, that, one]   \n",
      "1                                         [this, one]   \n",
      "2                    [yeah, that, one, do, not, look]   \n",
      "3                                   [drop, something]   \n",
      "4                              [like, that, the, all]   \n",
      "5   [yeah, just, make, thing, like, like, last, time]   \n",
      "6                 [yeah, how, this, from, last, time]   \n",
      "7                                     [do, not, know]   \n",
      "8   [wait, now, try, and, the, thing, and, if, jus...   \n",
      "9   [think, need, swing, the, right, like, go, up,...   \n",
      "10          [yeah, but, think, might, have, get, now]   \n",
      "11                       [if, you, the, think, start]   \n",
      "12                               [we, get, gold, one]   \n",
      "13                                               [on]   \n",
      "14  [do, not, know, what, that, mean, but, gold, g...   \n",
      "\n",
      "                                         tagged_token  \\\n",
      "0   [('yeah', 'NN'), ('let', 'VBD'), ('us', 'PRP')...   \n",
      "1                     [('this', 'DT'), ('one', 'NN')]   \n",
      "2   [('yeah', 'NN'), ('that', 'WDT'), ('one', 'CD'...   \n",
      "3           [('dropped', 'VBD'), ('something', 'NN')]   \n",
      "4   [('i', 'JJ'), ('feel', 'VBP'), ('like', 'IN'),...   \n",
      "5   [('boy', 'NN'), ('yeah', 'NN'), ('just', 'RB')...   \n",
      "6   [('yeah', 'VB'), ('how', 'WRB'), ('is', 'VBZ')...   \n",
      "7       [('do', 'VB'), ('not', 'RB'), ('know', 'VB')]   \n",
      "8   [('k', 'NNS'), ('wait', 'VBP'), ('now', 'RB'),...   \n",
      "9   [('i', 'NN'), ('think', 'VBP'), ('it', 'PRP'),...   \n",
      "10  [('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...   \n",
      "11  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
      "12  [('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...   \n",
      "13    [('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]   \n",
      "14  [('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...   \n",
      "\n",
      "                                         tagged_lemma  \\\n",
      "0   [('yeah', 'NNS'), ('let', 'VBP'), ('u', 'JJ'),...   \n",
      "1                     [('this', 'DT'), ('one', 'NN')]   \n",
      "2   [('yeah', 'NN'), ('that', 'WDT'), ('one', 'CD'...   \n",
      "3               [('drop', 'NN'), ('something', 'NN')]   \n",
      "4   [('i', 'JJ'), ('feel', 'VBP'), ('like', 'IN'),...   \n",
      "5   [('boy', 'NN'), ('yeah', 'NN'), ('just', 'RB')...   \n",
      "6   [('yeah', 'VB'), ('how', 'WRB'), ('be', 'VB'),...   \n",
      "7       [('do', 'VB'), ('not', 'RB'), ('know', 'VB')]   \n",
      "8   [('k', 'NNS'), ('wait', 'VBP'), ('now', 'RB'),...   \n",
      "9   [('i', 'NN'), ('think', 'VBP'), ('it', 'PRP'),...   \n",
      "10  [('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...   \n",
      "11  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
      "12  [('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...   \n",
      "13    [('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]   \n",
      "14  [('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...   \n",
      "\n",
      "                                    tagged_stan_token  \\\n",
      "0   [('yeah', 'JJ'), ('let', 'VB'), ('us', 'PRP'),...   \n",
      "1                     [('this', 'DT'), ('one', 'CD')]   \n",
      "2   [('yeah', 'NN'), ('that', 'IN'), ('one', 'CD')...   \n",
      "3           [('dropped', 'VBD'), ('something', 'NN')]   \n",
      "4   [('i', 'LS'), ('feel', 'VB'), ('like', 'IN'), ...   \n",
      "5   [('boy', 'NN'), ('yeah', 'RB'), ('just', 'RB')...   \n",
      "6   [('yeah', 'VB'), ('how', 'WRB'), ('is', 'VBZ')...   \n",
      "7       [('do', 'VB'), ('not', 'RB'), ('know', 'VB')]   \n",
      "8   [('k', 'NN'), ('wait', 'NN'), ('now', 'RB'), (...   \n",
      "9   [('i', 'LS'), ('think', 'VB'), ('it', 'PRP'), ...   \n",
      "10  [('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...   \n",
      "11  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
      "12  [('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...   \n",
      "13   [('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]   \n",
      "14  [('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...   \n",
      "\n",
      "                                    tagged_stan_lemma  \\\n",
      "0   [('yeah', 'JJ'), ('let', 'VB'), ('u', 'FW'), (...   \n",
      "1                     [('this', 'DT'), ('one', 'CD')]   \n",
      "2   [('yeah', 'NN'), ('that', 'IN'), ('one', 'CD')...   \n",
      "3               [('drop', 'NN'), ('something', 'NN')]   \n",
      "4   [('i', 'LS'), ('feel', 'VB'), ('like', 'IN'), ...   \n",
      "5   [('boy', 'NN'), ('yeah', 'RB'), ('just', 'RB')...   \n",
      "6   [('yeah', 'VB'), ('how', 'WRB'), ('be', 'VB'),...   \n",
      "7       [('do', 'VB'), ('not', 'RB'), ('know', 'VB')]   \n",
      "8   [('k', 'NN'), ('wait', 'NN'), ('now', 'RB'), (...   \n",
      "9   [('i', 'LS'), ('think', 'VB'), ('it', 'PRP'), ...   \n",
      "10  [('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...   \n",
      "11  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
      "12  [('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...   \n",
      "13   [('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]   \n",
      "14  [('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...   \n",
      "\n",
      "                                file  \\\n",
      "0   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "1   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "2   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "3   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "4   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "5   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "6   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "7   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "8   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "9   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "10  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "11  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "12  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "13  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "14  ASU-T10_ExpBlock1-Oneatatime.txt   \n",
      "\n",
      "                                               utter1  \\\n",
      "0                             yeah let's try that one   \n",
      "1                                            this one   \n",
      "2                  yeah that one doesn't look too bad   \n",
      "3                                   dropped something   \n",
      "4     i feel like that's the solution to all of these   \n",
      "5    boy yeah just make a thing like a weight like...   \n",
      "6       yeah how is this any different from last time   \n",
      "7                                          don't know   \n",
      "8   k wait now try and delete the red thing and se...   \n",
      "9   i think it needs to swing out to the right sid...   \n",
      "10    yeah but i think it might have gotten stuck now   \n",
      "11    if you hit the space bar i think it restarts it   \n",
      "12                                  we got a gold one   \n",
      "13                                      sweet come on   \n",
      "14      i don't know what that means but gold is good   \n",
      "\n",
      "                                               utter2 utter_order  \\\n",
      "0                                            this one     PC: PB:   \n",
      "1                  yeah that one doesn't look too bad     PB: PA:   \n",
      "2                                   dropped something     PA: PB:   \n",
      "3     i feel like that's the solution to all of these     PB: PA:   \n",
      "4    boy yeah just make a thing like a weight like...     PA: PB:   \n",
      "5       yeah how is this any different from last time     PB: PC:   \n",
      "6                                          don't know     PC: PA:   \n",
      "7   k wait now try and delete the red thing and se...     PA: PC:   \n",
      "8   i think it needs to swing out to the right sid...     PC: PA:   \n",
      "9     yeah but i think it might have gotten stuck now     PA: PC:   \n",
      "10    if you hit the space bar i think it restarts it     PC: PA:   \n",
      "11                                  we got a gold one     PA: PC:   \n",
      "12                                      sweet come on     PC: PB:   \n",
      "13      i don't know what that means but gold is good     PB: PA:   \n",
      "14                                               None         NaN   \n",
      "\n",
      "                                           embeddings  \\\n",
      "0   [[0.24707031, -0.045654297, -0.076660156, 0.33...   \n",
      "1   [[0.109375, 0.140625, -0.03173828, 0.16601562,...   \n",
      "2   [[0.24707031, -0.045654297, -0.076660156, 0.33...   \n",
      "3   [[-0.04272461, -0.1796875, 0.025390625, 0.1601...   \n",
      "4   [[0.103515625, 0.13769531, -0.0029754639, 0.18...   \n",
      "5   [[0.24707031, -0.045654297, -0.076660156, 0.33...   \n",
      "6   [[0.24707031, -0.045654297, -0.076660156, 0.33...   \n",
      "7   [[0.123046875, 0.012817383, 0.01940918, 0.2304...   \n",
      "8   [[0.050048828, 0.15429688, 0.15039062, 0.16992...   \n",
      "9   [[-0.046875, 0.06689453, 0.009338379, 0.263671...   \n",
      "10  [[0.24707031, -0.045654297, -0.076660156, 0.33...   \n",
      "11  [[0.10986328, -0.08496094, 0.11669922, 0.12695...   \n",
      "12  [[-0.017944336, 0.1171875, 0.052734375, 0.2558...   \n",
      "13  [[0.026733398, -0.09082031, 0.027832031, 0.204...   \n",
      "14  [[0.123046875, 0.012817383, 0.01940918, 0.2304...   \n",
      "\n",
      "                                       mean_embedding  \\\n",
      "0   [0.14504394, 0.004296875, 0.115039065, 0.19990...   \n",
      "1   [0.07751465, -0.0024414062, 0.06225586, 0.1660...   \n",
      "2   [0.05234782, -0.020365397, 0.054097492, 0.1604...   \n",
      "3   [0.03479004, -0.08068848, 0.05126953, 0.139404...   \n",
      "4   [0.040008545, 0.04660034, 0.042713165, 0.10076...   \n",
      "5   [0.088378906, 0.062236786, 0.017986298, 0.1358...   \n",
      "6   [0.12988281, 0.07341655, 0.015757242, 0.120971...   \n",
      "7   [0.05102539, -0.06652832, 0.055460613, 0.13899...   \n",
      "8   [0.10028076, 0.041866302, 0.107803345, 0.13183...   \n",
      "9   [0.077301025, 0.02734375, 0.064222716, 0.14614...   \n",
      "10  [0.042881556, 0.016793387, -0.039995465, 0.194...   \n",
      "11  [0.06254883, 0.040722657, 0.054626465, 0.17182...   \n",
      "12  [-0.03677368, -0.036590576, -0.016021729, 0.17...   \n",
      "13  [0.026733398, -0.09082031, 0.027832031, 0.2041...   \n",
      "14  [6.781684e-05, -0.020731607, 0.07054308, 0.118...   \n",
      "\n",
      "                                        sum_embedding  \n",
      "0   [0.7252197, 0.021484375, 0.5751953, 0.9995117,...  \n",
      "1   [0.1550293, -0.0048828125, 0.12451172, 0.33203...  \n",
      "2   [0.3140869, -0.12219238, 0.32458496, 0.9628906...  \n",
      "3   [0.06958008, -0.16137695, 0.10253906, 0.278808...  \n",
      "4   [0.16003418, 0.18640137, 0.17085266, 0.4030761...  \n",
      "5   [0.70703125, 0.4978943, 0.14389038, 1.086853, ...  \n",
      "6   [0.7792969, 0.4404993, 0.09454346, 0.7258301, ...  \n",
      "7   [0.15307617, -0.19958496, 0.16638184, 0.416992...  \n",
      "8   [0.8022461, 0.33493042, 0.86242676, 1.0546875,...  \n",
      "9   [0.77301025, 0.2734375, 0.6422272, 1.4614258, ...  \n",
      "10  [0.3001709, 0.11755371, -0.27996826, 1.364502,...  \n",
      "11  [0.31274414, 0.20361328, 0.27313232, 0.8591308...  \n",
      "12  [-0.14709473, -0.1463623, -0.064086914, 0.6972...  \n",
      "13  [0.026733398, -0.09082031, 0.027832031, 0.2041...  \n",
      "14  [0.00061035156, -0.18658447, 0.6348877, 1.0620...  \n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "# w2v_google_model_path = 'path/to/GoogleNews-vectors-negative300.bin.gz'\n",
    "# w2v_google_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_google_model_path, binary=True)\n",
    "\n",
    "# Sample DataFrame\n",
    "# data = {\n",
    "#     'token': ['[\"this\", \"is\", \"a\", \"sample\", \"sentence\"]', '[\"word2vec\", \"is\", \"a\", \"powerful\", \"tool\"]', '[\"gensim\", \"makes\", \"it\", \"easy\", \"to\", \"use\"]']\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "# df['lemma']\n",
    "\n",
    "# Function to get embeddings for each word in the list\n",
    "\n",
    "# def get_embeddings(token_list, model):\n",
    "#     embeddings = []\n",
    "#     for word in token_list:\n",
    "#         if word in model.key_to_index:  # Check if word is in the model vocabulary\n",
    "#             embeddings.append(model[word])\n",
    "#         else:\n",
    "#             embeddings.append(None)  # Or handle unknown words as you see fit\n",
    "#     return embeddings\n",
    "\n",
    "# # Apply the function to the 'token' column and store in a new column\n",
    "# df['embeddings'] = df['lemma'].apply(lambda tokens: get_embeddings(tokens, w2v_google_model))\n",
    "\n",
    "# Function to get the mean and sum embeddings for each list of tokens\n",
    "def get_mean_and_sum_embeddings(token_list, model):\n",
    "    embeddings = []\n",
    "    for word in token_list:\n",
    "        if word in model.key_to_index:  # Check if word is in the model vocabulary\n",
    "            embeddings.append(model[word])\n",
    "    if embeddings:\n",
    "        mean_embedding = np.mean(embeddings, axis=0)\n",
    "        sum_embedding = np.sum(embeddings, axis=0)\n",
    "        return mean_embedding, sum_embedding\n",
    "    else:\n",
    "        return None, None  # Or handle empty embeddings as you see fit\n",
    "\n",
    "# Apply the function to the 'token' column and store the results in new columns\n",
    "df['mean_embedding'], df['sum_embedding'] = zip(*df['lemma'].apply(lambda tokens: get_mean_and_sum_embeddings(tokens, w2v_google_model)))\n",
    "\n",
    "\n",
    "# Create column of embeddings\n",
    "for column in [\"utter1\", \"utter2\"]:\n",
    "    df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n",
    "\n",
    "\n",
    "\n",
    "# Print the DataFrame with embeddings\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT SURE IF BELOW IS NECESSARY, JUST FOR PLAYING WITH THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model word2vec-google-news-300 exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "Model glove-twitter-200 exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/glove-twitter-200/glove-twitter-200.gz\n"
     ]
    }
   ],
   "source": [
    "# Verify if models are downloaded\n",
    "for model_name in models_to_cache:\n",
    "    model_file = os.path.join(local_cache_dir, model_name, f\"{model_name}.gz\")\n",
    "    if os.path.exists(model_file):\n",
    "        print(f\"Model {model_name} exists at: {model_file}\")\n",
    "    else:\n",
    "        print(f\"Model {model_name} not found at: {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05844001,  0.01495306,  0.04695388,  0.13069172, -0.06963488,\n",
       "       -0.01979959,  0.05705287, -0.06206347,  0.0486094 ,  0.05643959,\n",
       "       -0.04690355, -0.11485673, -0.04162246, -0.00579954, -0.12551017,\n",
       "        0.07829428,  0.05789276,  0.07433286,  0.03147802, -0.08316714,\n",
       "       -0.04800898,  0.04535313,  0.06810266, -0.03640107,  0.05034975,\n",
       "        0.00141228, -0.06145631,  0.01032026,  0.03499172,  0.01205086,\n",
       "       -0.04704227,  0.07149567, -0.01036917, -0.05731743, -0.004206  ,\n",
       "       -0.01163774,  0.05081906,  0.0198385 ,  0.03804623,  0.08060738,\n",
       "        0.09270204, -0.05585926,  0.16217482, -0.03939044, -0.01891305,\n",
       "        0.00020894, -0.0025041 , -0.02201688,  0.05255189,  0.00180374,\n",
       "       -0.00704225,  0.07698126, -0.01503362, -0.02992011,  0.01114059,\n",
       "        0.03326823, -0.01235591, -0.02948997,  0.07253172, -0.0603702 ,\n",
       "       -0.02808144,  0.06454868, -0.07118034, -0.06671734, -0.01440898,\n",
       "       -0.04135864, -0.04096557,  0.09711972, -0.06773841,  0.06609015,\n",
       "        0.05901062,  0.03698027,  0.0860453 , -0.01061481, -0.17408328,\n",
       "       -0.07249162,  0.083492  ,  0.10902738,  0.02189501,  0.11542562,\n",
       "        0.0115069 , -0.09180438,  0.06309044, -0.00359906, -0.08378497,\n",
       "       -0.07316995, -0.04989561,  0.13055782,  0.00039959, -0.00112925,\n",
       "        0.0609324 ,  0.09305517, -0.09213613, -0.05966665, -0.04461153,\n",
       "       -0.0756686 ,  0.08932877,  0.06987934, -0.01713696, -0.01204445,\n",
       "       -0.07709279, -0.04347704,  0.02951799,  0.03132807, -0.05975898,\n",
       "       -0.02720904, -0.05325386, -0.05499901,  0.01037897, -0.05012985,\n",
       "       -0.05930947, -0.0207986 , -0.02444213,  0.00933356,  0.06674942,\n",
       "       -0.01443806,  0.03941284, -0.02891496,  0.03745139,  0.02116774,\n",
       "       -0.09145622, -0.00690005, -0.03293971,  0.08851399, -0.04996731,\n",
       "       -0.03157711, -0.06019282, -0.04293876, -0.00387185, -0.00262828,\n",
       "       -0.05901735, -0.11251725, -0.07151742, -0.02141273, -0.02388063,\n",
       "       -0.10270573,  0.04259763,  0.03293931,  0.00687067,  0.06858388,\n",
       "        0.06338825, -0.0988382 ,  0.03916019, -0.00691664,  0.0750538 ,\n",
       "        0.03992511, -0.06437819, -0.1284659 , -0.03431477, -0.01502359,\n",
       "        0.04350271,  0.06209371, -0.14114963,  0.04769973, -0.00908652,\n",
       "        0.00503798, -0.06046   , -0.07861729, -0.06767615,  0.03247789,\n",
       "        0.00275758,  0.07018251,  0.01206993,  0.03098986,  0.03972154,\n",
       "       -0.10600728,  0.05038783, -0.03393915,  0.04025044, -0.02951509,\n",
       "       -0.13235295, -0.00232722, -0.0266886 , -0.06281661, -0.04349585,\n",
       "        0.00242884,  0.089253  , -0.07722939, -0.01461452,  0.02818202,\n",
       "       -0.07295427, -0.04398176,  0.0286127 ,  0.03209613, -0.00594395,\n",
       "       -0.02538438, -0.04688651,  0.01886965,  0.07151671,  0.04540423,\n",
       "        0.04275239,  0.03677458,  0.03613431, -0.01890326, -0.01327204,\n",
       "       -0.02232974, -0.02438212,  0.00281144, -0.0668821 , -0.11543698,\n",
       "        0.04146996,  0.05576229, -0.04480213,  0.01823135,  0.02036604,\n",
       "        0.00711968, -0.07125317, -0.049583  ,  0.03483866,  0.02086057,\n",
       "       -0.00723149,  0.076054  , -0.03743295,  0.04437899, -0.11325628,\n",
       "       -0.00801547,  0.13322392, -0.03209046, -0.09444222, -0.00020009,\n",
       "       -0.02802305,  0.00491106, -0.05832369, -0.04660716,  0.07127891,\n",
       "       -0.0734378 ,  0.09167076,  0.0430153 , -0.0075171 ,  0.02417476,\n",
       "        0.01600925, -0.03154902,  0.01062035, -0.01599167,  0.05472233,\n",
       "        0.01003198, -0.04051321, -0.0469824 ,  0.11205263,  0.02124381,\n",
       "        0.03816927,  0.04006802,  0.02407232, -0.07928118, -0.03486553,\n",
       "       -0.01622812,  0.01113869,  0.04821829, -0.00884209, -0.03528327,\n",
       "       -0.00458787,  0.0279514 ,  0.09927736,  0.04396546,  0.07595351,\n",
       "       -0.03614019,  0.04242489,  0.02939621, -0.04600362, -0.09034593,\n",
       "       -0.02856189, -0.01814107, -0.07565161,  0.02868394,  0.02035437,\n",
       "        0.0930039 ,  0.01417481, -0.01276027, -0.06219203,  0.01824915,\n",
       "        0.05913188,  0.09970238,  0.17057954,  0.07566816,  0.0194514 ,\n",
       "       -0.07207964, -0.06437679, -0.12953345, -0.03367762, -0.00632988,\n",
       "       -0.0026794 , -0.01398706,  0.01189567,  0.06192594,  0.01338462,\n",
       "       -0.02606533, -0.05566296, -0.08051451, -0.00113889,  0.03129394,\n",
       "       -0.03892265,  0.0658132 , -0.07245183,  0.0057961 , -0.04260574,\n",
       "       -0.00137295,  0.03473262, -0.04131724,  0.02673827, -0.05861545],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the lists of tokens in the \"token\" column and filter the tokens\n",
    "filter_vocablist = [word for token_list in concatenated_df[\"token\"] for word in token_list if word in vocab_filtered]\n",
    "\n",
    "print(filter_vocablist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tagged_token</th>\n",
       "      <th>tagged_lemma</th>\n",
       "      <th>tagged_stan_token</th>\n",
       "      <th>tagged_stan_lemma</th>\n",
       "      <th>file</th>\n",
       "      <th>utter1</th>\n",
       "      <th>utter2</th>\n",
       "      <th>utter_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC:</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>['i', 'thought', 'that', 'maybe', 'that', 'wou...</td>\n",
       "      <td>['i', 'think', 'that', 'maybe', 'that', 'would...</td>\n",
       "      <td>[('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
       "      <td>[('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
       "      <td>[('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>PC: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PA:</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
       "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
       "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
       "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>PA: PC:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PC:</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>['should', 'i', 'start', 'this', 'it', 'is', '...</td>\n",
       "      <td>['should', 'i', 'start', 'this', 'it', 'be', '...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>PC: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PA:</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
       "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
       "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>PA: PC:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PC:</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>['okay', 'now', 'what']</td>\n",
       "      <td>['okay', 'now', 'what']</td>\n",
       "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>you didn't close it</td>\n",
       "      <td>PC: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>PC:</td>\n",
       "      <td>yeah but i think it might have gotten stuck now</td>\n",
       "      <td>['yeah', 'but', 'i', 'think', 'it', 'might', '...</td>\n",
       "      <td>['yeah', 'but', 'i', 'think', 'it', 'might', '...</td>\n",
       "      <td>[('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...</td>\n",
       "      <td>[('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...</td>\n",
       "      <td>[('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...</td>\n",
       "      <td>[('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>yeah but i think it might have gotten stuck now</td>\n",
       "      <td>if you hit the space bar i think it restarts it</td>\n",
       "      <td>PC: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>PA:</td>\n",
       "      <td>if you hit the space bar i think it restarts it</td>\n",
       "      <td>['if', 'you', 'hit', 'the', 'space', 'bar', 'i...</td>\n",
       "      <td>['if', 'you', 'hit', 'the', 'space', 'bar', 'i...</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>if you hit the space bar i think it restarts it</td>\n",
       "      <td>we got a gold one</td>\n",
       "      <td>PA: PC:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>PC:</td>\n",
       "      <td>we got a gold one</td>\n",
       "      <td>['we', 'got', 'a', 'gold', 'one']</td>\n",
       "      <td>['we', 'get', 'a', 'gold', 'one']</td>\n",
       "      <td>[('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...</td>\n",
       "      <td>[('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...</td>\n",
       "      <td>[('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...</td>\n",
       "      <td>[('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>we got a gold one</td>\n",
       "      <td>sweet come on</td>\n",
       "      <td>PC: PB:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>PB:</td>\n",
       "      <td>sweet come on</td>\n",
       "      <td>['sweet', 'come', 'on']</td>\n",
       "      <td>['sweet', 'come', 'on']</td>\n",
       "      <td>[('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]</td>\n",
       "      <td>[('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]</td>\n",
       "      <td>[('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]</td>\n",
       "      <td>[('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>sweet come on</td>\n",
       "      <td>i don't know what that means but gold is good</td>\n",
       "      <td>PB: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>PA:</td>\n",
       "      <td>i don't know what that means but gold is good</td>\n",
       "      <td>['i', 'do', 'not', 'know', 'what', 'that', 'me...</td>\n",
       "      <td>['i', 'do', 'not', 'know', 'what', 'that', 'me...</td>\n",
       "      <td>[('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...</td>\n",
       "      <td>[('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...</td>\n",
       "      <td>[('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...</td>\n",
       "      <td>[('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>i don't know what that means but gold is good</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant                                            content  \\\n",
       "0          PC:       i thought that maybe that would do something   \n",
       "1          PA:  i think you have to c maybe close it and it wi...   \n",
       "2          PC:         should i restart this it's like down there   \n",
       "3          PA:                       yeah i would just restart it   \n",
       "4          PC:                                    okay now what     \n",
       "..         ...                                                ...   \n",
       "60         PC:    yeah but i think it might have gotten stuck now   \n",
       "61         PA:    if you hit the space bar i think it restarts it   \n",
       "62         PC:                                  we got a gold one   \n",
       "63         PB:                                      sweet come on   \n",
       "64         PA:      i don't know what that means but gold is good   \n",
       "\n",
       "                                                token  \\\n",
       "0   ['i', 'thought', 'that', 'maybe', 'that', 'wou...   \n",
       "1   ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
       "2   ['should', 'i', 'start', 'this', 'it', 'is', '...   \n",
       "3       ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
       "4                             ['okay', 'now', 'what']   \n",
       "..                                                ...   \n",
       "60  ['yeah', 'but', 'i', 'think', 'it', 'might', '...   \n",
       "61  ['if', 'you', 'hit', 'the', 'space', 'bar', 'i...   \n",
       "62                  ['we', 'got', 'a', 'gold', 'one']   \n",
       "63                            ['sweet', 'come', 'on']   \n",
       "64  ['i', 'do', 'not', 'know', 'what', 'that', 'me...   \n",
       "\n",
       "                                                lemma  \\\n",
       "0   ['i', 'think', 'that', 'maybe', 'that', 'would...   \n",
       "1   ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
       "2   ['should', 'i', 'start', 'this', 'it', 'be', '...   \n",
       "3       ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
       "4                             ['okay', 'now', 'what']   \n",
       "..                                                ...   \n",
       "60  ['yeah', 'but', 'i', 'think', 'it', 'might', '...   \n",
       "61  ['if', 'you', 'hit', 'the', 'space', 'bar', 'i...   \n",
       "62                  ['we', 'get', 'a', 'gold', 'one']   \n",
       "63                            ['sweet', 'come', 'on']   \n",
       "64  ['i', 'do', 'not', 'know', 'what', 'that', 'me...   \n",
       "\n",
       "                                         tagged_token  \\\n",
       "0   [('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...   \n",
       "1   [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
       "2   [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
       "3   [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
       "4     [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
       "..                                                ...   \n",
       "60  [('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...   \n",
       "61  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
       "62  [('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...   \n",
       "63    [('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]   \n",
       "64  [('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...   \n",
       "\n",
       "                                         tagged_lemma  \\\n",
       "0   [('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...   \n",
       "1   [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
       "2   [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
       "3   [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
       "4     [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
       "..                                                ...   \n",
       "60  [('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...   \n",
       "61  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
       "62  [('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...   \n",
       "63    [('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]   \n",
       "64  [('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...   \n",
       "\n",
       "                                    tagged_stan_token  \\\n",
       "0   [('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...   \n",
       "1   [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
       "2   [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
       "3   [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
       "4     [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
       "..                                                ...   \n",
       "60  [('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...   \n",
       "61  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
       "62  [('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...   \n",
       "63   [('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]   \n",
       "64  [('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...   \n",
       "\n",
       "                                    tagged_stan_lemma  \\\n",
       "0   [('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...   \n",
       "1   [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
       "2   [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
       "3   [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
       "4     [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
       "..                                                ...   \n",
       "60  [('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...   \n",
       "61  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
       "62  [('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...   \n",
       "63   [('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]   \n",
       "64  [('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...   \n",
       "\n",
       "                                 file  \\\n",
       "0   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "1   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "2   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "3   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "4   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "..                                ...   \n",
       "60   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "61   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "62   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "63   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "64   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "\n",
       "                                               utter1  \\\n",
       "0        i thought that maybe that would do something   \n",
       "1   i think you have to c maybe close it and it wi...   \n",
       "2          should i restart this it's like down there   \n",
       "3                        yeah i would just restart it   \n",
       "4                                     okay now what     \n",
       "..                                                ...   \n",
       "60    yeah but i think it might have gotten stuck now   \n",
       "61    if you hit the space bar i think it restarts it   \n",
       "62                                  we got a gold one   \n",
       "63                                      sweet come on   \n",
       "64      i don't know what that means but gold is good   \n",
       "\n",
       "                                               utter2 utter_order  \n",
       "0   i think you have to c maybe close it and it wi...     PC: PA:  \n",
       "1          should i restart this it's like down there     PA: PC:  \n",
       "2                        yeah i would just restart it     PC: PA:  \n",
       "3                                     okay now what       PA: PC:  \n",
       "4                                 you didn't close it     PC: PA:  \n",
       "..                                                ...         ...  \n",
       "60    if you hit the space bar i think it restarts it     PC: PA:  \n",
       "61                                  we got a gold one     PA: PC:  \n",
       "62                                      sweet come on     PC: PB:  \n",
       "63      i don't know what that means but gold is good     PB: PA:  \n",
       "64                                               None         NaN  \n",
       "\n",
       "[65 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w2v_google_model\n",
    "\n",
    "def get_embedding_with_cache(text):\n",
    "    # Only consider the words that are in the vocablist after filtering for various criteria (e.g., only occur once, high frequency)\n",
    "    filter_vocablist = [word for word in tok_seq if word in vocablist]\n",
    "    \n",
    "df = concatenated_df\n",
    "\n",
    "# Create column of embeddings\n",
    "for column in [\"utter1\", \"utter2\"]:\n",
    "    df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v_trained(pretrained_input_file):\n",
    "\n",
    "    model = api.load(pretrained_input_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unable to read local cache '/Users/nduran4/gensim-data/information.json' during fallback, connect to the Internet and retry",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:219\u001b[0m, in \u001b[0;36m_load_info\u001b[0;34m(url, encoding)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# We need io.open here because Py2 open doesn't support encoding keyword\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/nduran4/gensim-data/information.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# w2v_Google\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m w2v_model_goog \u001b[38;5;241m=\u001b[39m \u001b[43mload_w2v_trained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_input_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_w2v_google\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# w2v_Twitter\u001b[39;00m\n\u001b[1;32m      7\u001b[0m w2v_model_twit \u001b[38;5;241m=\u001b[39m load_w2v_trained(\n\u001b[1;32m      8\u001b[0m     pretrained_input_file\u001b[38;5;241m=\u001b[39mMODEL_w2v_google\n\u001b[1;32m      9\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36mload_w2v_trained\u001b[0;34m(pretrained_input_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_w2v_trained\u001b[39m(pretrained_input_file):\n\u001b[0;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_input_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:490\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download (if needed) dataset/model and load it to memory (unless `return_path` is set).\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m _create_base_dir()\n\u001b[0;32m--> 490\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect model/corpus name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:426\u001b[0m, in \u001b[0;36m_get_filename\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_filename\u001b[39m(name):\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve the filename of the dataset/model.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m     information \u001b[38;5;241m=\u001b[39m \u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     corpora \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpora\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    428\u001b[0m     models \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:268\u001b[0m, in \u001b[0;36minfo\u001b[0;34m(name, show_only_latest, name_only)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, show_only_latest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide the information related to model/dataset.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     information \u001b[38;5;241m=\u001b[39m \u001b[43m_load_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         corpora \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpora\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:222\u001b[0m, in \u001b[0;36m_load_info\u001b[0;34m(url, encoding)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(fin)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munable to read local cache \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m during fallback, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnect to the Internet and retry\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m cache_path\n\u001b[1;32m    225\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: unable to read local cache '/Users/nduran4/gensim-data/information.json' during fallback, connect to the Internet and retry"
     ]
    }
   ],
   "source": [
    "# w2v_Google\n",
    "w2v_model_goog = load_w2v_trained(\n",
    "    pretrained_input_file=MODEL_w2v_google\n",
    "    )        \n",
    "    \n",
    "# w2v_Twitter\n",
    "w2v_model_twit = load_w2v_trained(\n",
    "    pretrained_input_file=MODEL_w2v_google\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unable to read local cache '/Users/nduran4/gensim-data/information.json' during fallback, connect to the Internet and retry",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:219\u001b[0m, in \u001b[0;36m_load_info\u001b[0;34m(url, encoding)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# We need io.open here because Py2 open doesn't support encoding keyword\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/nduran4/gensim-data/information.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_w2v_google\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:490\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download (if needed) dataset/model and load it to memory (unless `return_path` is set).\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m _create_base_dir()\n\u001b[0;32m--> 490\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect model/corpus name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:426\u001b[0m, in \u001b[0;36m_get_filename\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_filename\u001b[39m(name):\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve the filename of the dataset/model.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m     information \u001b[38;5;241m=\u001b[39m \u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     corpora \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpora\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    428\u001b[0m     models \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:268\u001b[0m, in \u001b[0;36minfo\u001b[0;34m(name, show_only_latest, name_only)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, show_only_latest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide the information related to model/dataset.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     information \u001b[38;5;241m=\u001b[39m \u001b[43m_load_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         corpora \u001b[38;5;241m=\u001b[39m information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpora\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Dropbox (ASU)/Mac/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/gensim/downloader.py:222\u001b[0m, in \u001b[0;36m_load_info\u001b[0;34m(url, encoding)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(fin)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munable to read local cache \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m during fallback, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnect to the Internet and retry\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m cache_path\n\u001b[1;32m    225\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: unable to read local cache '/Users/nduran4/gensim-data/information.json' during fallback, connect to the Internet and retry"
     ]
    }
   ],
   "source": [
    "model = api.load(MODEL_w2v_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tagged_token</th>\n",
       "      <th>tagged_lemma</th>\n",
       "      <th>tagged_stan_token</th>\n",
       "      <th>tagged_stan_lemma</th>\n",
       "      <th>file</th>\n",
       "      <th>utter1</th>\n",
       "      <th>utter2</th>\n",
       "      <th>utter_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC:</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>['i', 'thought', 'that', 'maybe', 'that', 'wou...</td>\n",
       "      <td>['i', 'think', 'that', 'maybe', 'that', 'would...</td>\n",
       "      <td>[('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
       "      <td>[('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
       "      <td>[('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>PC: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PA:</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
       "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
       "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
       "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>PA: PC:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PC:</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>['should', 'i', 'start', 'this', 'it', 'is', '...</td>\n",
       "      <td>['should', 'i', 'start', 'this', 'it', 'be', '...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>PC: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PA:</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
       "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
       "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>PA: PC:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PC:</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>['okay', 'now', 'what']</td>\n",
       "      <td>['okay', 'now', 'what']</td>\n",
       "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>you didn't close it</td>\n",
       "      <td>PC: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>PC:</td>\n",
       "      <td>yeah but i think it might have gotten stuck now</td>\n",
       "      <td>['yeah', 'but', 'i', 'think', 'it', 'might', '...</td>\n",
       "      <td>['yeah', 'but', 'i', 'think', 'it', 'might', '...</td>\n",
       "      <td>[('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...</td>\n",
       "      <td>[('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...</td>\n",
       "      <td>[('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...</td>\n",
       "      <td>[('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>yeah but i think it might have gotten stuck now</td>\n",
       "      <td>if you hit the space bar i think it restarts it</td>\n",
       "      <td>PC: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>PA:</td>\n",
       "      <td>if you hit the space bar i think it restarts it</td>\n",
       "      <td>['if', 'you', 'hit', 'the', 'space', 'bar', 'i...</td>\n",
       "      <td>['if', 'you', 'hit', 'the', 'space', 'bar', 'i...</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>if you hit the space bar i think it restarts it</td>\n",
       "      <td>we got a gold one</td>\n",
       "      <td>PA: PC:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>PC:</td>\n",
       "      <td>we got a gold one</td>\n",
       "      <td>['we', 'got', 'a', 'gold', 'one']</td>\n",
       "      <td>['we', 'get', 'a', 'gold', 'one']</td>\n",
       "      <td>[('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...</td>\n",
       "      <td>[('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...</td>\n",
       "      <td>[('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...</td>\n",
       "      <td>[('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>we got a gold one</td>\n",
       "      <td>sweet come on</td>\n",
       "      <td>PC: PB:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>PB:</td>\n",
       "      <td>sweet come on</td>\n",
       "      <td>['sweet', 'come', 'on']</td>\n",
       "      <td>['sweet', 'come', 'on']</td>\n",
       "      <td>[('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]</td>\n",
       "      <td>[('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]</td>\n",
       "      <td>[('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]</td>\n",
       "      <td>[('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>sweet come on</td>\n",
       "      <td>i don't know what that means but gold is good</td>\n",
       "      <td>PB: PA:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>PA:</td>\n",
       "      <td>i don't know what that means but gold is good</td>\n",
       "      <td>['i', 'do', 'not', 'know', 'what', 'that', 'me...</td>\n",
       "      <td>['i', 'do', 'not', 'know', 'what', 'that', 'me...</td>\n",
       "      <td>[('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...</td>\n",
       "      <td>[('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...</td>\n",
       "      <td>[('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...</td>\n",
       "      <td>[('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...</td>\n",
       "      <td>ASU-T10_ExpBlock1-Oneatatime.txt</td>\n",
       "      <td>i don't know what that means but gold is good</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant                                            content  \\\n",
       "0          PC:       i thought that maybe that would do something   \n",
       "1          PA:  i think you have to c maybe close it and it wi...   \n",
       "2          PC:         should i restart this it's like down there   \n",
       "3          PA:                       yeah i would just restart it   \n",
       "4          PC:                                    okay now what     \n",
       "..         ...                                                ...   \n",
       "60         PC:    yeah but i think it might have gotten stuck now   \n",
       "61         PA:    if you hit the space bar i think it restarts it   \n",
       "62         PC:                                  we got a gold one   \n",
       "63         PB:                                      sweet come on   \n",
       "64         PA:      i don't know what that means but gold is good   \n",
       "\n",
       "                                                token  \\\n",
       "0   ['i', 'thought', 'that', 'maybe', 'that', 'wou...   \n",
       "1   ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
       "2   ['should', 'i', 'start', 'this', 'it', 'is', '...   \n",
       "3       ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
       "4                             ['okay', 'now', 'what']   \n",
       "..                                                ...   \n",
       "60  ['yeah', 'but', 'i', 'think', 'it', 'might', '...   \n",
       "61  ['if', 'you', 'hit', 'the', 'space', 'bar', 'i...   \n",
       "62                  ['we', 'got', 'a', 'gold', 'one']   \n",
       "63                            ['sweet', 'come', 'on']   \n",
       "64  ['i', 'do', 'not', 'know', 'what', 'that', 'me...   \n",
       "\n",
       "                                                lemma  \\\n",
       "0   ['i', 'think', 'that', 'maybe', 'that', 'would...   \n",
       "1   ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
       "2   ['should', 'i', 'start', 'this', 'it', 'be', '...   \n",
       "3       ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
       "4                             ['okay', 'now', 'what']   \n",
       "..                                                ...   \n",
       "60  ['yeah', 'but', 'i', 'think', 'it', 'might', '...   \n",
       "61  ['if', 'you', 'hit', 'the', 'space', 'bar', 'i...   \n",
       "62                  ['we', 'get', 'a', 'gold', 'one']   \n",
       "63                            ['sweet', 'come', 'on']   \n",
       "64  ['i', 'do', 'not', 'know', 'what', 'that', 'me...   \n",
       "\n",
       "                                         tagged_token  \\\n",
       "0   [('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...   \n",
       "1   [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
       "2   [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
       "3   [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
       "4     [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
       "..                                                ...   \n",
       "60  [('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...   \n",
       "61  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
       "62  [('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...   \n",
       "63    [('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]   \n",
       "64  [('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...   \n",
       "\n",
       "                                         tagged_lemma  \\\n",
       "0   [('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...   \n",
       "1   [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
       "2   [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
       "3   [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
       "4     [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
       "..                                                ...   \n",
       "60  [('yeah', 'NN'), ('but', 'CC'), ('i', 'JJ'), (...   \n",
       "61  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
       "62  [('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...   \n",
       "63    [('sweet', 'JJ'), ('come', 'NN'), ('on', 'IN')]   \n",
       "64  [('i', 'NNS'), ('do', 'VBP'), ('not', 'RB'), (...   \n",
       "\n",
       "                                    tagged_stan_token  \\\n",
       "0   [('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...   \n",
       "1   [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
       "2   [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
       "3   [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
       "4     [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
       "..                                                ...   \n",
       "60  [('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...   \n",
       "61  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
       "62  [('we', 'PRP'), ('got', 'VBD'), ('a', 'DT'), (...   \n",
       "63   [('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]   \n",
       "64  [('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...   \n",
       "\n",
       "                                    tagged_stan_lemma  \\\n",
       "0   [('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...   \n",
       "1   [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
       "2   [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
       "3   [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
       "4     [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
       "..                                                ...   \n",
       "60  [('yeah', 'NN'), ('but', 'CC'), ('i', 'FW'), (...   \n",
       "61  [('if', 'IN'), ('you', 'PRP'), ('hit', 'VBP'),...   \n",
       "62  [('we', 'PRP'), ('get', 'VBP'), ('a', 'DT'), (...   \n",
       "63   [('sweet', 'JJ'), ('come', 'VBN'), ('on', 'IN')]   \n",
       "64  [('i', 'LS'), ('do', 'VBP'), ('not', 'RB'), ('...   \n",
       "\n",
       "                                 file  \\\n",
       "0   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "1   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "2   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "3   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "4   ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "..                                ...   \n",
       "60   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "61   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "62   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "63   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "64   ASU-T10_ExpBlock1-Oneatatime.txt   \n",
       "\n",
       "                                               utter1  \\\n",
       "0        i thought that maybe that would do something   \n",
       "1   i think you have to c maybe close it and it wi...   \n",
       "2          should i restart this it's like down there   \n",
       "3                        yeah i would just restart it   \n",
       "4                                     okay now what     \n",
       "..                                                ...   \n",
       "60    yeah but i think it might have gotten stuck now   \n",
       "61    if you hit the space bar i think it restarts it   \n",
       "62                                  we got a gold one   \n",
       "63                                      sweet come on   \n",
       "64      i don't know what that means but gold is good   \n",
       "\n",
       "                                               utter2 utter_order  \n",
       "0   i think you have to c maybe close it and it wi...     PC: PA:  \n",
       "1          should i restart this it's like down there     PA: PC:  \n",
       "2                        yeah i would just restart it     PC: PA:  \n",
       "3                                     okay now what       PA: PC:  \n",
       "4                                 you didn't close it     PC: PA:  \n",
       "..                                                ...         ...  \n",
       "60    if you hit the space bar i think it restarts it     PC: PA:  \n",
       "61                                  we got a gold one     PA: PC:  \n",
       "62                                      sweet come on     PC: PB:  \n",
       "63      i don't know what that means but gold is good     PB: PA:  \n",
       "64                                               None         NaN  \n",
       "\n",
       "[65 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
