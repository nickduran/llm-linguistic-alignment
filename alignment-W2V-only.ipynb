{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Provides a way to use operating system dependent functionality like reading or writing to the file system\n",
    "from os import listdir, path  # listdir lists the files in a directory, path provides functions to manipulate file paths\n",
    "# import pickle  # Implements binary protocols for serializing and de-serializing Python object structures\n",
    "import pandas as pd  # Powerful data structures for data analysis, time series, and statistics\n",
    "import numpy as np  # Support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays\n",
    "from collections import Counter  # Provides a way to count the frequency of elements in a collection\n",
    "import re  # Provides regular expression matching operations\n",
    "# import requests  # Allows sending HTTP requests to interact with web services\n",
    "import ast  # Abstract Syntax Trees, used for parsing and analyzing Python source code\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Computes the cosine similarity between samples in a matrix\n",
    "import gensim  # Library for unsupervised topic modeling and natural language processing\n",
    "import gensim.downloader as api  # Downloads and loads pre-trained models and datasets\n",
    "# from gensim.models import KeyedVectors  # Provides efficient word vector representation and storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP for W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cache directory: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data\n",
      "Gensim BASE_DIR set to: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data\n",
      "Model word2vec-google-news-300 already exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/word2vec-google-news-300\n",
      "Model glove-twitter-200 already exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/glove-twitter-200\n"
     ]
    }
   ],
   "source": [
    "# Determine the current working directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Define the local cache directory relative to the current working directory\n",
    "local_cache_dir = os.path.join(script_dir, \"gensim-data\")\n",
    "os.makedirs(local_cache_dir, exist_ok=True)\n",
    "print(f\"Local cache directory: {local_cache_dir}\")\n",
    "\n",
    "# Set the BASE_DIR for gensim data\n",
    "api.BASE_DIR = local_cache_dir\n",
    "print(f\"Gensim BASE_DIR set to: {api.BASE_DIR}\")\n",
    "\n",
    "# Function to download and cache models\n",
    "def download_and_cache_models(models, cache_dir):\n",
    "    api.BASE_DIR = cache_dir\n",
    "    for model_name in models:\n",
    "        model_path = os.path.join(cache_dir, model_name)\n",
    "        if not os.path.exists(model_path):\n",
    "            try:\n",
    "                print(f\"Downloading model: {model_name}\")\n",
    "                model = api.load(model_name)\n",
    "                print(f\"Downloaded and cached model: {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {model_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Model {model_name} already exists at: {model_path}\")\n",
    "\n",
    "# List of models to download and cache\n",
    "models_to_cache = ['word2vec-google-news-300', 'glove-twitter-200']\n",
    "download_and_cache_models(models_to_cache, local_cache_dir)\n",
    "\n",
    "# Function to load models if they are not already loaded\n",
    "def load_model_if_not_exists(model_path, binary=True):\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file {model_path} does not exist.\")\n",
    "        return gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=binary)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the Google News model if it is not already loaded\n",
    "if 'w2v_google_model' not in globals():\n",
    "    w2v_google_model_path = os.path.join(local_cache_dir, 'word2vec-google-news-300', 'word2vec-google-news-300.gz')\n",
    "    w2v_google_model = load_model_if_not_exists(w2v_google_model_path, binary=True)\n",
    "    if w2v_google_model is not None:\n",
    "        print(\"Word2Vec Google News model loaded from local cache successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to load Word2Vec Google News model.\")\n",
    "\n",
    "# note: possible todo: is it more efficient to use gensim.downloader.load(model_name)?\n",
    "# note, downloading model, it downloads properly, but also throws the exception warning for some reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate conversations\n",
    "def aggregate_conversations(folder_path: str) -> pd.DataFrame:\n",
    "    text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    for file_name in text_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "# Function to build filtered vocabulary from aggegrated conversations\n",
    "def build_filtered_vocab(data: pd.DataFrame, output_file_directory: str, high_sd_cutoff: float = 3, low_n_cutoff: int = 1):\n",
    "    # Tokenize the lemmas\n",
    "    all_sentences = [re.sub(r'[^\\w\\s]+', '', str(row)).split() for row in data['lemma']]\n",
    "    all_words = [word for sentence in all_sentences for word in sentence]\n",
    "    \n",
    "    # Frequency count using Counter\n",
    "    frequency = Counter(all_words)\n",
    "    \n",
    "    # Filter out one-letter words and those below low_n_cutoff\n",
    "    frequency_filt = {word: freq for word, freq in frequency.items() if len(word) > 1 and freq > low_n_cutoff}\n",
    "    \n",
    "    # Remove high-frequency words if high_sd_cutoff is specified\n",
    "    if high_sd_cutoff is not None:\n",
    "        mean_freq = np.mean(list(frequency_filt.values()))\n",
    "        std_freq = np.std(list(frequency_filt.values()))\n",
    "        cutoff_freq = mean_freq + (std_freq * high_sd_cutoff)\n",
    "        filteredWords = {word: freq for word, freq in frequency_filt.items() if freq < cutoff_freq}\n",
    "    else:\n",
    "        filteredWords = frequency_filt\n",
    "    \n",
    "    # Convert to DataFrames for exporting\n",
    "    vocabfreq_all = pd.DataFrame(frequency.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
    "    vocabfreq_filt = pd.DataFrame(filteredWords.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
    "    \n",
    "    # Save to files\n",
    "    vocabfreq_all.to_csv(os.path.join(output_file_directory, 'vocab_unfilt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
    "    vocabfreq_filt.to_csv(os.path.join(output_file_directory, 'vocab_filt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
    "    \n",
    "    return list(frequency.keys()), list(filteredWords.keys())\n",
    "\n",
    "# Function to check if a column contains list-like strings\n",
    "def is_list_like_column(series):\n",
    "    try:\n",
    "        return series.apply(lambda x: x.strip().startswith(\"[\")).all()\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "# Function to convert columns with list-like strings to actual lists\n",
    "def convert_columns_to_lists(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns_converted = []\n",
    "    for col in df.columns:\n",
    "        if is_list_like_column(df[col]):\n",
    "            df[col] = df[col].apply(ast.literal_eval)\n",
    "            columns_converted.append(col)\n",
    "    return df, columns_converted\n",
    "\n",
    "# Function to get lagged conversational turns and restructure dataframe\n",
    "def process_input_data(df: pd.DataFrame, include_stan: bool = True) -> pd.DataFrame:\n",
    "    # Base columns to lag\n",
    "    columns_to_lag = ['content', 'token', 'lemma', 'tagged_token', 'tagged_lemma']\n",
    "    \n",
    "    # Optionally include \"stan\" columns if they exist\n",
    "    if include_stan:\n",
    "        stan_columns = [col for col in df.columns if 'stan' in col]\n",
    "        columns_to_lag.extend(stan_columns)\n",
    "    \n",
    "    for col in columns_to_lag:\n",
    "        if col in df.columns:  # Ensure the column exists in the DataFrame\n",
    "            df[f'{col}1'] = df[col]\n",
    "            df[f'{col}2'] = df[col].shift(-1)\n",
    "    \n",
    "    df['utter_order'] = df['participant'] + ' ' + df['participant'].shift(-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main function to process the file\n",
    "def process_file(file_path, large_list: list):       \n",
    "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "    \n",
    "    # Convert columns with list-like strings to actual lists\n",
    "    df, columns_converted = convert_columns_to_lists(df)\n",
    "\n",
    "    # Filtering based on user-specified requests\n",
    "    columns_to_filter = ['lemma','token']\n",
    "    for col in columns_to_filter:\n",
    "        # First filter: Keep only words in filter_model\n",
    "        df[col] = df[col].apply(lambda token_list: [word for word in token_list if word in large_list])\n",
    "    \n",
    "    # Do the lagging\n",
    "    df = process_input_data(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to compute cosine similarities between embeddings\n",
    "def compute_cosine_similarities(df: pd.DataFrame, columns: list):\n",
    "    for col1, col2 in columns:\n",
    "        similarities = []\n",
    "        for i in range(len(df)):\n",
    "            vec1 = df.iloc[i][col1]\n",
    "            vec2 = df.iloc[i][col2]\n",
    "            if vec1 is not None and vec2 is not None:\n",
    "                similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "            else:\n",
    "                similarity = None\n",
    "            similarities.append(similarity)\n",
    "\n",
    "        # Determine whether this is for \"token\" or \"lemma\" based on the column name\n",
    "        if 'token' in col1:\n",
    "            similarity_column_name = 'token_cosine_similarity'\n",
    "        elif 'lemma' in col1:\n",
    "            similarity_column_name = 'lemma_cosine_similarity'\n",
    "\n",
    "        df[similarity_column_name] = similarities\n",
    "    return df\n",
    "\n",
    "# Function sum embeddings for each list of tokens\n",
    "def get_sum_embeddings(token_list, model):\n",
    "    if token_list is None:\n",
    "        return None    \n",
    "    embeddings = []\n",
    "    for word in token_list:\n",
    "        if word in model.key_to_index:  # Check if word is in the model vocabulary\n",
    "            embeddings.append(model[word])    \n",
    "    if embeddings:\n",
    "        sum_embedding = np.sum(embeddings, axis=0)\n",
    "        return sum_embedding\n",
    "    else:\n",
    "        return None  # Or handle empty embeddings as you see fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2/2 [00:00<00:00, 33.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing the text files\n",
    "folder_path = \"./data/prepped_stan_small\"\n",
    "output_file_directory = \"output\"\n",
    "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "\n",
    "# Aggregate individual conversation files\n",
    "concatenated_text_files = aggregate_conversations(folder_path)\n",
    "    \n",
    "# Build filtered vocabulary from aggregated data\n",
    "vocab_all, vocab_filtered = build_filtered_vocab(concatenated_text_files, output_file_directory)\n",
    "\n",
    "# Process each file and update the cache\n",
    "concatenated_df = pd.DataFrame()\n",
    "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = process_file(file_path, vocab_filtered)\n",
    "\n",
    "    # Create columns of embeddings\n",
    "    for column in [\"lemma\", \"token\"]:\n",
    "        df[f\"{column}1_sum_embedding\"] = df[f\"{column}1\"].apply(lambda tokens: get_sum_embeddings(tokens, w2v_google_model))\n",
    "        df[f\"{column}2_sum_embedding\"] = df[f\"{column}2\"].apply(lambda tokens: get_sum_embeddings(tokens, w2v_google_model))\n",
    "\n",
    "    # Columns to compute similarities\n",
    "    embedding_columns = [\n",
    "        (\"lemma1_sum_embedding\", \"lemma2_sum_embedding\"),\n",
    "        (\"token1_sum_embedding\", \"token2_sum_embedding\")\n",
    "    ]\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    df = compute_cosine_similarities(df, embedding_columns)\n",
    "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tagged_token</th>\n",
       "      <th>tagged_lemma</th>\n",
       "      <th>tagged_stan_token</th>\n",
       "      <th>tagged_stan_lemma</th>\n",
       "      <th>file</th>\n",
       "      <th>content1</th>\n",
       "      <th>...</th>\n",
       "      <th>tagged_stan_token2</th>\n",
       "      <th>tagged_stan_lemma1</th>\n",
       "      <th>tagged_stan_lemma2</th>\n",
       "      <th>utter_order</th>\n",
       "      <th>lemma1_sum_embedding</th>\n",
       "      <th>lemma2_sum_embedding</th>\n",
       "      <th>token1_sum_embedding</th>\n",
       "      <th>token2_sum_embedding</th>\n",
       "      <th>lemma_cosine_similarity</th>\n",
       "      <th>token_cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC:</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>[that, maybe, that, would, do, something]</td>\n",
       "      <td>[think, that, maybe, that, would, do, something]</td>\n",
       "      <td>[(i, NN), (thought, VBD), (that, IN), (maybe, ...</td>\n",
       "      <td>[(i, NN), (think, VBP), (that, IN), (maybe, RB...</td>\n",
       "      <td>[(i, LS), (thought, VBD), (that, IN), (maybe, ...</td>\n",
       "      <td>[(i, FW), (think, VBP), (that, IN), (maybe, RB...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>...</td>\n",
       "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
       "      <td>[(i, FW), (think, VBP), (that, IN), (maybe, RB...</td>\n",
       "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[0.33276367, 0.18133545, 0.54364014, 1.277832,...</td>\n",
       "      <td>[0.71087646, 0.19366455, 0.7921753, 2.4785156,...</td>\n",
       "      <td>[0.37963867, 0.11444092, 0.53430176, 1.0141602...</td>\n",
       "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
       "      <td>0.861987</td>\n",
       "      <td>0.826057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PA:</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>[think, you, have, maybe, close, and, will, yo...</td>\n",
       "      <td>[think, you, have, maybe, close, and, will, yo...</td>\n",
       "      <td>[(i, NN), (think, VBP), (you, PRP), (have, VBP...</td>\n",
       "      <td>[(i, NN), (think, VBP), (you, PRP), (have, VBP...</td>\n",
       "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
       "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>...</td>\n",
       "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
       "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
       "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
       "      <td>PA: PC:</td>\n",
       "      <td>[0.71087646, 0.19366455, 0.7921753, 2.4785156,...</td>\n",
       "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
       "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
       "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
       "      <td>0.749735</td>\n",
       "      <td>0.751777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PC:</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>[start, this, like, down, there]</td>\n",
       "      <td>[start, this, like, down, there]</td>\n",
       "      <td>[(should, MD), (i, VB), (start, VB), (this, DT...</td>\n",
       "      <td>[(should, MD), (i, VB), (start, VB), (this, DT...</td>\n",
       "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
       "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>...</td>\n",
       "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
       "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
       "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
       "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
       "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
       "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
       "      <td>0.777175</td>\n",
       "      <td>0.777175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PA:</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>[yeah, would, just, start]</td>\n",
       "      <td>[yeah, would, just, start]</td>\n",
       "      <td>[(yeah, NN), (i, NN), (would, MD), (just, RB),...</td>\n",
       "      <td>[(yeah, NN), (i, NN), (would, MD), (just, RB),...</td>\n",
       "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
       "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>...</td>\n",
       "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
       "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
       "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
       "      <td>PA: PC:</td>\n",
       "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
       "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
       "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
       "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
       "      <td>0.720251</td>\n",
       "      <td>0.720251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PC:</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>[okay, now, what]</td>\n",
       "      <td>[okay, now, what]</td>\n",
       "      <td>[(okay, RB), (now, RB), (what, WP)]</td>\n",
       "      <td>[(okay, RB), (now, RB), (what, WP)]</td>\n",
       "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
       "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>...</td>\n",
       "      <td>[(you, PRP), (did, VBD), (not, RB), (close, VB...</td>\n",
       "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
       "      <td>[(you, PRP), (do, VBP), (not, RB), (close, VB)...</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
       "      <td>[0.40890503, -0.14001465, 0.30407715, 0.839355...</td>\n",
       "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
       "      <td>[0.28585815, -0.15283203, 0.28466797, 0.608886...</td>\n",
       "      <td>0.661706</td>\n",
       "      <td>0.616577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant                                            content  \\\n",
       "0         PC:       i thought that maybe that would do something   \n",
       "1         PA:  i think you have to c maybe close it and it wi...   \n",
       "2         PC:         should i restart this it's like down there   \n",
       "3         PA:                       yeah i would just restart it   \n",
       "4         PC:                                    okay now what     \n",
       "\n",
       "                                               token  \\\n",
       "0          [that, maybe, that, would, do, something]   \n",
       "1  [think, you, have, maybe, close, and, will, yo...   \n",
       "2                   [start, this, like, down, there]   \n",
       "3                         [yeah, would, just, start]   \n",
       "4                                  [okay, now, what]   \n",
       "\n",
       "                                               lemma  \\\n",
       "0   [think, that, maybe, that, would, do, something]   \n",
       "1  [think, you, have, maybe, close, and, will, yo...   \n",
       "2                   [start, this, like, down, there]   \n",
       "3                         [yeah, would, just, start]   \n",
       "4                                  [okay, now, what]   \n",
       "\n",
       "                                        tagged_token  \\\n",
       "0  [(i, NN), (thought, VBD), (that, IN), (maybe, ...   \n",
       "1  [(i, NN), (think, VBP), (you, PRP), (have, VBP...   \n",
       "2  [(should, MD), (i, VB), (start, VB), (this, DT...   \n",
       "3  [(yeah, NN), (i, NN), (would, MD), (just, RB),...   \n",
       "4                [(okay, RB), (now, RB), (what, WP)]   \n",
       "\n",
       "                                        tagged_lemma  \\\n",
       "0  [(i, NN), (think, VBP), (that, IN), (maybe, RB...   \n",
       "1  [(i, NN), (think, VBP), (you, PRP), (have, VBP...   \n",
       "2  [(should, MD), (i, VB), (start, VB), (this, DT...   \n",
       "3  [(yeah, NN), (i, NN), (would, MD), (just, RB),...   \n",
       "4                [(okay, RB), (now, RB), (what, WP)]   \n",
       "\n",
       "                                   tagged_stan_token  \\\n",
       "0  [(i, LS), (thought, VBD), (that, IN), (maybe, ...   \n",
       "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
       "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
       "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
       "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
       "\n",
       "                                   tagged_stan_lemma  \\\n",
       "0  [(i, FW), (think, VBP), (that, IN), (maybe, RB...   \n",
       "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
       "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
       "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
       "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
       "\n",
       "                                file  \\\n",
       "0  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "1  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "2  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "3  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "4  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "\n",
       "                                            content1  ...  \\\n",
       "0       i thought that maybe that would do something  ...   \n",
       "1  i think you have to c maybe close it and it wi...  ...   \n",
       "2         should i restart this it's like down there  ...   \n",
       "3                       yeah i would just restart it  ...   \n",
       "4                                    okay now what    ...   \n",
       "\n",
       "                                  tagged_stan_token2  \\\n",
       "0  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
       "1  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
       "2  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
       "3                [(okay, JJ), (now, RB), (what, WP)]   \n",
       "4  [(you, PRP), (did, VBD), (not, RB), (close, VB...   \n",
       "\n",
       "                                  tagged_stan_lemma1  \\\n",
       "0  [(i, FW), (think, VBP), (that, IN), (maybe, RB...   \n",
       "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
       "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
       "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
       "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
       "\n",
       "                                  tagged_stan_lemma2 utter_order  \\\n",
       "0  [(i, LS), (think, VB), (you, PRP), (have, VBP)...     PC: PA:   \n",
       "1  [(should, MD), (i, FW), (start, VB), (this, DT...     PA: PC:   \n",
       "2  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...     PC: PA:   \n",
       "3                [(okay, JJ), (now, RB), (what, WP)]     PA: PC:   \n",
       "4  [(you, PRP), (do, VBP), (not, RB), (close, VB)...     PC: PA:   \n",
       "\n",
       "                                lemma1_sum_embedding  \\\n",
       "0  [0.33276367, 0.18133545, 0.54364014, 1.277832,...   \n",
       "1  [0.71087646, 0.19366455, 0.7921753, 2.4785156,...   \n",
       "2  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
       "3  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
       "4  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
       "\n",
       "                                lemma2_sum_embedding  \\\n",
       "0  [0.71087646, 0.19366455, 0.7921753, 2.4785156,...   \n",
       "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
       "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
       "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
       "4  [0.40890503, -0.14001465, 0.30407715, 0.839355...   \n",
       "\n",
       "                                token1_sum_embedding  \\\n",
       "0  [0.37963867, 0.11444092, 0.53430176, 1.0141602...   \n",
       "1  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...   \n",
       "2  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
       "3  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
       "4  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
       "\n",
       "                                token2_sum_embedding lemma_cosine_similarity  \\\n",
       "0  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...                0.861987   \n",
       "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...                0.749735   \n",
       "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...                0.777175   \n",
       "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...                0.720251   \n",
       "4  [0.28585815, -0.15283203, 0.28466797, 0.608886...                0.661706   \n",
       "\n",
       "  token_cosine_similarity  \n",
       "0                0.826057  \n",
       "1                0.751777  \n",
       "2                0.777175  \n",
       "3                0.720251  \n",
       "4                0.616577  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
