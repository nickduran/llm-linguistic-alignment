{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Get the requirements.txt file\n",
    "# add the openAI API key as an environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os  # Provides a way to use operating system dependent functionality like reading or writing to the file system\n",
    "import pickle\n",
    "import pandas as pd  # Powerful data structures for data analysis, time series, and statistics\n",
    "import numpy as np  # Support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Computes the cosine similarity between samples in a matrix\n",
    "\n",
    "# For W2V specifically\n",
    "import re  # Provides regular expression matching operations\n",
    "import ast  # Abstract Syntax Trees, used for parsing and analyzing Python source code\n",
    "from collections import Counter  # Provides a way to count the frequency of elements in a collection\n",
    "import gensim  # Library for unsupervised topic modeling and natural language processing\n",
    "import gensim.downloader as api  # Downloads and loads pre-trained models and datasets\n",
    "# from gensim.models import KeyedVectors  # Provides efficient word vector representation and storage\n",
    "\n",
    "# For BERT specifically \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# For GPT specifically\n",
    "import openai\n",
    "# Retrieve the API key from the environment variable\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "# For LLAMA specifically\n",
    "# Going to skip LLAMA for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_and_lag_columns(df: pd.DataFrame, columns_to_lag: list, suffix1: str = '1', suffix2: str = '2') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates lagged pairs of specified columns, generating new columns with a \n",
    "    suffix of `suffix1` for the original content and `suffix2` for the lagged content. \n",
    "    Also adds a new column indicating the order of participants between successive rows.\n",
    "    \"\"\"\n",
    "    for col in columns_to_lag:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}{suffix1}'] = df[col]\n",
    "            df[f'{col}{suffix2}'] = df[col].shift(-1)\n",
    "    df['utter_order'] = df['participant'] + ' ' + df['participant'].shift(-1)\n",
    "    return df\n",
    "\n",
    "def calculate_cosine_similarity(df: pd.DataFrame, embedding_pairs: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes cosine similarities between pairs of vectors in the specified columns \n",
    "    and adds the results as new columns in the DataFrame.\n",
    "    \"\"\"\n",
    "    for col1, col2 in embedding_pairs:\n",
    "        similarities = df.apply(\n",
    "            lambda row: cosine_similarity(\n",
    "                np.array(row[col1]).reshape(1, -1),\n",
    "                np.array(row[col2]).reshape(1, -1)\n",
    "            )[0][0] if row[col1] is not None and row[col2] is not None else None,\n",
    "            axis=1\n",
    "        )\n",
    "        similarity_column_name = f\"{col1}_{col2}_cosine_similarity\"\n",
    "        df[similarity_column_name] = similarities\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEGIN WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps: Load in models → Aggregate conversations → Build vocabulary → Pair and lag columns → Compute embeddings → Compute cosine similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP for W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cache directory: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data\n",
      "Gensim BASE_DIR set to: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data\n",
      "Model word2vec-google-news-300 already exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/word2vec-google-news-300\n",
      "Model glove-twitter-200 already exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/glove-twitter-200\n",
      "Word2Vec Google News model loaded from local cache successfully.\n"
     ]
    }
   ],
   "source": [
    "# retrieve the curent working directory where the script is being executed\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# create a directory called \"gensim-data\" (if doesn't already exist)\n",
    "local_cache_dir = os.path.join(script_dir, \"gensim-data\")\n",
    "os.makedirs(local_cache_dir, exist_ok=True)\n",
    "print(f\"Local cache directory: {local_cache_dir}\")\n",
    "\n",
    "# configure Gensim to use local_cache_dir as base directory for downloading and storing models\n",
    "api.BASE_DIR = local_cache_dir\n",
    "print(f\"Gensim BASE_DIR set to: {api.BASE_DIR}\")\n",
    "\n",
    "# checks if specified models are already downloaded to cache directory, if not, download them\n",
    "def download_and_cache_models(models, cache_dir):\n",
    "    api.BASE_DIR = cache_dir\n",
    "    for model_name in models:\n",
    "        model_path = os.path.join(cache_dir, model_name)\n",
    "        if not os.path.exists(model_path):\n",
    "            try:\n",
    "                print(f\"Downloading model: {model_name}\")\n",
    "                model = api.load(model_name)\n",
    "                print(f\"Downloaded and cached model: {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {model_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Model {model_name} already exists at: {model_path}\")\n",
    "\n",
    "# specifies the list of models to be cached locally, invoking the download_and_cache_models function \n",
    "models_to_cache = ['word2vec-google-news-300', 'glove-twitter-200']\n",
    "download_and_cache_models(models_to_cache, local_cache_dir)\n",
    "\n",
    "# attempts to load a model from specified file path\n",
    "def load_model_if_not_exists(model_path, binary=True):\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file {model_path} does not exist.\")\n",
    "        return gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=binary)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# checks if the w2v_google_model is already loaded in the global namespace. if not, attempts to load it from local cache directory.\n",
    "if 'w2v_google_model' not in globals():\n",
    "    w2v_google_model_path = os.path.join(local_cache_dir, 'word2vec-google-news-300', 'word2vec-google-news-300.gz')\n",
    "    w2v_google_model = load_model_if_not_exists(w2v_google_model_path, binary=True)\n",
    "    if w2v_google_model is not None:\n",
    "        print(\"Word2Vec Google News model loaded from local cache successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to load Word2Vec Google News model.\")\n",
    "\n",
    "# note: possible todo: is it more efficient to use gensim.downloader.load(model_name)?\n",
    "# note, downloading model, it downloads properly, but also throws the exception warning for some reason. \n",
    "# TODO: instead of just loading google news model into global workspace, load in all within \"models_to_cache\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate conversations → Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_conversations(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates multiple .txt files located in a specified folder \n",
    "    into a single pandas DataFrame. Each file is expected to be \n",
    "    tab-separated. \n",
    "    \n",
    "    Returns a DataFrame containing the concatenated content of all \n",
    "    the .txt files\n",
    "    \"\"\"\n",
    "    text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    for file_name in text_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "def build_filtered_vocab(data: pd.DataFrame, output_file_directory: str, high_sd_cutoff: float = 3, low_n_cutoff: int = 1):\n",
    "    \"\"\"\n",
    "    Constructs a vocabulary from the ‘lemma’ column of the input DataFrame, \n",
    "    applying frequency-based filtering: ords occurring less frequently \n",
    "    than low_n_cutoff or more frequently than a certain standard deviation \n",
    "    above the mean (high_sd_cutoff) are filtered out. \n",
    "    \n",
    "    Returns: Two lists: one with all vocabulary words and another with filtered words\n",
    "    Outputs: The vocabulary frequencies to files\n",
    "    \"\"\" \n",
    "\n",
    "    all_sentences = [re.sub(r'[^\\w\\s]+', '', str(row)).split() for row in data['lemma']]\n",
    "    all_words = [word for sentence in all_sentences for word in sentence]\n",
    "\n",
    "    frequency = Counter(all_words)\n",
    "\n",
    "    frequency_filt = {word: freq for word, freq in frequency.items() if len(word) > 1 and freq > low_n_cutoff}\n",
    "    \n",
    "    if high_sd_cutoff is not None:\n",
    "        mean_freq = np.mean(list(frequency_filt.values()))\n",
    "        std_freq = np.std(list(frequency_filt.values()))\n",
    "        cutoff_freq = mean_freq + (std_freq * high_sd_cutoff)\n",
    "        filteredWords = {word: freq for word, freq in frequency_filt.items() if freq < cutoff_freq}\n",
    "    else:\n",
    "        filteredWords = frequency_filt\n",
    "  \n",
    "    vocabfreq_all = pd.DataFrame(frequency.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
    "    vocabfreq_filt = pd.DataFrame(filteredWords.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
    "  \n",
    "    vocabfreq_all.to_csv(os.path.join(output_file_directory, 'vocab_unfilt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
    "    vocabfreq_filt.to_csv(os.path.join(output_file_directory, 'vocab_filt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
    "    \n",
    "    return list(frequency.keys()), list(filteredWords.keys())\n",
    "\n",
    "def is_list_like_column(series):\n",
    "    \"\"\"\n",
    "    Checks if a pandas Series contains list-like strings (i.e., strings that \n",
    "    look like lists).\n",
    "    \"\"\"    \n",
    "\n",
    "    try:\n",
    "        return series.apply(lambda x: x.strip().startswith(\"[\")).all()\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "def convert_columns_to_lists(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts any columns in a DataFrame that contain list-like strings into \n",
    "    actual Python lists using ast.literal_eval.\n",
    "    \"\"\"        \n",
    "\n",
    "    columns_converted = []\n",
    "    for col in df.columns:\n",
    "        if is_list_like_column(df[col]):\n",
    "            df[col] = df[col].apply(ast.literal_eval)\n",
    "            columns_converted.append(col)\n",
    "    return df, columns_converted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair and lag columns → Compute embeddings → Compute cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_embeddings(token_list, model):\n",
    "    \"\"\"\n",
    "    Calculates the sum of word embeddings for a list of tokens using a \n",
    "    pre-trained Word2Vec model.\n",
    "    \"\"\" \n",
    "\n",
    "    if token_list is None:\n",
    "        return None    \n",
    "    embeddings = []\n",
    "    for word in token_list:\n",
    "        if word in model.key_to_index:  \n",
    "            embeddings.append(model[word])    \n",
    "    if embeddings:\n",
    "        sum_embedding = np.sum(embeddings, axis=0)\n",
    "        return sum_embedding\n",
    "    else:\n",
    "        return None  \n",
    "    \n",
    "def process_file_for_W2V(file_path, vocab_list: list):\n",
    "    \"\"\"\n",
    "    Processes a file containing conversation data, filters tokens based on a provided vocabulary list,\n",
    "    pairs and lags columns, computes word embeddings, and then calculates cosine similarities \n",
    "    between the embeddings.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    df, columns_converted = convert_columns_to_lists(df)\n",
    "\n",
    "    # Filter tokens based on the vocabulary list\n",
    "    columns_to_filter = ['lemma', 'token']\n",
    "    for col in columns_to_filter:\n",
    "        df[col] = df[col].apply(lambda token_list: [word for word in token_list if word in vocab_list])\n",
    "\n",
    "    # Pair and lag the columns\n",
    "    columns_to_lag = ['content', 'token', 'lemma']\n",
    "    df = pair_and_lag_columns(df, columns_to_lag)\n",
    "\n",
    "    # Compute embeddings\n",
    "    for column in [\"lemma\", \"token\"]:\n",
    "        df[f\"{column}1_sum_embedding\"] = df[f\"{column}1\"].apply(lambda tokens: get_sum_embeddings(tokens, w2v_google_model))\n",
    "        df[f\"{column}2_sum_embedding\"] = df[f\"{column}2\"].apply(lambda tokens: get_sum_embeddings(tokens, w2v_google_model))\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    embedding_columns = [\n",
    "        (\"lemma1_sum_embedding\", \"lemma2_sum_embedding\"),\n",
    "        (\"token1_sum_embedding\", \"token2_sum_embedding\")\n",
    "    ]\n",
    "    df = calculate_cosine_similarity(df, embedding_columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2/2 [00:00<00:00, 34.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define folder paths\n",
    "folder_path = \"./data/prepped_stan_small\"\n",
    "output_file_directory = \"outputW2V\"\n",
    "\n",
    "# Check if the output directory exists, create it if not\n",
    "os.makedirs(output_file_directory, exist_ok=True)\n",
    "\n",
    "# List all text files in the folder\n",
    "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "\n",
    "# Aggregate the conversations into a single DataFrame\n",
    "concatenated_text_files = aggregate_conversations(folder_path)\n",
    "\n",
    "# Build the filtered vocabulary and save it to the output directory\n",
    "vocab_all, vocab_filtered = build_filtered_vocab(concatenated_text_files, output_file_directory)\n",
    "\n",
    "# Process each file and concatenate the results into a single DataFrame\n",
    "concatenated_df = pd.DataFrame()\n",
    "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = process_file_for_W2V(file_path, vocab_filtered)\n",
    "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEGIN BERT\n",
    "\n",
    "#### Steps: Pair and lag columns → Compute embeddings → Compute cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_with_cache(text):\n",
    "    \"\"\"\n",
    "    Generates a BERT embedding for a given text while utilizing a cache to avoid redundant computations:\n",
    "\t•\t  Checks if the embedding for the given text is already in the cache. If so, returns it.\n",
    "\t•\t  If not cached, tokenizes the text, converts tokens to IDs, and feeds them to the BERT model to get the embedding.\n",
    "\t•\t  The embedding is then averaged over all tokens and stored in the cache for future use.\n",
    "    \"\"\" \n",
    "\n",
    "    if text is None:\n",
    "      return None\n",
    "\n",
    "    if text in embedding_cache:\n",
    "      return embedding_cache[text]\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_ids = [tokenizer.cls_token_id] + token_ids + [tokenizer.sep_token_id]\n",
    "    input_ids = torch.tensor([token_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    embedding = torch.mean(last_hidden_states, dim=1).numpy()\n",
    "    embedding_cache[text] = embedding\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def process_file(file_path, embedding_cache):\n",
    "    \"\"\"\n",
    "    Processes a single file to compute BERT embeddings for pairs of utterances and \n",
    "    calculates the cosine similarity between these embeddings:\n",
    "    • Reads the file into a DataFrame.\n",
    "    • Pairs and lags the `content` column using `pair_and_lag_columns`.\n",
    "    • Applies the `get_embedding_with_cache` function to each utterance pair \n",
    "      to compute embeddings.\n",
    "    • Computes the cosine similarity between embeddings of successive utterances.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # Pair and lag the columns\n",
    "    df = pair_and_lag_columns(df, columns_to_lag=['content'])\n",
    "\n",
    "    # Compute embeddings for the lagged columns\n",
    "    for column in [\"content1\", \"content2\"]:\n",
    "        df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n",
    "\n",
    "    # Calculate cosine similarities between embeddings\n",
    "    embedding_columns = [(\"content1_embedding\", \"content2_embedding\")]\n",
    "    df = calculate_cosine_similarity(df, embedding_columns)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2/2 [00:00<00:00, 92.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Attempts to load a pre-existing embedding cache from a file (bert_embedding_cache.pkl).\n",
    "embedding_cache_path = \"data/bert_embedding_cache.pkl\"\n",
    "try:\n",
    "    with open(embedding_cache_path, \"rb\") as f:\n",
    "        embedding_cache = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "\n",
    "# Path to the folder containing the text files\n",
    "folder_path = \"data/prepped_stan_small\"\n",
    "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "\n",
    "# This loop processes each text file one by one:\n",
    "concatenated_df = pd.DataFrame()\n",
    "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = process_file(file_path, embedding_cache)\n",
    "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "    with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "        pickle.dump(embedding_cache, embedding_cache_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tagged_token</th>\n",
       "      <th>tagged_lemma</th>\n",
       "      <th>tagged_stan_token</th>\n",
       "      <th>tagged_stan_lemma</th>\n",
       "      <th>file</th>\n",
       "      <th>content1</th>\n",
       "      <th>content2</th>\n",
       "      <th>utter_order</th>\n",
       "      <th>content1_embedding</th>\n",
       "      <th>content2_embedding</th>\n",
       "      <th>content1_embedding_content2_embedding_cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC:</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>['i', 'thought', 'that', 'maybe', 'that', 'wou...</td>\n",
       "      <td>['i', 'think', 'that', 'maybe', 'that', 'would...</td>\n",
       "      <td>[('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
       "      <td>[('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
       "      <td>[('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[[0.16115269, -0.18967551, -0.091776446, 0.072...</td>\n",
       "      <td>[[0.16965881, -0.09342009, 0.31895435, 0.05972...</td>\n",
       "      <td>0.631972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PA:</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
       "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
       "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
       "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>PA: PC:</td>\n",
       "      <td>[[0.16965881, -0.09342009, 0.31895435, 0.05972...</td>\n",
       "      <td>[[0.16538174, 0.040479627, 0.08581757, 0.03785...</td>\n",
       "      <td>0.730899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PC:</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>['should', 'i', 'start', 'this', 'it', 'is', '...</td>\n",
       "      <td>['should', 'i', 'start', 'this', 'it', 'be', '...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[[0.16538174, 0.040479627, 0.08581757, 0.03785...</td>\n",
       "      <td>[[0.42204928, -0.110650204, 0.27723312, 0.3426...</td>\n",
       "      <td>0.633439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PA:</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
       "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
       "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>PA: PC:</td>\n",
       "      <td>[[0.42204928, -0.110650204, 0.27723312, 0.3426...</td>\n",
       "      <td>[[0.29328585, -0.4999022, 0.16143966, 0.002446...</td>\n",
       "      <td>0.640285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PC:</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>['okay', 'now', 'what']</td>\n",
       "      <td>['okay', 'now', 'what']</td>\n",
       "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>you didn't close it</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[[0.29328585, -0.4999022, 0.16143966, 0.002446...</td>\n",
       "      <td>[[0.29594588, 0.2687162, -0.15591031, 0.149712...</td>\n",
       "      <td>0.505967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant                                            content  \\\n",
       "0         PC:       i thought that maybe that would do something   \n",
       "1         PA:  i think you have to c maybe close it and it wi...   \n",
       "2         PC:         should i restart this it's like down there   \n",
       "3         PA:                       yeah i would just restart it   \n",
       "4         PC:                                    okay now what     \n",
       "\n",
       "                                               token  \\\n",
       "0  ['i', 'thought', 'that', 'maybe', 'that', 'wou...   \n",
       "1  ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
       "2  ['should', 'i', 'start', 'this', 'it', 'is', '...   \n",
       "3      ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
       "4                            ['okay', 'now', 'what']   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  ['i', 'think', 'that', 'maybe', 'that', 'would...   \n",
       "1  ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
       "2  ['should', 'i', 'start', 'this', 'it', 'be', '...   \n",
       "3      ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
       "4                            ['okay', 'now', 'what']   \n",
       "\n",
       "                                        tagged_token  \\\n",
       "0  [('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...   \n",
       "1  [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
       "2  [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
       "3  [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
       "4    [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
       "\n",
       "                                        tagged_lemma  \\\n",
       "0  [('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...   \n",
       "1  [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
       "2  [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
       "3  [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
       "4    [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
       "\n",
       "                                   tagged_stan_token  \\\n",
       "0  [('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...   \n",
       "1  [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
       "2  [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
       "3  [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
       "4    [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
       "\n",
       "                                   tagged_stan_lemma  \\\n",
       "0  [('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...   \n",
       "1  [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
       "2  [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
       "3  [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
       "4    [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
       "\n",
       "                                file  \\\n",
       "0  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "1  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "2  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "3  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "4  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "\n",
       "                                            content1  \\\n",
       "0       i thought that maybe that would do something   \n",
       "1  i think you have to c maybe close it and it wi...   \n",
       "2         should i restart this it's like down there   \n",
       "3                       yeah i would just restart it   \n",
       "4                                    okay now what     \n",
       "\n",
       "                                            content2 utter_order  \\\n",
       "0  i think you have to c maybe close it and it wi...     PC: PA:   \n",
       "1         should i restart this it's like down there     PA: PC:   \n",
       "2                       yeah i would just restart it     PC: PA:   \n",
       "3                                    okay now what       PA: PC:   \n",
       "4                                you didn't close it     PC: PA:   \n",
       "\n",
       "                                  content1_embedding  \\\n",
       "0  [[0.16115269, -0.18967551, -0.091776446, 0.072...   \n",
       "1  [[0.16965881, -0.09342009, 0.31895435, 0.05972...   \n",
       "2  [[0.16538174, 0.040479627, 0.08581757, 0.03785...   \n",
       "3  [[0.42204928, -0.110650204, 0.27723312, 0.3426...   \n",
       "4  [[0.29328585, -0.4999022, 0.16143966, 0.002446...   \n",
       "\n",
       "                                  content2_embedding  \\\n",
       "0  [[0.16965881, -0.09342009, 0.31895435, 0.05972...   \n",
       "1  [[0.16538174, 0.040479627, 0.08581757, 0.03785...   \n",
       "2  [[0.42204928, -0.110650204, 0.27723312, 0.3426...   \n",
       "3  [[0.29328585, -0.4999022, 0.16143966, 0.002446...   \n",
       "4  [[0.29594588, 0.2687162, -0.15591031, 0.149712...   \n",
       "\n",
       "   content1_embedding_content2_embedding_cosine_similarity  \n",
       "0                                           0.631972        \n",
       "1                                           0.730899        \n",
       "2                                           0.633439        \n",
       "3                                           0.640285        \n",
       "4                                           0.505967        "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEGIN GPT\n",
    "\n",
    "#### Steps: Pair and lag columns → Compute embeddings → Compute cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_embedding_engine = \"text-embedding-ada-002\"  \n",
    "def get_embedding_with_cache(text: str, engine: str = default_embedding_engine) -> list:\n",
    "    \"\"\"\n",
    "    Generates a GPT embedding for a given text while utilizing a cache to avoid redundant computations.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    if (text, engine) not in embedding_cache:\n",
    "        embedding_cache[(text, engine)] = openai.embeddings.create(input=[text], model=engine).data[0].embedding\n",
    "    return embedding_cache[(text, engine)]\n",
    "\n",
    "def process_file_for_GPT(file_path, embedding_cache, engine=default_embedding_engine):\n",
    "    \"\"\"\n",
    "    Processes a single file to compute GPT embeddings for pairs of utterances and \n",
    "    calculates the cosine similarity between these embeddings.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # Pair and lag the columns\n",
    "    df = pair_and_lag_columns(df, columns_to_lag=['content'])\n",
    "\n",
    "    # Compute embeddings for the lagged columns\n",
    "    for column in [\"content1\", \"content2\"]:\n",
    "        df[f\"{column}_embedding\"] = df[column].apply(lambda x: get_embedding_with_cache(x, engine))\n",
    "\n",
    "    # Calculate cosine similarities between embeddings\n",
    "    embedding_columns = [(\"content1_embedding\", \"content2_embedding\")]\n",
    "    df = calculate_cosine_similarity(df, embedding_columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2/2 [00:00<00:00, 76.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Attempts to load a pre-existing embedding cache from a file (gpt_embedding_cache.pkl).\n",
    "embedding_cache_path = \"data/gpt_embedding_cache.pkl\"\n",
    "try:\n",
    "    with open(embedding_cache_path, \"rb\") as f:\n",
    "        embedding_cache = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "\n",
    "# Path to the folder containing the text files\n",
    "folder_path = \"./data/prepped_stan_small\"\n",
    "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "\n",
    "# This loop processes each text file one by one:\n",
    "concatenated_df = pd.DataFrame()\n",
    "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = process_file_for_GPT(file_path, embedding_cache, default_embedding_engine)\n",
    "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "    with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "        pickle.dump(embedding_cache, embedding_cache_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tagged_token</th>\n",
       "      <th>tagged_lemma</th>\n",
       "      <th>tagged_stan_token</th>\n",
       "      <th>tagged_stan_lemma</th>\n",
       "      <th>file</th>\n",
       "      <th>content1</th>\n",
       "      <th>content2</th>\n",
       "      <th>utter_order</th>\n",
       "      <th>content1_embedding</th>\n",
       "      <th>content2_embedding</th>\n",
       "      <th>content1_embedding_content2_embedding_cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC:</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>['i', 'thought', 'that', 'maybe', 'that', 'wou...</td>\n",
       "      <td>['i', 'think', 'that', 'maybe', 'that', 'would...</td>\n",
       "      <td>[('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
       "      <td>[('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
       "      <td>[('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i thought that maybe that would do something</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[-0.032679855823516846, 0.00039228188688866794...</td>\n",
       "      <td>[0.0034637455828487873, 0.003785194829106331, ...</td>\n",
       "      <td>0.807814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PA:</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
       "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
       "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
       "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
       "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>i think you have to c maybe close it and it wi...</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>PA: PC:</td>\n",
       "      <td>[0.0034637455828487873, 0.003785194829106331, ...</td>\n",
       "      <td>[-0.0030778965447098017, -0.018122322857379913...</td>\n",
       "      <td>0.781031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PC:</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>['should', 'i', 'start', 'this', 'it', 'is', '...</td>\n",
       "      <td>['should', 'i', 'start', 'this', 'it', 'be', '...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
       "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>should i restart this it's like down there</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[-0.0030778965447098017, -0.018122322857379913...</td>\n",
       "      <td>[-0.003490022150799632, -0.007978125475347042,...</td>\n",
       "      <td>0.862541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PA:</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
       "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
       "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
       "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>yeah i would just restart it</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>PA: PC:</td>\n",
       "      <td>[-0.003490022150799632, -0.007978125475347042,...</td>\n",
       "      <td>[0.006083404179662466, -0.016767257824540138, ...</td>\n",
       "      <td>0.766547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PC:</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>['okay', 'now', 'what']</td>\n",
       "      <td>['okay', 'now', 'what']</td>\n",
       "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
       "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
       "      <td>okay now what</td>\n",
       "      <td>you didn't close it</td>\n",
       "      <td>PC: PA:</td>\n",
       "      <td>[0.006083404179662466, -0.016767257824540138, ...</td>\n",
       "      <td>[0.005405202973634005, -0.007813462056219578, ...</td>\n",
       "      <td>0.750888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant                                            content  \\\n",
       "0         PC:       i thought that maybe that would do something   \n",
       "1         PA:  i think you have to c maybe close it and it wi...   \n",
       "2         PC:         should i restart this it's like down there   \n",
       "3         PA:                       yeah i would just restart it   \n",
       "4         PC:                                    okay now what     \n",
       "\n",
       "                                               token  \\\n",
       "0  ['i', 'thought', 'that', 'maybe', 'that', 'wou...   \n",
       "1  ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
       "2  ['should', 'i', 'start', 'this', 'it', 'is', '...   \n",
       "3      ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
       "4                            ['okay', 'now', 'what']   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  ['i', 'think', 'that', 'maybe', 'that', 'would...   \n",
       "1  ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
       "2  ['should', 'i', 'start', 'this', 'it', 'be', '...   \n",
       "3      ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
       "4                            ['okay', 'now', 'what']   \n",
       "\n",
       "                                        tagged_token  \\\n",
       "0  [('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...   \n",
       "1  [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
       "2  [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
       "3  [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
       "4    [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
       "\n",
       "                                        tagged_lemma  \\\n",
       "0  [('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...   \n",
       "1  [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
       "2  [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
       "3  [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
       "4    [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
       "\n",
       "                                   tagged_stan_token  \\\n",
       "0  [('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...   \n",
       "1  [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
       "2  [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
       "3  [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
       "4    [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
       "\n",
       "                                   tagged_stan_lemma  \\\n",
       "0  [('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...   \n",
       "1  [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
       "2  [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
       "3  [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
       "4    [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
       "\n",
       "                                file  \\\n",
       "0  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "1  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "2  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "3  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "4  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
       "\n",
       "                                            content1  \\\n",
       "0       i thought that maybe that would do something   \n",
       "1  i think you have to c maybe close it and it wi...   \n",
       "2         should i restart this it's like down there   \n",
       "3                       yeah i would just restart it   \n",
       "4                                    okay now what     \n",
       "\n",
       "                                            content2 utter_order  \\\n",
       "0  i think you have to c maybe close it and it wi...     PC: PA:   \n",
       "1         should i restart this it's like down there     PA: PC:   \n",
       "2                       yeah i would just restart it     PC: PA:   \n",
       "3                                    okay now what       PA: PC:   \n",
       "4                                you didn't close it     PC: PA:   \n",
       "\n",
       "                                  content1_embedding  \\\n",
       "0  [-0.032679855823516846, 0.00039228188688866794...   \n",
       "1  [0.0034637455828487873, 0.003785194829106331, ...   \n",
       "2  [-0.0030778965447098017, -0.018122322857379913...   \n",
       "3  [-0.003490022150799632, -0.007978125475347042,...   \n",
       "4  [0.006083404179662466, -0.016767257824540138, ...   \n",
       "\n",
       "                                  content2_embedding  \\\n",
       "0  [0.0034637455828487873, 0.003785194829106331, ...   \n",
       "1  [-0.0030778965447098017, -0.018122322857379913...   \n",
       "2  [-0.003490022150799632, -0.007978125475347042,...   \n",
       "3  [0.006083404179662466, -0.016767257824540138, ...   \n",
       "4  [0.005405202973634005, -0.007813462056219578, ...   \n",
       "\n",
       "   content1_embedding_content2_embedding_cosine_similarity  \n",
       "0                                           0.807814        \n",
       "1                                           0.781031        \n",
       "2                                           0.862541        \n",
       "3                                           0.766547        \n",
       "4                                           0.750888        "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
