{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip uninstall transformers\n",
        "# pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "# pip install git+https://github.com/huggingface/peft.git\n",
        "# pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os  # Provides a way to use operating system dependent functionality like reading or writing to the file system\n",
        "import pickle\n",
        "import pandas as pd  # Powerful data structures for data analysis, time series, and statistics\n",
        "import numpy as np  # Support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays\n",
        "from tqdm import tqdm  # For progress bars\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # Computes the cosine similarity between samples in a matrix\n",
        "\n",
        "# For W2V specifically\n",
        "import re  # Provides regular expression matching operations\n",
        "import ast  # Abstract Syntax Trees, used for parsing and analyzing Python source code\n",
        "from collections import Counter  # Provides a way to count the frequency of elements in a collection\n",
        "import gensim  # Library for unsupervised topic modeling and natural language processing\n",
        "import gensim.downloader as api  # Downloads and loads pre-trained models and datasets\n",
        "# from gensim.models import KeyedVectors  # Provides efficient word vector representation and storage\n",
        "\n",
        "# For BERT specifically \n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# For GPT specifically\n",
        "import openai\n",
        "openai.api_key = [ENTER HERE]\n",
        "\n",
        "# For LLAMA specifically\n",
        "import torch\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM\n",
        "from transformers import GenerationConfig\n",
        "token = \"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\"\n",
        "tokenizer = LLaMATokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_auth_token=token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SETUP for W2V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local cache directory: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data\n",
            "Gensim BASE_DIR set to: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data\n",
            "Model word2vec-google-news-300 already exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/word2vec-google-news-300\n",
            "Model glove-twitter-200 already exists at: /Users/nduran4/Desktop/GitProjects/llm-linguistic-alignment/gensim-data/glove-twitter-200\n"
          ]
        }
      ],
      "source": [
        "# retrieve the curent working directory where the script is being executed\n",
        "script_dir = os.getcwd()\n",
        "\n",
        "# create a directory called \"gensim-data\" (if doesn't already exist)\n",
        "local_cache_dir = os.path.join(script_dir, \"gensim-data\")\n",
        "os.makedirs(local_cache_dir, exist_ok=True)\n",
        "print(f\"Local cache directory: {local_cache_dir}\")\n",
        "\n",
        "# configure Gensim to use local_cache_dir as base directory for downloading and storing models\n",
        "api.BASE_DIR = local_cache_dir\n",
        "print(f\"Gensim BASE_DIR set to: {api.BASE_DIR}\")\n",
        "\n",
        "# checks if specified models are already downloaded to cache directory, if not, download them\n",
        "def download_and_cache_models(models, cache_dir):\n",
        "    api.BASE_DIR = cache_dir\n",
        "    for model_name in models:\n",
        "        model_path = os.path.join(cache_dir, model_name)\n",
        "        if not os.path.exists(model_path):\n",
        "            try:\n",
        "                print(f\"Downloading model: {model_name}\")\n",
        "                model = api.load(model_name)\n",
        "                print(f\"Downloaded and cached model: {model_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {model_name}: {e}\")\n",
        "        else:\n",
        "            print(f\"Model {model_name} already exists at: {model_path}\")\n",
        "\n",
        "# specifies the list of models to be cached locally, invoking the download_and_cache_models function \n",
        "models_to_cache = ['word2vec-google-news-300', 'glove-twitter-200']\n",
        "download_and_cache_models(models_to_cache, local_cache_dir)\n",
        "\n",
        "# attempts to load a model from specified file path\n",
        "def load_model_if_not_exists(model_path, binary=True):\n",
        "    try:\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file {model_path} does not exist.\")\n",
        "        return gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=binary)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from {model_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# checks if the w2v_google_model is already loaded in the global namespace. if not, attempts to load it from local cache directory.\n",
        "if 'w2v_google_model' not in globals():\n",
        "    w2v_google_model_path = os.path.join(local_cache_dir, 'word2vec-google-news-300', 'word2vec-google-news-300.gz')\n",
        "    w2v_google_model = load_model_if_not_exists(w2v_google_model_path, binary=True)\n",
        "    if w2v_google_model is not None:\n",
        "        print(\"Word2Vec Google News model loaded from local cache successfully.\")\n",
        "    else:\n",
        "        print(\"Failed to load Word2Vec Google News model.\")\n",
        "\n",
        "# note: possible todo: is it more efficient to use gensim.downloader.load(model_name)?\n",
        "# note, downloading model, it downloads properly, but also throws the exception warning for some reason. \n",
        "# TODO: instead of just loading google news model into global workspace, load in all within \"models_to_cache\" list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FUNCTION DEFINITIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_conversations(folder_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates multiple .txt files located in a specified folder \n",
        "    into a single pandas DataFrame. Each file is expected to be \n",
        "    tab-separated. \n",
        "    \n",
        "    Returns a DataFrame containing the concatenated content of all \n",
        "    the .txt files\n",
        "    \"\"\"\n",
        "    text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
        "    concatenated_df = pd.DataFrame()\n",
        "\n",
        "    for file_name in text_files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
        "    \n",
        "    return concatenated_df\n",
        "\n",
        "def build_filtered_vocab(data: pd.DataFrame, output_file_directory: str, high_sd_cutoff: float = 3, low_n_cutoff: int = 1):\n",
        "    \"\"\"\n",
        "    Constructs a vocabulary from the ‘lemma’ column of the input DataFrame, \n",
        "    applying frequency-based filtering: ords occurring less frequently \n",
        "    than low_n_cutoff or more frequently than a certain standard deviation \n",
        "    above the mean (high_sd_cutoff) are filtered out. \n",
        "    \n",
        "    Returns: Two lists: one with all vocabulary words and another with filtered words\n",
        "    Outputs: The vocabulary frequencies to files\n",
        "    \"\"\" \n",
        "\n",
        "    all_sentences = [re.sub(r'[^\\w\\s]+', '', str(row)).split() for row in data['lemma']]\n",
        "    all_words = [word for sentence in all_sentences for word in sentence]\n",
        "\n",
        "    frequency = Counter(all_words)\n",
        "\n",
        "    frequency_filt = {word: freq for word, freq in frequency.items() if len(word) > 1 and freq > low_n_cutoff}\n",
        "    \n",
        "    if high_sd_cutoff is not None:\n",
        "        mean_freq = np.mean(list(frequency_filt.values()))\n",
        "        std_freq = np.std(list(frequency_filt.values()))\n",
        "        cutoff_freq = mean_freq + (std_freq * high_sd_cutoff)\n",
        "        filteredWords = {word: freq for word, freq in frequency_filt.items() if freq < cutoff_freq}\n",
        "    else:\n",
        "        filteredWords = frequency_filt\n",
        "  \n",
        "    vocabfreq_all = pd.DataFrame(frequency.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
        "    vocabfreq_filt = pd.DataFrame(filteredWords.items(), columns=[\"word\", \"count\"]).sort_values(by='count', ascending=False)\n",
        "  \n",
        "    vocabfreq_all.to_csv(os.path.join(output_file_directory, 'vocab_unfilt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
        "    vocabfreq_filt.to_csv(os.path.join(output_file_directory, 'vocab_filt_freqs.txt'), encoding='utf-8', index=False, sep='\\t')\n",
        "    \n",
        "    return list(frequency.keys()), list(filteredWords.keys())\n",
        "\n",
        "def is_list_like_column(series):\n",
        "    \"\"\"\n",
        "    Checks if a pandas Series contains list-like strings (i.e., strings that \n",
        "    look like lists).\n",
        "    \"\"\"    \n",
        "\n",
        "    try:\n",
        "        return series.apply(lambda x: x.strip().startswith(\"[\")).all()\n",
        "    except AttributeError:\n",
        "        return False\n",
        "\n",
        "def convert_columns_to_lists(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converts any columns in a DataFrame that contain list-like strings into \n",
        "    actual Python lists using ast.literal_eval.\n",
        "    \"\"\"        \n",
        "\n",
        "    columns_converted = []\n",
        "    for col in df.columns:\n",
        "        if is_list_like_column(df[col]):\n",
        "            df[col] = df[col].apply(ast.literal_eval)\n",
        "            columns_converted.append(col)\n",
        "    return df, columns_converted\n",
        "\n",
        "def pair_columns_lagged(df: pd.DataFrame, include_stan: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates lagged pairs of specified columns, generating new columns with a \n",
        "    suffix of ‘1’ for the original content and ‘2’ for the lagged content. Also \n",
        "    adds a new column indicating the order of participants between successive rows.\n",
        "    \"\"\"   \n",
        "\n",
        "    columns_to_lag = ['content', 'token', 'lemma', 'tagged_token', 'tagged_lemma']\n",
        "    \n",
        "    if include_stan:\n",
        "        stan_columns = [col for col in df.columns if 'stan' in col]\n",
        "        columns_to_lag.extend(stan_columns)\n",
        "    \n",
        "    for col in columns_to_lag:\n",
        "        if col in df.columns:  \n",
        "            df[f'{col}1'] = df[col]\n",
        "            df[f'{col}2'] = df[col].shift(-1)\n",
        "    \n",
        "    df['utter_order'] = df['participant'] + ' ' + df['participant'].shift(-1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def compute_cosine_similarities(df: pd.DataFrame, columns: list):\n",
        "    \"\"\"\n",
        "    Computes cosine similarities between pairs of vectors in the specified columns \n",
        "    and adds the results as new columns in the DataFrame. Handles ‘token’ and ‘lemma’ \n",
        "    columns separately.\n",
        "    \"\"\"   \n",
        "\n",
        "    for col1, col2 in columns:\n",
        "        similarities = []\n",
        "        for i in range(len(df)):\n",
        "            vec1 = df.iloc[i][col1]\n",
        "            vec2 = df.iloc[i][col2]\n",
        "            if vec1 is not None and vec2 is not None:\n",
        "                similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
        "            else:\n",
        "                similarity = None\n",
        "            similarities.append(similarity)\n",
        "\n",
        "        if 'token' in col1:\n",
        "            similarity_column_name = 'token_cosine_similarity'\n",
        "        elif 'lemma' in col1:\n",
        "            similarity_column_name = 'lemma_cosine_similarity'\n",
        "\n",
        "        df[similarity_column_name] = similarities\n",
        "    return df\n",
        "\n",
        "def get_sum_embeddings(token_list, model):\n",
        "    \"\"\"\n",
        "    Calculates the sum of word embeddings for a list of tokens using a \n",
        "    pre-trained Word2Vec model.\n",
        "    \"\"\" \n",
        "\n",
        "    if token_list is None:\n",
        "        return None    \n",
        "    embeddings = []\n",
        "    for word in token_list:\n",
        "        if word in model.key_to_index:  \n",
        "            embeddings.append(model[word])    \n",
        "    if embeddings:\n",
        "        sum_embedding = np.sum(embeddings, axis=0)\n",
        "        return sum_embedding\n",
        "    else:\n",
        "        return None  \n",
        "    \n",
        "def process_file_for_W2V(file_path, vocab_list: list):     \n",
        "    \"\"\"\n",
        "    Processes a file containing conversation data, filters tokens \n",
        "    based on a provided vocabulary list, pairs and lags columns, computes \n",
        "    word embeddings, and then calculates cosine similarities between the embeddings.\n",
        "    \"\"\" \n",
        "\n",
        "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "\n",
        "    df, columns_converted = convert_columns_to_lists(df)\n",
        "\n",
        "    columns_to_filter = ['lemma','token']\n",
        "    for col in columns_to_filter:\n",
        "\n",
        "        df[col] = df[col].apply(lambda token_list: [word for word in token_list if word in vocab_list])\n",
        "\n",
        "    df = pair_columns_lagged(df)\n",
        "\n",
        "    for column in [\"lemma\", \"token\"]:\n",
        "        df[f\"{column}1_sum_embedding\"] = df[f\"{column}1\"].apply(lambda tokens: get_sum_embeddings(tokens, w2v_google_model))\n",
        "        df[f\"{column}2_sum_embedding\"] = df[f\"{column}2\"].apply(lambda tokens: get_sum_embeddings(tokens, w2v_google_model))\n",
        "\n",
        "    embedding_columns = [\n",
        "        (\"lemma1_sum_embedding\", \"lemma2_sum_embedding\"),\n",
        "        (\"token1_sum_embedding\", \"token2_sum_embedding\")\n",
        "    ] \n",
        "\n",
        "    df = compute_cosine_similarities(df, embedding_columns)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:00<00:00, 34.29it/s]\n"
          ]
        }
      ],
      "source": [
        "# Define folder paths\n",
        "folder_path = \"./data/prepped_stan_small\"\n",
        "output_file_directory = \"outputW2V\"\n",
        "\n",
        "# Check if the output directory exists, create it if not\n",
        "os.makedirs(output_file_directory, exist_ok=True)\n",
        "\n",
        "# List all text files in the folder\n",
        "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
        "\n",
        "# Aggregate the conversations into a single DataFrame\n",
        "concatenated_text_files = aggregate_conversations(folder_path)\n",
        "\n",
        "# Build the filtered vocabulary and save it to the output directory\n",
        "vocab_all, vocab_filtered = build_filtered_vocab(concatenated_text_files, output_file_directory)\n",
        "\n",
        "# Process each file and concatenate the results into a single DataFrame\n",
        "concatenated_df = pd.DataFrame()\n",
        "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = process_file_for_W2V(file_path, vocab_filtered)\n",
        "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>content1</th>\n",
              "      <th>...</th>\n",
              "      <th>tagged_stan_token2</th>\n",
              "      <th>tagged_stan_lemma1</th>\n",
              "      <th>tagged_stan_lemma2</th>\n",
              "      <th>utter_order</th>\n",
              "      <th>lemma1_sum_embedding</th>\n",
              "      <th>lemma2_sum_embedding</th>\n",
              "      <th>token1_sum_embedding</th>\n",
              "      <th>token2_sum_embedding</th>\n",
              "      <th>lemma_cosine_similarity</th>\n",
              "      <th>token_cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PC:</td>\n",
              "      <td>i thought that maybe that would do something</td>\n",
              "      <td>[that, maybe, that, would, do, something]</td>\n",
              "      <td>[think, that, maybe, that, would, do, something]</td>\n",
              "      <td>[(i, NN), (thought, VBD), (that, IN), (maybe, ...</td>\n",
              "      <td>[(i, NN), (think, VBP), (that, IN), (maybe, RB...</td>\n",
              "      <td>[(i, LS), (thought, VBD), (that, IN), (maybe, ...</td>\n",
              "      <td>[(i, FW), (think, VBP), (that, IN), (maybe, RB...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>i thought that maybe that would do something</td>\n",
              "      <td>...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>[(i, FW), (think, VBP), (that, IN), (maybe, RB...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>PC: PA:</td>\n",
              "      <td>[0.33276367, 0.18133545, 0.54364014, 1.277832,...</td>\n",
              "      <td>[0.71087646, 0.19366455, 0.7921753, 2.4785156,...</td>\n",
              "      <td>[0.37963867, 0.11444092, 0.53430176, 1.0141602...</td>\n",
              "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
              "      <td>0.861987</td>\n",
              "      <td>0.826057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PA:</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>[think, you, have, maybe, close, and, will, yo...</td>\n",
              "      <td>[think, you, have, maybe, close, and, will, yo...</td>\n",
              "      <td>[(i, NN), (think, VBP), (you, PRP), (have, VBP...</td>\n",
              "      <td>[(i, NN), (think, VBP), (you, PRP), (have, VBP...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>PA: PC:</td>\n",
              "      <td>[0.71087646, 0.19366455, 0.7921753, 2.4785156,...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>0.749735</td>\n",
              "      <td>0.751777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PC:</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>[start, this, like, down, there]</td>\n",
              "      <td>[start, this, like, down, there]</td>\n",
              "      <td>[(should, MD), (i, VB), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, VB), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>...</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>PC: PA:</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>0.777175</td>\n",
              "      <td>0.777175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PA:</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>[yeah, would, just, start]</td>\n",
              "      <td>[yeah, would, just, start]</td>\n",
              "      <td>[(yeah, NN), (i, NN), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, NN), (i, NN), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>...</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>PA: PC:</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>0.720251</td>\n",
              "      <td>0.720251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PC:</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>[okay, now, what]</td>\n",
              "      <td>[okay, now, what]</td>\n",
              "      <td>[(okay, RB), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, RB), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>...</td>\n",
              "      <td>[(you, PRP), (did, VBD), (not, RB), (close, VB...</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>[(you, PRP), (do, VBP), (not, RB), (close, VB)...</td>\n",
              "      <td>PC: PA:</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.40890503, -0.14001465, 0.30407715, 0.839355...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.28585815, -0.15283203, 0.28466797, 0.608886...</td>\n",
              "      <td>0.661706</td>\n",
              "      <td>0.616577</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  participant                                            content  \\\n",
              "0         PC:       i thought that maybe that would do something   \n",
              "1         PA:  i think you have to c maybe close it and it wi...   \n",
              "2         PC:         should i restart this it's like down there   \n",
              "3         PA:                       yeah i would just restart it   \n",
              "4         PC:                                    okay now what     \n",
              "\n",
              "                                               token  \\\n",
              "0          [that, maybe, that, would, do, something]   \n",
              "1  [think, you, have, maybe, close, and, will, yo...   \n",
              "2                   [start, this, like, down, there]   \n",
              "3                         [yeah, would, just, start]   \n",
              "4                                  [okay, now, what]   \n",
              "\n",
              "                                               lemma  \\\n",
              "0   [think, that, maybe, that, would, do, something]   \n",
              "1  [think, you, have, maybe, close, and, will, yo...   \n",
              "2                   [start, this, like, down, there]   \n",
              "3                         [yeah, would, just, start]   \n",
              "4                                  [okay, now, what]   \n",
              "\n",
              "                                        tagged_token  \\\n",
              "0  [(i, NN), (thought, VBD), (that, IN), (maybe, ...   \n",
              "1  [(i, NN), (think, VBP), (you, PRP), (have, VBP...   \n",
              "2  [(should, MD), (i, VB), (start, VB), (this, DT...   \n",
              "3  [(yeah, NN), (i, NN), (would, MD), (just, RB),...   \n",
              "4                [(okay, RB), (now, RB), (what, WP)]   \n",
              "\n",
              "                                        tagged_lemma  \\\n",
              "0  [(i, NN), (think, VBP), (that, IN), (maybe, RB...   \n",
              "1  [(i, NN), (think, VBP), (you, PRP), (have, VBP...   \n",
              "2  [(should, MD), (i, VB), (start, VB), (this, DT...   \n",
              "3  [(yeah, NN), (i, NN), (would, MD), (just, RB),...   \n",
              "4                [(okay, RB), (now, RB), (what, WP)]   \n",
              "\n",
              "                                   tagged_stan_token  \\\n",
              "0  [(i, LS), (thought, VBD), (that, IN), (maybe, ...   \n",
              "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
              "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
              "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
              "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
              "\n",
              "                                   tagged_stan_lemma  \\\n",
              "0  [(i, FW), (think, VBP), (that, IN), (maybe, RB...   \n",
              "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
              "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
              "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
              "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
              "\n",
              "                                file  \\\n",
              "0  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "1  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "2  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "3  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "4  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "\n",
              "                                            content1  ...  \\\n",
              "0       i thought that maybe that would do something  ...   \n",
              "1  i think you have to c maybe close it and it wi...  ...   \n",
              "2         should i restart this it's like down there  ...   \n",
              "3                       yeah i would just restart it  ...   \n",
              "4                                    okay now what    ...   \n",
              "\n",
              "                                  tagged_stan_token2  \\\n",
              "0  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
              "1  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
              "2  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
              "3                [(okay, JJ), (now, RB), (what, WP)]   \n",
              "4  [(you, PRP), (did, VBD), (not, RB), (close, VB...   \n",
              "\n",
              "                                  tagged_stan_lemma1  \\\n",
              "0  [(i, FW), (think, VBP), (that, IN), (maybe, RB...   \n",
              "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
              "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
              "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
              "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
              "\n",
              "                                  tagged_stan_lemma2 utter_order  \\\n",
              "0  [(i, LS), (think, VB), (you, PRP), (have, VBP)...     PC: PA:   \n",
              "1  [(should, MD), (i, FW), (start, VB), (this, DT...     PA: PC:   \n",
              "2  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...     PC: PA:   \n",
              "3                [(okay, JJ), (now, RB), (what, WP)]     PA: PC:   \n",
              "4  [(you, PRP), (do, VBP), (not, RB), (close, VB)...     PC: PA:   \n",
              "\n",
              "                                lemma1_sum_embedding  \\\n",
              "0  [0.33276367, 0.18133545, 0.54364014, 1.277832,...   \n",
              "1  [0.71087646, 0.19366455, 0.7921753, 2.4785156,...   \n",
              "2  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "3  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "4  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "\n",
              "                                lemma2_sum_embedding  \\\n",
              "0  [0.71087646, 0.19366455, 0.7921753, 2.4785156,...   \n",
              "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "4  [0.40890503, -0.14001465, 0.30407715, 0.839355...   \n",
              "\n",
              "                                token1_sum_embedding  \\\n",
              "0  [0.37963867, 0.11444092, 0.53430176, 1.0141602...   \n",
              "1  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...   \n",
              "2  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "3  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "4  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "\n",
              "                                token2_sum_embedding lemma_cosine_similarity  \\\n",
              "0  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...                0.861987   \n",
              "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...                0.749735   \n",
              "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...                0.777175   \n",
              "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...                0.720251   \n",
              "4  [0.28585815, -0.15283203, 0.28466797, 0.608886...                0.661706   \n",
              "\n",
              "  token_cosine_similarity  \n",
              "0                0.826057  \n",
              "1                0.751777  \n",
              "2                0.777175  \n",
              "3                0.720251  \n",
              "4                0.616577  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(65, 30)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify and rename the semantic specific columns\n",
        "df = concatenated_df.filter(like='embedding').join(concatenated_df.filter(like='cosine_similarity'))\n",
        "df = df.rename(columns=lambda x: f\"{x}_W2V\")\n",
        "\n",
        "# Merge these renamed columns back to the original dataframe\n",
        "concatenated_df = concatenated_df.join(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>content1</th>\n",
              "      <th>...</th>\n",
              "      <th>token1_sum_embedding</th>\n",
              "      <th>token2_sum_embedding</th>\n",
              "      <th>lemma_cosine_similarity</th>\n",
              "      <th>token_cosine_similarity</th>\n",
              "      <th>lemma1_sum_embedding_W2V</th>\n",
              "      <th>lemma2_sum_embedding_W2V</th>\n",
              "      <th>token1_sum_embedding_W2V</th>\n",
              "      <th>token2_sum_embedding_W2V</th>\n",
              "      <th>lemma_cosine_similarity_W2V</th>\n",
              "      <th>token_cosine_similarity_W2V</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PC:</td>\n",
              "      <td>i thought that maybe that would do something</td>\n",
              "      <td>[that, maybe, that, would, do, something]</td>\n",
              "      <td>[think, that, maybe, that, would, do, something]</td>\n",
              "      <td>[(i, NN), (thought, VBD), (that, IN), (maybe, ...</td>\n",
              "      <td>[(i, NN), (think, VBP), (that, IN), (maybe, RB...</td>\n",
              "      <td>[(i, LS), (thought, VBD), (that, IN), (maybe, ...</td>\n",
              "      <td>[(i, FW), (think, VBP), (that, IN), (maybe, RB...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>i thought that maybe that would do something</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.37963867, 0.11444092, 0.53430176, 1.0141602...</td>\n",
              "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
              "      <td>0.861987</td>\n",
              "      <td>0.826057</td>\n",
              "      <td>[0.33276367, 0.18133545, 0.54364014, 1.277832,...</td>\n",
              "      <td>[0.71087646, 0.19366455, 0.7921753, 2.4785156,...</td>\n",
              "      <td>[0.37963867, 0.11444092, 0.53430176, 1.0141602...</td>\n",
              "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
              "      <td>0.861987</td>\n",
              "      <td>0.826057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PA:</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>[think, you, have, maybe, close, and, will, yo...</td>\n",
              "      <td>[think, you, have, maybe, close, and, will, yo...</td>\n",
              "      <td>[(i, NN), (think, VBP), (you, PRP), (have, VBP...</td>\n",
              "      <td>[(i, NN), (think, VBP), (you, PRP), (have, VBP...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>0.749735</td>\n",
              "      <td>0.751777</td>\n",
              "      <td>[0.71087646, 0.19366455, 0.7921753, 2.4785156,...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>0.749735</td>\n",
              "      <td>0.751777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PC:</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>[start, this, like, down, there]</td>\n",
              "      <td>[start, this, like, down, there]</td>\n",
              "      <td>[(should, MD), (i, VB), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, VB), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>0.777175</td>\n",
              "      <td>0.777175</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>0.777175</td>\n",
              "      <td>0.777175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PA:</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>[yeah, would, just, start]</td>\n",
              "      <td>[yeah, would, just, start]</td>\n",
              "      <td>[(yeah, NN), (i, NN), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, NN), (i, NN), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>0.720251</td>\n",
              "      <td>0.720251</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>0.720251</td>\n",
              "      <td>0.720251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PC:</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>[okay, now, what]</td>\n",
              "      <td>[okay, now, what]</td>\n",
              "      <td>[(okay, RB), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, RB), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.28585815, -0.15283203, 0.28466797, 0.608886...</td>\n",
              "      <td>0.661706</td>\n",
              "      <td>0.616577</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.40890503, -0.14001465, 0.30407715, 0.839355...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.28585815, -0.15283203, 0.28466797, 0.608886...</td>\n",
              "      <td>0.661706</td>\n",
              "      <td>0.616577</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  participant                                            content  \\\n",
              "0         PC:       i thought that maybe that would do something   \n",
              "1         PA:  i think you have to c maybe close it and it wi...   \n",
              "2         PC:         should i restart this it's like down there   \n",
              "3         PA:                       yeah i would just restart it   \n",
              "4         PC:                                    okay now what     \n",
              "\n",
              "                                               token  \\\n",
              "0          [that, maybe, that, would, do, something]   \n",
              "1  [think, you, have, maybe, close, and, will, yo...   \n",
              "2                   [start, this, like, down, there]   \n",
              "3                         [yeah, would, just, start]   \n",
              "4                                  [okay, now, what]   \n",
              "\n",
              "                                               lemma  \\\n",
              "0   [think, that, maybe, that, would, do, something]   \n",
              "1  [think, you, have, maybe, close, and, will, yo...   \n",
              "2                   [start, this, like, down, there]   \n",
              "3                         [yeah, would, just, start]   \n",
              "4                                  [okay, now, what]   \n",
              "\n",
              "                                        tagged_token  \\\n",
              "0  [(i, NN), (thought, VBD), (that, IN), (maybe, ...   \n",
              "1  [(i, NN), (think, VBP), (you, PRP), (have, VBP...   \n",
              "2  [(should, MD), (i, VB), (start, VB), (this, DT...   \n",
              "3  [(yeah, NN), (i, NN), (would, MD), (just, RB),...   \n",
              "4                [(okay, RB), (now, RB), (what, WP)]   \n",
              "\n",
              "                                        tagged_lemma  \\\n",
              "0  [(i, NN), (think, VBP), (that, IN), (maybe, RB...   \n",
              "1  [(i, NN), (think, VBP), (you, PRP), (have, VBP...   \n",
              "2  [(should, MD), (i, VB), (start, VB), (this, DT...   \n",
              "3  [(yeah, NN), (i, NN), (would, MD), (just, RB),...   \n",
              "4                [(okay, RB), (now, RB), (what, WP)]   \n",
              "\n",
              "                                   tagged_stan_token  \\\n",
              "0  [(i, LS), (thought, VBD), (that, IN), (maybe, ...   \n",
              "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
              "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
              "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
              "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
              "\n",
              "                                   tagged_stan_lemma  \\\n",
              "0  [(i, FW), (think, VBP), (that, IN), (maybe, RB...   \n",
              "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
              "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
              "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
              "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
              "\n",
              "                                file  \\\n",
              "0  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "1  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "2  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "3  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "4  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "\n",
              "                                            content1  ...  \\\n",
              "0       i thought that maybe that would do something  ...   \n",
              "1  i think you have to c maybe close it and it wi...  ...   \n",
              "2         should i restart this it's like down there  ...   \n",
              "3                       yeah i would just restart it  ...   \n",
              "4                                    okay now what    ...   \n",
              "\n",
              "                                token1_sum_embedding  \\\n",
              "0  [0.37963867, 0.11444092, 0.53430176, 1.0141602...   \n",
              "1  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...   \n",
              "2  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "3  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "4  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "\n",
              "                                token2_sum_embedding lemma_cosine_similarity  \\\n",
              "0  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...                0.861987   \n",
              "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...                0.749735   \n",
              "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...                0.777175   \n",
              "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...                0.720251   \n",
              "4  [0.28585815, -0.15283203, 0.28466797, 0.608886...                0.661706   \n",
              "\n",
              "  token_cosine_similarity                           lemma1_sum_embedding_W2V  \\\n",
              "0                0.826057  [0.33276367, 0.18133545, 0.54364014, 1.277832,...   \n",
              "1                0.751777  [0.71087646, 0.19366455, 0.7921753, 2.4785156,...   \n",
              "2                0.777175  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "3                0.720251  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "4                0.616577  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "\n",
              "                            lemma2_sum_embedding_W2V  \\\n",
              "0  [0.71087646, 0.19366455, 0.7921753, 2.4785156,...   \n",
              "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "4  [0.40890503, -0.14001465, 0.30407715, 0.839355...   \n",
              "\n",
              "                            token1_sum_embedding_W2V  \\\n",
              "0  [0.37963867, 0.11444092, 0.53430176, 1.0141602...   \n",
              "1  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...   \n",
              "2  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "3  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "4  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "\n",
              "                            token2_sum_embedding_W2V  \\\n",
              "0  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...   \n",
              "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "4  [0.28585815, -0.15283203, 0.28466797, 0.608886...   \n",
              "\n",
              "  lemma_cosine_similarity_W2V token_cosine_similarity_W2V  \n",
              "0                    0.861987                    0.826057  \n",
              "1                    0.749735                    0.751777  \n",
              "2                    0.777175                    0.777175  \n",
              "3                    0.720251                    0.720251  \n",
              "4                    0.661706                    0.616577  \n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "W2V_concatenated_df = concatenated_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FUNCTION DEFINITIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_input_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares a DataFrame for further processing by creating new columns:\n",
        "\t•\tutter1: Holds the content of the current utterance.\n",
        "\t•\tutter2: Holds the content of the next utterance (shifted by -1).\n",
        "\t•\tutter_order: Concatenates the current and next participant labels to track the order of interactions.\n",
        "    \"\"\" \n",
        "\n",
        "    df['utter1'] = df['content']\n",
        "    df['utter2'] = df['content'].shift(-1)\n",
        "    df['utter_order'] = df['participant'] + ' ' + df['participant'].shift(-1)\n",
        "    return df\n",
        "\n",
        "def get_embedding_with_cache(text):\n",
        "    \"\"\"\n",
        "    Generates a BERT embedding for a given text while utilizing a cache to avoid redundant computations:\n",
        "\t•\t  Checks if the embedding for the given text is already in the cache. If so, returns it.\n",
        "\t•\t  If not cached, tokenizes the text, converts tokens to IDs, and feeds them to the BERT model to get the embedding.\n",
        "\t•\t  The embedding is then averaged over all tokens and stored in the cache for future use.\n",
        "    \"\"\" \n",
        "\n",
        "    if text is None:\n",
        "      return None\n",
        "\n",
        "    if text in embedding_cache:\n",
        "      return embedding_cache[text]\n",
        "\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    token_ids = [tokenizer.cls_token_id] + token_ids + [tokenizer.sep_token_id]\n",
        "    input_ids = torch.tensor([token_ids])\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    embedding = torch.mean(last_hidden_states, dim=1).numpy()\n",
        "    embedding_cache[text] = embedding\n",
        "\n",
        "    return embedding\n",
        "\n",
        "def process_file(file_path, embedding_cache):            \n",
        "  \"\"\"\n",
        "  Processes a single file to compute BERT embeddings for pairs of utterances and \n",
        "  calculates the cosine similarity between these embeddings:\n",
        "\t•\t  Reads the file into a DataFrame.\n",
        "\t•\t  Processes the input data using process_input_data.\n",
        "\t•\t  Applies the get_embedding_with_cache function to each utterance pair \n",
        "      to compute embeddings.\n",
        "\t•\t  Computes the cosine similarity between embeddings of successive utterances.\n",
        "  \"\"\" \n",
        "  \n",
        "  df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "  df = process_input_data(df)\\\n",
        "\n",
        "  for column in [\"utter1\", \"utter2\"]:\n",
        "    df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n",
        "\n",
        "  df[\"cosine_similarity\"] = df.apply(\n",
        "      lambda row: cosine_similarity(\n",
        "          np.array(row[\"utter1_embedding\"]).reshape(1, -1),\n",
        "          np.array(row[\"utter2_embedding\"]).reshape(1, -1)\n",
        "          )[0][0] if row[\"utter1_embedding\"] is not None and row[\"utter2_embedding\"] is not None else None,\n",
        "      axis=1\n",
        "    )\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\n"
          ]
        }
      ],
      "source": [
        "# Attempts to load a pre-existing embedding cache from a file (bert_embedding_cache.pkl).\n",
        "\t# •\tIf the file exists, it loads the cache using pickle, which allows previously computed \n",
        "    #   embeddings to be reused, saving computation time.\n",
        "\t# •\tIf the file does not exist (e.g., the first time the code is run), it initializes an \n",
        "    #   empty dictionary (embedding_cache = {}) to start building the cache from \n",
        "    #   scratch.embedding_cache_path = \"data/bert_embedding_cache.pkl\"\n",
        "embedding_cache_path = \"data/bert_embedding_cache.pkl\"\n",
        "try:\n",
        "    with open(embedding_cache_path, \"rb\") as f:\n",
        "        embedding_cache = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    embedding_cache = {}\n",
        "\n",
        "# Path to the folder containing the text files\n",
        "folder_path = \"data/prepped_stan_small\"\n",
        "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
        "\n",
        "# This loop processes each text file one by one:\n",
        "\t# •\tFile Processing: For each file, it reads the content, generates BERT embeddings \n",
        "    #   for the utterances, and computes cosine similarities between consecutive utterances \n",
        "    #   using the process_file function.\n",
        "\t# •\tConcatenation: The results from each file are concatenated into a single DataFrame \n",
        "    #   (concatenated_df) that will hold all the processed data. concatenated_df = pd.DataFrame()\n",
        "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = process_file(file_path, embedding_cache)\n",
        "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
        "\n",
        "    with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
        "        pickle.dump(embedding_cache, embedding_cache_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>content1</th>\n",
              "      <th>...</th>\n",
              "      <th>lemma2_sum_embedding</th>\n",
              "      <th>token1_sum_embedding</th>\n",
              "      <th>token2_sum_embedding</th>\n",
              "      <th>lemma_cosine_similarity</th>\n",
              "      <th>token_cosine_similarity</th>\n",
              "      <th>utter1</th>\n",
              "      <th>utter2</th>\n",
              "      <th>utter1_embedding</th>\n",
              "      <th>utter2_embedding</th>\n",
              "      <th>cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PC:</td>\n",
              "      <td>i thought that maybe that would do something</td>\n",
              "      <td>[that, maybe, that, would, do, something]</td>\n",
              "      <td>[think, that, maybe, that, would, do, something]</td>\n",
              "      <td>[(i, NN), (thought, VBD), (that, IN), (maybe, ...</td>\n",
              "      <td>[(i, NN), (think, VBP), (that, IN), (maybe, RB...</td>\n",
              "      <td>[(i, LS), (thought, VBD), (that, IN), (maybe, ...</td>\n",
              "      <td>[(i, FW), (think, VBP), (that, IN), (maybe, RB...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>i thought that maybe that would do something</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.71087646, 0.19366455, 0.7921753, 2.4785156,...</td>\n",
              "      <td>[0.37963867, 0.11444092, 0.53430176, 1.0141602...</td>\n",
              "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
              "      <td>0.861987</td>\n",
              "      <td>0.826057</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PA:</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>[think, you, have, maybe, close, and, will, yo...</td>\n",
              "      <td>[think, you, have, maybe, close, and, will, yo...</td>\n",
              "      <td>[(i, NN), (think, VBP), (you, PRP), (have, VBP...</td>\n",
              "      <td>[(i, NN), (think, VBP), (you, PRP), (have, VBP...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>[(i, LS), (think, VB), (you, PRP), (have, VBP)...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.5878296, 0.18084717, 0.7727661, 2.2480469, ...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>0.749735</td>\n",
              "      <td>0.751777</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PC:</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>[start, this, like, down, there]</td>\n",
              "      <td>[start, this, like, down, there]</td>\n",
              "      <td>[(should, MD), (i, VB), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, VB), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>[(should, MD), (i, FW), (start, VB), (this, DT...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.2972412, 0.25402832, 0.30085754, 0.6894531,...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>0.777175</td>\n",
              "      <td>0.777175</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PA:</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>[yeah, would, just, start]</td>\n",
              "      <td>[yeah, would, just, start]</td>\n",
              "      <td>[(yeah, NN), (i, NN), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, NN), (i, NN), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>[(yeah, JJ), (i, FW), (would, MD), (just, RB),...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.40307617, 0.18392944, 0.17602539, 0.7675781...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>0.720251</td>\n",
              "      <td>0.720251</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PC:</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>[okay, now, what]</td>\n",
              "      <td>[okay, now, what]</td>\n",
              "      <td>[(okay, RB), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, RB), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>[(okay, JJ), (now, RB), (what, WP)]</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.40890503, -0.14001465, 0.30407715, 0.839355...</td>\n",
              "      <td>[0.24768066, -0.050964355, 0.26208496, 0.27709...</td>\n",
              "      <td>[0.28585815, -0.15283203, 0.28466797, 0.608886...</td>\n",
              "      <td>0.661706</td>\n",
              "      <td>0.616577</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  participant                                            content  \\\n",
              "0         PC:       i thought that maybe that would do something   \n",
              "1         PA:  i think you have to c maybe close it and it wi...   \n",
              "2         PC:         should i restart this it's like down there   \n",
              "3         PA:                       yeah i would just restart it   \n",
              "4         PC:                                    okay now what     \n",
              "\n",
              "                                               token  \\\n",
              "0          [that, maybe, that, would, do, something]   \n",
              "1  [think, you, have, maybe, close, and, will, yo...   \n",
              "2                   [start, this, like, down, there]   \n",
              "3                         [yeah, would, just, start]   \n",
              "4                                  [okay, now, what]   \n",
              "\n",
              "                                               lemma  \\\n",
              "0   [think, that, maybe, that, would, do, something]   \n",
              "1  [think, you, have, maybe, close, and, will, yo...   \n",
              "2                   [start, this, like, down, there]   \n",
              "3                         [yeah, would, just, start]   \n",
              "4                                  [okay, now, what]   \n",
              "\n",
              "                                        tagged_token  \\\n",
              "0  [(i, NN), (thought, VBD), (that, IN), (maybe, ...   \n",
              "1  [(i, NN), (think, VBP), (you, PRP), (have, VBP...   \n",
              "2  [(should, MD), (i, VB), (start, VB), (this, DT...   \n",
              "3  [(yeah, NN), (i, NN), (would, MD), (just, RB),...   \n",
              "4                [(okay, RB), (now, RB), (what, WP)]   \n",
              "\n",
              "                                        tagged_lemma  \\\n",
              "0  [(i, NN), (think, VBP), (that, IN), (maybe, RB...   \n",
              "1  [(i, NN), (think, VBP), (you, PRP), (have, VBP...   \n",
              "2  [(should, MD), (i, VB), (start, VB), (this, DT...   \n",
              "3  [(yeah, NN), (i, NN), (would, MD), (just, RB),...   \n",
              "4                [(okay, RB), (now, RB), (what, WP)]   \n",
              "\n",
              "                                   tagged_stan_token  \\\n",
              "0  [(i, LS), (thought, VBD), (that, IN), (maybe, ...   \n",
              "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
              "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
              "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
              "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
              "\n",
              "                                   tagged_stan_lemma  \\\n",
              "0  [(i, FW), (think, VBP), (that, IN), (maybe, RB...   \n",
              "1  [(i, LS), (think, VB), (you, PRP), (have, VBP)...   \n",
              "2  [(should, MD), (i, FW), (start, VB), (this, DT...   \n",
              "3  [(yeah, JJ), (i, FW), (would, MD), (just, RB),...   \n",
              "4                [(okay, JJ), (now, RB), (what, WP)]   \n",
              "\n",
              "                                file  \\\n",
              "0  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "1  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "2  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "3  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "4  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "\n",
              "                                            content1  ...  \\\n",
              "0       i thought that maybe that would do something  ...   \n",
              "1  i think you have to c maybe close it and it wi...  ...   \n",
              "2         should i restart this it's like down there  ...   \n",
              "3                       yeah i would just restart it  ...   \n",
              "4                                    okay now what    ...   \n",
              "\n",
              "                                lemma2_sum_embedding  \\\n",
              "0  [0.71087646, 0.19366455, 0.7921753, 2.4785156,...   \n",
              "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "4  [0.40890503, -0.14001465, 0.30407715, 0.839355...   \n",
              "\n",
              "                                token1_sum_embedding  \\\n",
              "0  [0.37963867, 0.11444092, 0.53430176, 1.0141602...   \n",
              "1  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...   \n",
              "2  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...   \n",
              "3  [0.40307617, 0.18392944, 0.17602539, 0.7675781...   \n",
              "4  [0.24768066, -0.050964355, 0.26208496, 0.27709...   \n",
              "\n",
              "                                token2_sum_embedding lemma_cosine_similarity  \\\n",
              "0  [0.5878296, 0.18084717, 0.7727661, 2.2480469, ...                0.861987   \n",
              "1  [0.2972412, 0.25402832, 0.30085754, 0.6894531,...                0.749735   \n",
              "2  [0.40307617, 0.18392944, 0.17602539, 0.7675781...                0.777175   \n",
              "3  [0.24768066, -0.050964355, 0.26208496, 0.27709...                0.720251   \n",
              "4  [0.28585815, -0.15283203, 0.28466797, 0.608886...                0.661706   \n",
              "\n",
              "  token_cosine_similarity utter1 utter2 utter1_embedding utter2_embedding  \\\n",
              "0                0.826057    NaN    NaN              NaN              NaN   \n",
              "1                0.751777    NaN    NaN              NaN              NaN   \n",
              "2                0.777175    NaN    NaN              NaN              NaN   \n",
              "3                0.720251    NaN    NaN              NaN              NaN   \n",
              "4                0.616577    NaN    NaN              NaN              NaN   \n",
              "\n",
              "  cosine_similarity  \n",
              "0               NaN  \n",
              "1               NaN  \n",
              "2               NaN  \n",
              "3               NaN  \n",
              "4               NaN  \n",
              "\n",
              "[5 rows x 35 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3317, 15)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['participant', 'content', 'token', 'lemma', 'tagged_token',\n",
              "       'tagged_lemma', 'tagged_stan_token', 'tagged_stan_lemma', 'file',\n",
              "       'utter1', 'utter2', 'utter_order', 'utter1_embedding',\n",
              "       'utter2_embedding', 'cosine_similarity'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "BERT_columns = ['participant', 'content', 'token', 'lemma', 'tagged_token',\n",
        "       'tagged_lemma', 'tagged_stan_token', 'tagged_stan_lemma', 'file',\n",
        "       'BERT_utter1', 'BERT_utter2', 'BERT_utter_order', 'BERT_utter1_embedding',\n",
        "       'BERT_utter2_embedding', 'BERT_cosine_similarity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_df.columns = BERT_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>BERT_utter1</th>\n",
              "      <th>BERT_utter2</th>\n",
              "      <th>BERT_utter_order</th>\n",
              "      <th>BERT_utter1_embedding</th>\n",
              "      <th>BERT_utter2_embedding</th>\n",
              "      <th>BERT_cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>['tucson', 'police', 'xxx']</td>\n",
              "      <td>['tucson', 'police', 'xxx']</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>Operator female Operator 2 female</td>\n",
              "      <td>[[-0.10026315, -0.19990648, -0.06480523, -0.07...</td>\n",
              "      <td>[[0.0770198, 0.01935324, 0.46077225, -0.279308...</td>\n",
              "      <td>0.624709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>['this', 'is', 'xxx']</td>\n",
              "      <td>['this', 'be', 'xxx']</td>\n",
              "      <td>[('this', 'DT'), ('is', 'VBZ'), ('xxx', 'JJ')]</td>\n",
              "      <td>[('this', 'DT'), ('be', 'VB'), ('xxx', 'JJ')]</td>\n",
              "      <td>[('this', 'DT'), ('is', 'VBZ'), ('xxx', 'NN')]</td>\n",
              "      <td>[('this', 'DT'), ('be', 'VB'), ('xxx', 'NN')]</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>Operator 2 female Caller male</td>\n",
              "      <td>[[0.0770198, 0.01935324, 0.46077225, -0.279308...</td>\n",
              "      <td>[[0.6451, 0.094501376, 0.17763025, -0.04114826...</td>\n",
              "      <td>0.537029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Caller male</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>['i', 'need', 'somebody', 'here']</td>\n",
              "      <td>['i', 'need', 'somebody', 'here']</td>\n",
              "      <td>[('i', 'NNS'), ('need', 'VBP'), ('somebody', '...</td>\n",
              "      <td>[('i', 'NNS'), ('need', 'VBP'), ('somebody', '...</td>\n",
              "      <td>[('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...</td>\n",
              "      <td>[('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>Caller male Operator 2 female</td>\n",
              "      <td>[[0.6451, 0.094501376, 0.17763025, -0.04114826...</td>\n",
              "      <td>[[-0.00607915, -0.12721325, 0.34245804, -0.006...</td>\n",
              "      <td>0.560584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>['sir', 'hold', 'on', 'one', 'second', 'okay',...</td>\n",
              "      <td>['sir', 'hold', 'on', 'one', 'second', 'okay',...</td>\n",
              "      <td>[('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...</td>\n",
              "      <td>[('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...</td>\n",
              "      <td>[('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...</td>\n",
              "      <td>[('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>Operator 2 female Operator female</td>\n",
              "      <td>[[-0.00607915, -0.12721325, 0.34245804, -0.006...</td>\n",
              "      <td>[[0.35690638, -0.1165302, 0.14006563, -0.24390...</td>\n",
              "      <td>0.778168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>['okay', 'hello', 'does', 'it', 'look', 'like'...</td>\n",
              "      <td>['okay', 'hello', 'do', 'it', 'look', 'like', ...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'NN'), ('does', 'VB...</td>\n",
              "      <td>[('okay', 'NN'), ('hello', 'NN'), ('do', 'VBP'...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'UH'), ('does', 'VB...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'UH'), ('do', 'VBP'...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>honestly i couldn't i couldn't see anything i ...</td>\n",
              "      <td>Operator female Caller male</td>\n",
              "      <td>[[0.35690638, -0.1165302, 0.14006563, -0.24390...</td>\n",
              "      <td>[[0.5303155, 0.13550542, 0.1008422, -0.0862038...</td>\n",
              "      <td>0.770860</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         participant                                            content  \\\n",
              "0    Operator female                                  tuscon police xxx   \n",
              "1  Operator 2 female                                        this is xxx   \n",
              "2        Caller male                               i need somebody here   \n",
              "3  Operator 2 female   sir hold on one second okay he saw a patient ...   \n",
              "4    Operator female  okay hello does it look like he's been stabbed...   \n",
              "\n",
              "                                               token  \\\n",
              "0                        ['tucson', 'police', 'xxx']   \n",
              "1                              ['this', 'is', 'xxx']   \n",
              "2                  ['i', 'need', 'somebody', 'here']   \n",
              "3  ['sir', 'hold', 'on', 'one', 'second', 'okay',...   \n",
              "4  ['okay', 'hello', 'does', 'it', 'look', 'like'...   \n",
              "\n",
              "                                               lemma  \\\n",
              "0                        ['tucson', 'police', 'xxx']   \n",
              "1                              ['this', 'be', 'xxx']   \n",
              "2                  ['i', 'need', 'somebody', 'here']   \n",
              "3  ['sir', 'hold', 'on', 'one', 'second', 'okay',...   \n",
              "4  ['okay', 'hello', 'do', 'it', 'look', 'like', ...   \n",
              "\n",
              "                                        tagged_token  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1     [('this', 'DT'), ('is', 'VBZ'), ('xxx', 'JJ')]   \n",
              "2  [('i', 'NNS'), ('need', 'VBP'), ('somebody', '...   \n",
              "3  [('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...   \n",
              "4  [('okay', 'JJ'), ('hello', 'NN'), ('does', 'VB...   \n",
              "\n",
              "                                        tagged_lemma  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1      [('this', 'DT'), ('be', 'VB'), ('xxx', 'JJ')]   \n",
              "2  [('i', 'NNS'), ('need', 'VBP'), ('somebody', '...   \n",
              "3  [('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...   \n",
              "4  [('okay', 'NN'), ('hello', 'NN'), ('do', 'VBP'...   \n",
              "\n",
              "                                   tagged_stan_token  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1     [('this', 'DT'), ('is', 'VBZ'), ('xxx', 'NN')]   \n",
              "2  [('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...   \n",
              "3  [('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...   \n",
              "4  [('okay', 'JJ'), ('hello', 'UH'), ('does', 'VB...   \n",
              "\n",
              "                                   tagged_stan_lemma         file  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...  1-TUC_0.txt   \n",
              "1      [('this', 'DT'), ('be', 'VB'), ('xxx', 'NN')]  1-TUC_0.txt   \n",
              "2  [('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...  1-TUC_0.txt   \n",
              "3  [('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...  1-TUC_0.txt   \n",
              "4  [('okay', 'JJ'), ('hello', 'UH'), ('do', 'VBP'...  1-TUC_0.txt   \n",
              "\n",
              "                                         BERT_utter1  \\\n",
              "0                                  tuscon police xxx   \n",
              "1                                        this is xxx   \n",
              "2                               i need somebody here   \n",
              "3   sir hold on one second okay he saw a patient ...   \n",
              "4  okay hello does it look like he's been stabbed...   \n",
              "\n",
              "                                         BERT_utter2  \\\n",
              "0                                        this is xxx   \n",
              "1                               i need somebody here   \n",
              "2   sir hold on one second okay he saw a patient ...   \n",
              "3  okay hello does it look like he's been stabbed...   \n",
              "4  honestly i couldn't i couldn't see anything i ...   \n",
              "\n",
              "                    BERT_utter_order  \\\n",
              "0  Operator female Operator 2 female   \n",
              "1      Operator 2 female Caller male   \n",
              "2      Caller male Operator 2 female   \n",
              "3  Operator 2 female Operator female   \n",
              "4        Operator female Caller male   \n",
              "\n",
              "                               BERT_utter1_embedding  \\\n",
              "0  [[-0.10026315, -0.19990648, -0.06480523, -0.07...   \n",
              "1  [[0.0770198, 0.01935324, 0.46077225, -0.279308...   \n",
              "2  [[0.6451, 0.094501376, 0.17763025, -0.04114826...   \n",
              "3  [[-0.00607915, -0.12721325, 0.34245804, -0.006...   \n",
              "4  [[0.35690638, -0.1165302, 0.14006563, -0.24390...   \n",
              "\n",
              "                               BERT_utter2_embedding  BERT_cosine_similarity  \n",
              "0  [[0.0770198, 0.01935324, 0.46077225, -0.279308...                0.624709  \n",
              "1  [[0.6451, 0.094501376, 0.17763025, -0.04114826...                0.537029  \n",
              "2  [[-0.00607915, -0.12721325, 0.34245804, -0.006...                0.560584  \n",
              "3  [[0.35690638, -0.1165302, 0.14006563, -0.24390...                0.778168  \n",
              "4  [[0.5303155, 0.13550542, 0.1008422, -0.0862038...                0.770860  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "BERT_concatenated_df = concatenated_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:17<00:00,  8.88s/it]\n"
          ]
        }
      ],
      "source": [
        "def process_input_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df['utter1'] = df['content']\n",
        "    df['utter2'] = df['content'].shift(-1)\n",
        "    df['utter_order'] = df['participant'] + ' ' + df['participant'].shift(-1)\n",
        "    return df\n",
        "\n",
        "default_embedding_engine = \"text-embedding-ada-002\"  \n",
        "def get_embedding_with_cache(\n",
        "    text: str,\n",
        "    engine: str = default_embedding_engine\n",
        ") -> list:\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "    if (text, engine) not in embedding_cache.keys():\n",
        "\n",
        "        embedding_cache[(text, engine)] = openai.embeddings.create(input=[text], model=engine).data[0].embedding\n",
        "    return embedding_cache[(text, engine)]\n",
        "\n",
        "def process_file(file_path, embedding_cache, default_embedding_engine):       \n",
        "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "    df = process_input_data(df)\n",
        "\n",
        "    for column in [\"utter1\", \"utter2\"]:\n",
        "        df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n",
        "\n",
        "    df[\"cosine_similarity\"] = df.apply(\n",
        "        lambda row: cosine_similarity(\n",
        "            np.array(row[\"utter1_embedding\"]).reshape(1, -1),\n",
        "            np.array(row[\"utter2_embedding\"]).reshape(1, -1)\n",
        "        )[0][0] if row[\"utter1_embedding\"] is not None and row[\"utter2_embedding\"] is not None else None,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "##########\n",
        "\n",
        "embedding_cache_path = \"data/gpt_embedding_cache.pkl\"\n",
        "try:\n",
        "    with open(embedding_cache_path, \"rb\") as f:\n",
        "        embedding_cache = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    embedding_cache = {}\n",
        "\n",
        "##########\n",
        "\n",
        "folder_path = \"./data/prepped_stan_small\"\n",
        "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
        "\n",
        "concatenated_df = pd.DataFrame()\n",
        "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = process_file(file_path, embedding_cache, default_embedding_engine)\n",
        "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
        "\n",
        "    with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
        "        pickle.dump(embedding_cache, embedding_cache_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>utter1</th>\n",
              "      <th>utter2</th>\n",
              "      <th>utter_order</th>\n",
              "      <th>utter1_embedding</th>\n",
              "      <th>utter2_embedding</th>\n",
              "      <th>cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PC:</td>\n",
              "      <td>i thought that maybe that would do something</td>\n",
              "      <td>['i', 'thought', 'that', 'maybe', 'that', 'wou...</td>\n",
              "      <td>['i', 'think', 'that', 'maybe', 'that', 'would...</td>\n",
              "      <td>[('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
              "      <td>[('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
              "      <td>[('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...</td>\n",
              "      <td>[('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>i thought that maybe that would do something</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>PC: PA:</td>\n",
              "      <td>[-0.03237445279955864, 0.0003060059098061174, ...</td>\n",
              "      <td>[0.003450230695307255, 0.003762950887903571, -...</td>\n",
              "      <td>0.807910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PA:</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
              "      <td>['i', 'think', 'you', 'have', 'to', 'c', 'mayb...</td>\n",
              "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
              "      <td>[('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...</td>\n",
              "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
              "      <td>[('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>i think you have to c maybe close it and it wi...</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>PA: PC:</td>\n",
              "      <td>[0.003450230695307255, 0.003762950887903571, -...</td>\n",
              "      <td>[-0.0031581157818436623, -0.018044402822852135...</td>\n",
              "      <td>0.781013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PC:</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>['should', 'i', 'start', 'this', 'it', 'is', '...</td>\n",
              "      <td>['should', 'i', 'start', 'this', 'it', 'be', '...</td>\n",
              "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
              "      <td>[('should', 'MD'), ('i', 'VB'), ('start', 'VB'...</td>\n",
              "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
              "      <td>[('should', 'MD'), ('i', 'FW'), ('start', 'VB'...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>should i restart this it's like down there</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>PC: PA:</td>\n",
              "      <td>[-0.0031581157818436623, -0.018044402822852135...</td>\n",
              "      <td>[-0.003446547780185938, -0.007897388190031052,...</td>\n",
              "      <td>0.862412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PA:</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
              "      <td>['yeah', 'i', 'would', 'just', 'start', 'it']</td>\n",
              "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
              "      <td>[('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...</td>\n",
              "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
              "      <td>[('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>yeah i would just restart it</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>PA: PC:</td>\n",
              "      <td>[-0.003446547780185938, -0.007897388190031052,...</td>\n",
              "      <td>[0.006002475507557392, -0.016771331429481506, ...</td>\n",
              "      <td>0.766299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PC:</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>['okay', 'now', 'what']</td>\n",
              "      <td>['okay', 'now', 'what']</td>\n",
              "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
              "      <td>[('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]</td>\n",
              "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
              "      <td>[('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]</td>\n",
              "      <td>ASU-T10_ExpBlock2-DolphinShow.txt</td>\n",
              "      <td>okay now what</td>\n",
              "      <td>you didn't close it</td>\n",
              "      <td>PC: PA:</td>\n",
              "      <td>[0.006002475507557392, -0.016771331429481506, ...</td>\n",
              "      <td>[0.00545839499682188, -0.00776617182418704, -0...</td>\n",
              "      <td>0.750938</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  participant                                            content  \\\n",
              "0         PC:       i thought that maybe that would do something   \n",
              "1         PA:  i think you have to c maybe close it and it wi...   \n",
              "2         PC:         should i restart this it's like down there   \n",
              "3         PA:                       yeah i would just restart it   \n",
              "4         PC:                                    okay now what     \n",
              "\n",
              "                                               token  \\\n",
              "0  ['i', 'thought', 'that', 'maybe', 'that', 'wou...   \n",
              "1  ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
              "2  ['should', 'i', 'start', 'this', 'it', 'is', '...   \n",
              "3      ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
              "4                            ['okay', 'now', 'what']   \n",
              "\n",
              "                                               lemma  \\\n",
              "0  ['i', 'think', 'that', 'maybe', 'that', 'would...   \n",
              "1  ['i', 'think', 'you', 'have', 'to', 'c', 'mayb...   \n",
              "2  ['should', 'i', 'start', 'this', 'it', 'be', '...   \n",
              "3      ['yeah', 'i', 'would', 'just', 'start', 'it']   \n",
              "4                            ['okay', 'now', 'what']   \n",
              "\n",
              "                                        tagged_token  \\\n",
              "0  [('i', 'NN'), ('thought', 'VBD'), ('that', 'IN...   \n",
              "1  [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
              "2  [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
              "3  [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
              "4    [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
              "\n",
              "                                        tagged_lemma  \\\n",
              "0  [('i', 'NN'), ('think', 'VBP'), ('that', 'IN')...   \n",
              "1  [('i', 'NN'), ('think', 'VBP'), ('you', 'PRP')...   \n",
              "2  [('should', 'MD'), ('i', 'VB'), ('start', 'VB'...   \n",
              "3  [('yeah', 'NN'), ('i', 'NN'), ('would', 'MD'),...   \n",
              "4    [('okay', 'RB'), ('now', 'RB'), ('what', 'WP')]   \n",
              "\n",
              "                                   tagged_stan_token  \\\n",
              "0  [('i', 'LS'), ('thought', 'VBD'), ('that', 'IN...   \n",
              "1  [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
              "2  [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
              "3  [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
              "4    [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
              "\n",
              "                                   tagged_stan_lemma  \\\n",
              "0  [('i', 'FW'), ('think', 'VBP'), ('that', 'IN')...   \n",
              "1  [('i', 'LS'), ('think', 'VB'), ('you', 'PRP'),...   \n",
              "2  [('should', 'MD'), ('i', 'FW'), ('start', 'VB'...   \n",
              "3  [('yeah', 'JJ'), ('i', 'FW'), ('would', 'MD'),...   \n",
              "4    [('okay', 'JJ'), ('now', 'RB'), ('what', 'WP')]   \n",
              "\n",
              "                                file  \\\n",
              "0  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "1  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "2  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "3  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "4  ASU-T10_ExpBlock2-DolphinShow.txt   \n",
              "\n",
              "                                              utter1  \\\n",
              "0       i thought that maybe that would do something   \n",
              "1  i think you have to c maybe close it and it wi...   \n",
              "2         should i restart this it's like down there   \n",
              "3                       yeah i would just restart it   \n",
              "4                                    okay now what     \n",
              "\n",
              "                                              utter2 utter_order  \\\n",
              "0  i think you have to c maybe close it and it wi...     PC: PA:   \n",
              "1         should i restart this it's like down there     PA: PC:   \n",
              "2                       yeah i would just restart it     PC: PA:   \n",
              "3                                    okay now what       PA: PC:   \n",
              "4                                you didn't close it     PC: PA:   \n",
              "\n",
              "                                    utter1_embedding  \\\n",
              "0  [-0.03237445279955864, 0.0003060059098061174, ...   \n",
              "1  [0.003450230695307255, 0.003762950887903571, -...   \n",
              "2  [-0.0031581157818436623, -0.018044402822852135...   \n",
              "3  [-0.003446547780185938, -0.007897388190031052,...   \n",
              "4  [0.006002475507557392, -0.016771331429481506, ...   \n",
              "\n",
              "                                    utter2_embedding  cosine_similarity  \n",
              "0  [0.003450230695307255, 0.003762950887903571, -...           0.807910  \n",
              "1  [-0.0031581157818436623, -0.018044402822852135...           0.781013  \n",
              "2  [-0.003446547780185938, -0.007897388190031052,...           0.862412  \n",
              "3  [0.006002475507557392, -0.016771331429481506, ...           0.766299  \n",
              "4  [0.00545839499682188, -0.00776617182418704, -0...           0.750938  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['participant', 'content', 'token', 'lemma', 'tagged_token',\n",
              "       'tagged_lemma', 'tagged_stan_token', 'tagged_stan_lemma', 'file',\n",
              "       'utter1', 'utter2', 'utter_order', 'utter1_embedding',\n",
              "       'utter2_embedding', 'cosine_similarity'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPT_columns = ['participant', 'content', 'token', 'lemma', 'tagged_token',\n",
        "       'tagged_lemma', 'tagged_stan_token', 'tagged_stan_lemma', 'file',\n",
        "       'GPT_utter1', 'GPT_utter2', 'GPT_utter_order', 'GPT_utter1_embedding',\n",
        "       'GPT_utter2_embedding', 'GPT_cosine_similarity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_df.columns = GPT_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>GPT_utter1</th>\n",
              "      <th>GPT_utter2</th>\n",
              "      <th>GPT_utter_order</th>\n",
              "      <th>GPT_utter1_embedding</th>\n",
              "      <th>GPT_utter2_embedding</th>\n",
              "      <th>GPT_cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>['tucson', 'police', 'xxx']</td>\n",
              "      <td>['tucson', 'police', 'xxx']</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>Operator female Operator 2 female</td>\n",
              "      <td>[-0.009749563410878181, 0.0005670702084898949,...</td>\n",
              "      <td>[-0.010334055870771408, -0.008830797858536243,...</td>\n",
              "      <td>0.808507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>['this', 'is', 'xxx']</td>\n",
              "      <td>['this', 'be', 'xxx']</td>\n",
              "      <td>[('this', 'DT'), ('is', 'VBZ'), ('xxx', 'JJ')]</td>\n",
              "      <td>[('this', 'DT'), ('be', 'VB'), ('xxx', 'JJ')]</td>\n",
              "      <td>[('this', 'DT'), ('is', 'VBZ'), ('xxx', 'NN')]</td>\n",
              "      <td>[('this', 'DT'), ('be', 'VB'), ('xxx', 'NN')]</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>Operator 2 female Caller male</td>\n",
              "      <td>[-0.010334055870771408, -0.008830797858536243,...</td>\n",
              "      <td>[-0.03374534100294113, 0.005746633280068636, -...</td>\n",
              "      <td>0.806422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Caller male</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>['i', 'need', 'somebody', 'here']</td>\n",
              "      <td>['i', 'need', 'somebody', 'here']</td>\n",
              "      <td>[('i', 'NNS'), ('need', 'VBP'), ('somebody', '...</td>\n",
              "      <td>[('i', 'NNS'), ('need', 'VBP'), ('somebody', '...</td>\n",
              "      <td>[('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...</td>\n",
              "      <td>[('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>Caller male Operator 2 female</td>\n",
              "      <td>[-0.03374534100294113, 0.005746633280068636, -...</td>\n",
              "      <td>[-0.014573554508388042, 0.0018402658170089126,...</td>\n",
              "      <td>0.757559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>['sir', 'hold', 'on', 'one', 'second', 'okay',...</td>\n",
              "      <td>['sir', 'hold', 'on', 'one', 'second', 'okay',...</td>\n",
              "      <td>[('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...</td>\n",
              "      <td>[('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...</td>\n",
              "      <td>[('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...</td>\n",
              "      <td>[('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>Operator 2 female Operator female</td>\n",
              "      <td>[-0.014573554508388042, 0.0018402658170089126,...</td>\n",
              "      <td>[-0.01353168860077858, 0.0013247638707980514, ...</td>\n",
              "      <td>0.774660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>['okay', 'hello', 'does', 'it', 'look', 'like'...</td>\n",
              "      <td>['okay', 'hello', 'do', 'it', 'look', 'like', ...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'NN'), ('does', 'VB...</td>\n",
              "      <td>[('okay', 'NN'), ('hello', 'NN'), ('do', 'VBP'...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'UH'), ('does', 'VB...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'UH'), ('do', 'VBP'...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>honestly i couldn't i couldn't see anything i ...</td>\n",
              "      <td>Operator female Caller male</td>\n",
              "      <td>[-0.01353168860077858, 0.0013247638707980514, ...</td>\n",
              "      <td>[-0.027948414906859398, 0.004147775005549192, ...</td>\n",
              "      <td>0.820494</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         participant                                            content  \\\n",
              "0    Operator female                                  tuscon police xxx   \n",
              "1  Operator 2 female                                        this is xxx   \n",
              "2        Caller male                               i need somebody here   \n",
              "3  Operator 2 female   sir hold on one second okay he saw a patient ...   \n",
              "4    Operator female  okay hello does it look like he's been stabbed...   \n",
              "\n",
              "                                               token  \\\n",
              "0                        ['tucson', 'police', 'xxx']   \n",
              "1                              ['this', 'is', 'xxx']   \n",
              "2                  ['i', 'need', 'somebody', 'here']   \n",
              "3  ['sir', 'hold', 'on', 'one', 'second', 'okay',...   \n",
              "4  ['okay', 'hello', 'does', 'it', 'look', 'like'...   \n",
              "\n",
              "                                               lemma  \\\n",
              "0                        ['tucson', 'police', 'xxx']   \n",
              "1                              ['this', 'be', 'xxx']   \n",
              "2                  ['i', 'need', 'somebody', 'here']   \n",
              "3  ['sir', 'hold', 'on', 'one', 'second', 'okay',...   \n",
              "4  ['okay', 'hello', 'do', 'it', 'look', 'like', ...   \n",
              "\n",
              "                                        tagged_token  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1     [('this', 'DT'), ('is', 'VBZ'), ('xxx', 'JJ')]   \n",
              "2  [('i', 'NNS'), ('need', 'VBP'), ('somebody', '...   \n",
              "3  [('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...   \n",
              "4  [('okay', 'JJ'), ('hello', 'NN'), ('does', 'VB...   \n",
              "\n",
              "                                        tagged_lemma  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1      [('this', 'DT'), ('be', 'VB'), ('xxx', 'JJ')]   \n",
              "2  [('i', 'NNS'), ('need', 'VBP'), ('somebody', '...   \n",
              "3  [('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...   \n",
              "4  [('okay', 'NN'), ('hello', 'NN'), ('do', 'VBP'...   \n",
              "\n",
              "                                   tagged_stan_token  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1     [('this', 'DT'), ('is', 'VBZ'), ('xxx', 'NN')]   \n",
              "2  [('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...   \n",
              "3  [('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...   \n",
              "4  [('okay', 'JJ'), ('hello', 'UH'), ('does', 'VB...   \n",
              "\n",
              "                                   tagged_stan_lemma         file  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...  1-TUC_0.txt   \n",
              "1      [('this', 'DT'), ('be', 'VB'), ('xxx', 'NN')]  1-TUC_0.txt   \n",
              "2  [('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...  1-TUC_0.txt   \n",
              "3  [('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...  1-TUC_0.txt   \n",
              "4  [('okay', 'JJ'), ('hello', 'UH'), ('do', 'VBP'...  1-TUC_0.txt   \n",
              "\n",
              "                                          GPT_utter1  \\\n",
              "0                                  tuscon police xxx   \n",
              "1                                        this is xxx   \n",
              "2                               i need somebody here   \n",
              "3   sir hold on one second okay he saw a patient ...   \n",
              "4  okay hello does it look like he's been stabbed...   \n",
              "\n",
              "                                          GPT_utter2  \\\n",
              "0                                        this is xxx   \n",
              "1                               i need somebody here   \n",
              "2   sir hold on one second okay he saw a patient ...   \n",
              "3  okay hello does it look like he's been stabbed...   \n",
              "4  honestly i couldn't i couldn't see anything i ...   \n",
              "\n",
              "                     GPT_utter_order  \\\n",
              "0  Operator female Operator 2 female   \n",
              "1      Operator 2 female Caller male   \n",
              "2      Caller male Operator 2 female   \n",
              "3  Operator 2 female Operator female   \n",
              "4        Operator female Caller male   \n",
              "\n",
              "                                GPT_utter1_embedding  \\\n",
              "0  [-0.009749563410878181, 0.0005670702084898949,...   \n",
              "1  [-0.010334055870771408, -0.008830797858536243,...   \n",
              "2  [-0.03374534100294113, 0.005746633280068636, -...   \n",
              "3  [-0.014573554508388042, 0.0018402658170089126,...   \n",
              "4  [-0.01353168860077858, 0.0013247638707980514, ...   \n",
              "\n",
              "                                GPT_utter2_embedding  GPT_cosine_similarity  \n",
              "0  [-0.010334055870771408, -0.008830797858536243,...               0.808507  \n",
              "1  [-0.03374534100294113, 0.005746633280068636, -...               0.806422  \n",
              "2  [-0.014573554508388042, 0.0018402658170089126,...               0.757559  \n",
              "3  [-0.01353168860077858, 0.0013247638707980514, ...               0.774660  \n",
              "4  [-0.027948414906859398, 0.004147775005549192, ...               0.820494  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPT_concatenated_df = concatenated_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin Llama 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following model is Llama 2 version: 7 billion parameters, if you use CPU it would probably warn you (in such case we handeled it by try-except strategy) and if you have GPU it will automatically detect it and run it.\n",
        "\n",
        "The 2-part model would be downloaded by the following code, and you will only download the models ONCE! it will remember that it has been already downloaded if you run it again. So, you only need to download the pretrained weights once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
            "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: \n",
            "                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n",
            "                        the quantized model. If you have set a value for `max_memory` you should increase that. To have\n",
            "                        an idea of the modules that are set on the CPU or RAM you can print model.hf_device_map.\n",
            "                        \n",
            "The model could not be fully loaded on the GPU.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "719a30e6bf284a34b14f0e9b0c542cab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model successfully loaded on CPU.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    model = LLaMAForCausalLM.from_pretrained(\n",
        "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "        load_in_8bit=True,\n",
        "        device_map=\"auto\",\n",
        "        use_auth_token=token\n",
        "    )\n",
        "except ValueError as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "    # Attempt to handle the error without accessing the model if it wasn't loaded\n",
        "    print(\"The model could not be fully loaded on the GPU.\")\n",
        "    \n",
        "    # Now try to load the model on CPU\n",
        "    try:\n",
        "        model = LLaMAForCausalLM.from_pretrained(\n",
        "            \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "            load_in_8bit=False,  # Disable 8-bit loading if it causes issues\n",
        "            device_map={\"\": \"cpu\"},  # Force loading on CPU\n",
        "            use_auth_token=token\n",
        "        )\n",
        "        print(\"Model successfully loaded on CPU.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load the model on CPU as well: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_input_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df['utter1'] = df['content']\n",
        "    df['utter2'] = df['content'].shift(-1)\n",
        "    df['utter_order'] = df['participant'] + ' ' + df['participant'].shift(-1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_embedding_engine = \"LLaMA2\"\n",
        "def get_embedding_with_cache(\n",
        "    text: str,\n",
        "    embedding_cache: dict,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    engine: str = default_embedding_engine\n",
        ") -> list:\n",
        "    if text is None:\n",
        "        return None\n",
        "    if (text, engine) not in embedding_cache.keys():\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, output_hidden_states=True)\n",
        "\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "        embedding = hidden_states.mean(dim=1).cpu().numpy().tolist()[0]\n",
        "\n",
        "        embedding_cache[(text, engine)] = embedding\n",
        "    return embedding_cache[(text, engine)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(file_path, embedding_cache, tokenizer, model, default_embedding_engine):\n",
        "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "    df = process_input_data(df)\n",
        "\n",
        "    for column in [\"utter1\", \"utter2\"]:\n",
        "        df[f\"{column}_embedding\"] = df[column].apply(lambda x: get_embedding_with_cache(x, embedding_cache, tokenizer, model))\n",
        "\n",
        "    df[\"cosine_similarity\"] = df.apply(\n",
        "        lambda row: cosine_similarity(\n",
        "            np.array(row[\"utter1_embedding\"]).reshape(1, -1),\n",
        "            np.array(row[\"utter2_embedding\"]).reshape(1, -1)\n",
        "        )[0][0] if row[\"utter1_embedding\"] is not None and row[\"utter2_embedding\"] is not None else None,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_cache_path = \"./data/llama2_embedding_cache.pkl\"\n",
        "embedding_cache_dir = os.path.dirname(embedding_cache_path)\n",
        "\n",
        "if not os.path.exists(embedding_cache_dir):\n",
        "    os.makedirs(embedding_cache_dir)\n",
        "\n",
        "try:\n",
        "    with open(embedding_cache_path, \"rb\") as f:\n",
        "        embedding_cache = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    embedding_cache = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_path = \"./data/prepped_stan_small\"\n",
        "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 87/87 [00:28<00:00,  3.08it/s]\n"
          ]
        }
      ],
      "source": [
        "concatenated_df = pd.DataFrame()\n",
        "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = process_file(file_path, embedding_cache, tokenizer, model, default_embedding_engine)\n",
        "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
        "\n",
        "    with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
        "        pickle.dump(embedding_cache, embedding_cache_file)\n",
        "\n",
        "# concatenated_df.to_csv(\"concatenated_df_llama2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>utter1</th>\n",
              "      <th>utter2</th>\n",
              "      <th>utter_order</th>\n",
              "      <th>utter1_embedding</th>\n",
              "      <th>utter2_embedding</th>\n",
              "      <th>cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>['tucson', 'police', 'xxx']</td>\n",
              "      <td>['tucson', 'police', 'xxx']</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>Operator female Operator 2 female</td>\n",
              "      <td>[0.5087890625, -1.2724609375, 0.76220703125, 0...</td>\n",
              "      <td>[0.34619140625, -0.07781982421875, 0.751953125...</td>\n",
              "      <td>0.645413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>['this', 'is', 'xxx']</td>\n",
              "      <td>['this', 'be', 'xxx']</td>\n",
              "      <td>[('this', 'DT'), ('is', 'VBZ'), ('xxx', 'JJ')]</td>\n",
              "      <td>[('this', 'DT'), ('be', 'VB'), ('xxx', 'JJ')]</td>\n",
              "      <td>[('this', 'DT'), ('is', 'VBZ'), ('xxx', 'NN')]</td>\n",
              "      <td>[('this', 'DT'), ('be', 'VB'), ('xxx', 'NN')]</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>Operator 2 female Caller male</td>\n",
              "      <td>[0.34619140625, -0.07781982421875, 0.751953125...</td>\n",
              "      <td>[1.3330078125, -0.22119140625, -0.256591796875...</td>\n",
              "      <td>0.445610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Caller male</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>['i', 'need', 'somebody', 'here']</td>\n",
              "      <td>['i', 'need', 'somebody', 'here']</td>\n",
              "      <td>[('i', 'NNS'), ('need', 'VBP'), ('somebody', '...</td>\n",
              "      <td>[('i', 'NNS'), ('need', 'VBP'), ('somebody', '...</td>\n",
              "      <td>[('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...</td>\n",
              "      <td>[('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>Caller male Operator 2 female</td>\n",
              "      <td>[1.3330078125, -0.22119140625, -0.256591796875...</td>\n",
              "      <td>[1.45703125, -2.55078125, 0.83984375, -0.18359...</td>\n",
              "      <td>0.584490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>['sir', 'hold', 'on', 'one', 'second', 'okay',...</td>\n",
              "      <td>['sir', 'hold', 'on', 'one', 'second', 'okay',...</td>\n",
              "      <td>[('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...</td>\n",
              "      <td>[('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...</td>\n",
              "      <td>[('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...</td>\n",
              "      <td>[('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>Operator 2 female Operator female</td>\n",
              "      <td>[1.45703125, -2.55078125, 0.83984375, -0.18359...</td>\n",
              "      <td>[1.8779296875, -2.7109375, 1.240234375, 0.0852...</td>\n",
              "      <td>0.748699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>['okay', 'hello', 'does', 'it', 'look', 'like'...</td>\n",
              "      <td>['okay', 'hello', 'do', 'it', 'look', 'like', ...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'NN'), ('does', 'VB...</td>\n",
              "      <td>[('okay', 'NN'), ('hello', 'NN'), ('do', 'VBP'...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'UH'), ('does', 'VB...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'UH'), ('do', 'VBP'...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>honestly i couldn't i couldn't see anything i ...</td>\n",
              "      <td>Operator female Caller male</td>\n",
              "      <td>[1.8779296875, -2.7109375, 1.240234375, 0.0852...</td>\n",
              "      <td>[1.5244140625, -2.16015625, 1.0234375, 0.51464...</td>\n",
              "      <td>0.818535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3312</th>\n",
              "      <td>operator female</td>\n",
              "      <td>how many people assaulted him do you know</td>\n",
              "      <td>['how', 'many', 'people', 'assaulted', 'him', ...</td>\n",
              "      <td>['how', 'many', 'people', 'assault', 'him', 'd...</td>\n",
              "      <td>[('how', 'WRB'), ('many', 'JJ'), ('people', 'N...</td>\n",
              "      <td>[('how', 'WRB'), ('many', 'JJ'), ('people', 'N...</td>\n",
              "      <td>[('how', 'WRB'), ('many', 'JJ'), ('people', 'N...</td>\n",
              "      <td>[('how', 'WRB'), ('many', 'JJ'), ('people', 'N...</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>how many people assaulted him do you know</td>\n",
              "      <td>do not know i was at the front door for this n...</td>\n",
              "      <td>operator female caller male</td>\n",
              "      <td>[2.9140625, -2.720703125, 1.287109375, -0.5629...</td>\n",
              "      <td>[2.150390625, -1.9609375, 0.298583984375, -0.2...</td>\n",
              "      <td>0.587910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3313</th>\n",
              "      <td>caller male</td>\n",
              "      <td>do not know i was at the front door for this n...</td>\n",
              "      <td>['do', 'not', 'know', 'i', 'was', 'at', 'the',...</td>\n",
              "      <td>['do', 'not', 'know', 'i', 'be', 'at', 'the', ...</td>\n",
              "      <td>[('do', 'VB'), ('not', 'RB'), ('know', 'VB'), ...</td>\n",
              "      <td>[('do', 'VB'), ('not', 'RB'), ('know', 'VB'), ...</td>\n",
              "      <td>[('do', 'VB'), ('not', 'RB'), ('know', 'VB'), ...</td>\n",
              "      <td>[('do', 'VB'), ('not', 'RB'), ('know', 'VB'), ...</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>do not know i was at the front door for this n...</td>\n",
              "      <td>okay sir we'll get someone out as soon as we can</td>\n",
              "      <td>caller male operator female</td>\n",
              "      <td>[2.150390625, -1.9609375, 0.298583984375, -0.2...</td>\n",
              "      <td>[2.05078125, -2.91015625, 0.57666015625, 0.415...</td>\n",
              "      <td>0.563802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3314</th>\n",
              "      <td>operator female</td>\n",
              "      <td>okay sir we'll get someone out as soon as we can</td>\n",
              "      <td>['okay', 'sir', 'we', 'will', 'get', 'someone'...</td>\n",
              "      <td>['okay', 'sir', 'we', 'will', 'get', 'someone'...</td>\n",
              "      <td>[('okay', 'JJ'), ('sir', 'NN'), ('we', 'PRP'),...</td>\n",
              "      <td>[('okay', 'JJ'), ('sir', 'NN'), ('we', 'PRP'),...</td>\n",
              "      <td>[('okay', 'JJ'), ('sir', 'NNP'), ('we', 'PRP')...</td>\n",
              "      <td>[('okay', 'JJ'), ('sir', 'NNP'), ('we', 'PRP')...</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>okay sir we'll get someone out as soon as we can</td>\n",
              "      <td>thank you</td>\n",
              "      <td>operator female caller male</td>\n",
              "      <td>[2.05078125, -2.91015625, 0.57666015625, 0.415...</td>\n",
              "      <td>[1.6240234375, -1.8408203125, 0.6923828125, -0...</td>\n",
              "      <td>0.372024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3315</th>\n",
              "      <td>caller male</td>\n",
              "      <td>thank you</td>\n",
              "      <td>['thank', 'you']</td>\n",
              "      <td>['thank', 'you']</td>\n",
              "      <td>[('thank', 'NN'), ('you', 'PRP')]</td>\n",
              "      <td>[('thank', 'NN'), ('you', 'PRP')]</td>\n",
              "      <td>[('thank', 'VB'), ('you', 'PRP')]</td>\n",
              "      <td>[('thank', 'VB'), ('you', 'PRP')]</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>thank you</td>\n",
              "      <td>you're welcome</td>\n",
              "      <td>caller male operator female</td>\n",
              "      <td>[1.6240234375, -1.8408203125, 0.6923828125, -0...</td>\n",
              "      <td>[1.462890625, -2.310546875, 0.017913818359375,...</td>\n",
              "      <td>0.651379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3316</th>\n",
              "      <td>operator female</td>\n",
              "      <td>you're welcome</td>\n",
              "      <td>['you', 'are', 'welcome']</td>\n",
              "      <td>['you', 'be', 'welcome']</td>\n",
              "      <td>[('you', 'PRP'), ('are', 'VBP'), ('welcome', '...</td>\n",
              "      <td>[('you', 'PRP'), ('be', 'VB'), ('welcome', 'JJ')]</td>\n",
              "      <td>[('you', 'PRP'), ('are', 'VBP'), ('welcome', '...</td>\n",
              "      <td>[('you', 'PRP'), ('be', 'VB'), ('welcome', 'JJ')]</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>you're welcome</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[1.462890625, -2.310546875, 0.017913818359375,...</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3317 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            participant                                            content  \\\n",
              "0       Operator female                                  tuscon police xxx   \n",
              "1     Operator 2 female                                        this is xxx   \n",
              "2           Caller male                               i need somebody here   \n",
              "3     Operator 2 female   sir hold on one second okay he saw a patient ...   \n",
              "4       Operator female  okay hello does it look like he's been stabbed...   \n",
              "...                 ...                                                ...   \n",
              "3312    operator female          how many people assaulted him do you know   \n",
              "3313        caller male  do not know i was at the front door for this n...   \n",
              "3314    operator female   okay sir we'll get someone out as soon as we can   \n",
              "3315        caller male                                          thank you   \n",
              "3316    operator female                                     you're welcome   \n",
              "\n",
              "                                                  token  \\\n",
              "0                           ['tucson', 'police', 'xxx']   \n",
              "1                                 ['this', 'is', 'xxx']   \n",
              "2                     ['i', 'need', 'somebody', 'here']   \n",
              "3     ['sir', 'hold', 'on', 'one', 'second', 'okay',...   \n",
              "4     ['okay', 'hello', 'does', 'it', 'look', 'like'...   \n",
              "...                                                 ...   \n",
              "3312  ['how', 'many', 'people', 'assaulted', 'him', ...   \n",
              "3313  ['do', 'not', 'know', 'i', 'was', 'at', 'the',...   \n",
              "3314  ['okay', 'sir', 'we', 'will', 'get', 'someone'...   \n",
              "3315                                   ['thank', 'you']   \n",
              "3316                          ['you', 'are', 'welcome']   \n",
              "\n",
              "                                                  lemma  \\\n",
              "0                           ['tucson', 'police', 'xxx']   \n",
              "1                                 ['this', 'be', 'xxx']   \n",
              "2                     ['i', 'need', 'somebody', 'here']   \n",
              "3     ['sir', 'hold', 'on', 'one', 'second', 'okay',...   \n",
              "4     ['okay', 'hello', 'do', 'it', 'look', 'like', ...   \n",
              "...                                                 ...   \n",
              "3312  ['how', 'many', 'people', 'assault', 'him', 'd...   \n",
              "3313  ['do', 'not', 'know', 'i', 'be', 'at', 'the', ...   \n",
              "3314  ['okay', 'sir', 'we', 'will', 'get', 'someone'...   \n",
              "3315                                   ['thank', 'you']   \n",
              "3316                           ['you', 'be', 'welcome']   \n",
              "\n",
              "                                           tagged_token  \\\n",
              "0     [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1        [('this', 'DT'), ('is', 'VBZ'), ('xxx', 'JJ')]   \n",
              "2     [('i', 'NNS'), ('need', 'VBP'), ('somebody', '...   \n",
              "3     [('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...   \n",
              "4     [('okay', 'JJ'), ('hello', 'NN'), ('does', 'VB...   \n",
              "...                                                 ...   \n",
              "3312  [('how', 'WRB'), ('many', 'JJ'), ('people', 'N...   \n",
              "3313  [('do', 'VB'), ('not', 'RB'), ('know', 'VB'), ...   \n",
              "3314  [('okay', 'JJ'), ('sir', 'NN'), ('we', 'PRP'),...   \n",
              "3315                  [('thank', 'NN'), ('you', 'PRP')]   \n",
              "3316  [('you', 'PRP'), ('are', 'VBP'), ('welcome', '...   \n",
              "\n",
              "                                           tagged_lemma  \\\n",
              "0     [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1         [('this', 'DT'), ('be', 'VB'), ('xxx', 'JJ')]   \n",
              "2     [('i', 'NNS'), ('need', 'VBP'), ('somebody', '...   \n",
              "3     [('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...   \n",
              "4     [('okay', 'NN'), ('hello', 'NN'), ('do', 'VBP'...   \n",
              "...                                                 ...   \n",
              "3312  [('how', 'WRB'), ('many', 'JJ'), ('people', 'N...   \n",
              "3313  [('do', 'VB'), ('not', 'RB'), ('know', 'VB'), ...   \n",
              "3314  [('okay', 'JJ'), ('sir', 'NN'), ('we', 'PRP'),...   \n",
              "3315                  [('thank', 'NN'), ('you', 'PRP')]   \n",
              "3316  [('you', 'PRP'), ('be', 'VB'), ('welcome', 'JJ')]   \n",
              "\n",
              "                                      tagged_stan_token  \\\n",
              "0     [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1        [('this', 'DT'), ('is', 'VBZ'), ('xxx', 'NN')]   \n",
              "2     [('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...   \n",
              "3     [('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...   \n",
              "4     [('okay', 'JJ'), ('hello', 'UH'), ('does', 'VB...   \n",
              "...                                                 ...   \n",
              "3312  [('how', 'WRB'), ('many', 'JJ'), ('people', 'N...   \n",
              "3313  [('do', 'VB'), ('not', 'RB'), ('know', 'VB'), ...   \n",
              "3314  [('okay', 'JJ'), ('sir', 'NNP'), ('we', 'PRP')...   \n",
              "3315                  [('thank', 'VB'), ('you', 'PRP')]   \n",
              "3316  [('you', 'PRP'), ('are', 'VBP'), ('welcome', '...   \n",
              "\n",
              "                                      tagged_stan_lemma         file  \\\n",
              "0     [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...  1-TUC_0.txt   \n",
              "1         [('this', 'DT'), ('be', 'VB'), ('xxx', 'NN')]  1-TUC_0.txt   \n",
              "2     [('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...  1-TUC_0.txt   \n",
              "3     [('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...  1-TUC_0.txt   \n",
              "4     [('okay', 'JJ'), ('hello', 'UH'), ('do', 'VBP'...  1-TUC_0.txt   \n",
              "...                                                 ...          ...   \n",
              "3312  [('how', 'WRB'), ('many', 'JJ'), ('people', 'N...  9-TUC_0.txt   \n",
              "3313  [('do', 'VB'), ('not', 'RB'), ('know', 'VB'), ...  9-TUC_0.txt   \n",
              "3314  [('okay', 'JJ'), ('sir', 'NNP'), ('we', 'PRP')...  9-TUC_0.txt   \n",
              "3315                  [('thank', 'VB'), ('you', 'PRP')]  9-TUC_0.txt   \n",
              "3316  [('you', 'PRP'), ('be', 'VB'), ('welcome', 'JJ')]  9-TUC_0.txt   \n",
              "\n",
              "                                                 utter1  \\\n",
              "0                                     tuscon police xxx   \n",
              "1                                           this is xxx   \n",
              "2                                  i need somebody here   \n",
              "3      sir hold on one second okay he saw a patient ...   \n",
              "4     okay hello does it look like he's been stabbed...   \n",
              "...                                                 ...   \n",
              "3312          how many people assaulted him do you know   \n",
              "3313  do not know i was at the front door for this n...   \n",
              "3314   okay sir we'll get someone out as soon as we can   \n",
              "3315                                          thank you   \n",
              "3316                                     you're welcome   \n",
              "\n",
              "                                                 utter2  \\\n",
              "0                                           this is xxx   \n",
              "1                                  i need somebody here   \n",
              "2      sir hold on one second okay he saw a patient ...   \n",
              "3     okay hello does it look like he's been stabbed...   \n",
              "4     honestly i couldn't i couldn't see anything i ...   \n",
              "...                                                 ...   \n",
              "3312  do not know i was at the front door for this n...   \n",
              "3313   okay sir we'll get someone out as soon as we can   \n",
              "3314                                          thank you   \n",
              "3315                                     you're welcome   \n",
              "3316                                               None   \n",
              "\n",
              "                            utter_order  \\\n",
              "0     Operator female Operator 2 female   \n",
              "1         Operator 2 female Caller male   \n",
              "2         Caller male Operator 2 female   \n",
              "3     Operator 2 female Operator female   \n",
              "4           Operator female Caller male   \n",
              "...                                 ...   \n",
              "3312        operator female caller male   \n",
              "3313        caller male operator female   \n",
              "3314        operator female caller male   \n",
              "3315        caller male operator female   \n",
              "3316                                NaN   \n",
              "\n",
              "                                       utter1_embedding  \\\n",
              "0     [0.5087890625, -1.2724609375, 0.76220703125, 0...   \n",
              "1     [0.34619140625, -0.07781982421875, 0.751953125...   \n",
              "2     [1.3330078125, -0.22119140625, -0.256591796875...   \n",
              "3     [1.45703125, -2.55078125, 0.83984375, -0.18359...   \n",
              "4     [1.8779296875, -2.7109375, 1.240234375, 0.0852...   \n",
              "...                                                 ...   \n",
              "3312  [2.9140625, -2.720703125, 1.287109375, -0.5629...   \n",
              "3313  [2.150390625, -1.9609375, 0.298583984375, -0.2...   \n",
              "3314  [2.05078125, -2.91015625, 0.57666015625, 0.415...   \n",
              "3315  [1.6240234375, -1.8408203125, 0.6923828125, -0...   \n",
              "3316  [1.462890625, -2.310546875, 0.017913818359375,...   \n",
              "\n",
              "                                       utter2_embedding  cosine_similarity  \n",
              "0     [0.34619140625, -0.07781982421875, 0.751953125...           0.645413  \n",
              "1     [1.3330078125, -0.22119140625, -0.256591796875...           0.445610  \n",
              "2     [1.45703125, -2.55078125, 0.83984375, -0.18359...           0.584490  \n",
              "3     [1.8779296875, -2.7109375, 1.240234375, 0.0852...           0.748699  \n",
              "4     [1.5244140625, -2.16015625, 1.0234375, 0.51464...           0.818535  \n",
              "...                                                 ...                ...  \n",
              "3312  [2.150390625, -1.9609375, 0.298583984375, -0.2...           0.587910  \n",
              "3313  [2.05078125, -2.91015625, 0.57666015625, 0.415...           0.563802  \n",
              "3314  [1.6240234375, -1.8408203125, 0.6923828125, -0...           0.372024  \n",
              "3315  [1.462890625, -2.310546875, 0.017913818359375,...           0.651379  \n",
              "3316                                               None                NaN  \n",
              "\n",
              "[3317 rows x 15 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "830"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df[concatenated_df['cosine_similarity'] > 0.75].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['participant', 'content', 'token', 'lemma', 'tagged_token',\n",
              "       'tagged_lemma', 'tagged_stan_token', 'tagged_stan_lemma', 'file',\n",
              "       'utter1', 'utter2', 'utter_order', 'utter1_embedding',\n",
              "       'utter2_embedding', 'cosine_similarity'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "Llama_columns = ['participant', 'content', 'token', 'lemma', 'tagged_token',\n",
        "       'tagged_lemma', 'tagged_stan_token', 'tagged_stan_lemma', 'file',\n",
        "       'Llama_utter1', 'Llama_utter2', 'Llama_utter_order', 'Llama_utter1_embedding',\n",
        "       'Llama_utter2_embedding', 'Llama_cosine_similarity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_df.columns = Llama_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>Llama_utter1</th>\n",
              "      <th>Llama_utter2</th>\n",
              "      <th>Llama_utter_order</th>\n",
              "      <th>Llama_utter1_embedding</th>\n",
              "      <th>Llama_utter2_embedding</th>\n",
              "      <th>Llama_cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>['tucson', 'police', 'xxx']</td>\n",
              "      <td>['tucson', 'police', 'xxx']</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>[('tucson', 'NN'), ('police', 'NN'), ('xxx', '...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>Operator female Operator 2 female</td>\n",
              "      <td>[0.5087890625, -1.2724609375, 0.76220703125, 0...</td>\n",
              "      <td>[0.34619140625, -0.07781982421875, 0.751953125...</td>\n",
              "      <td>0.645413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>['this', 'is', 'xxx']</td>\n",
              "      <td>['this', 'be', 'xxx']</td>\n",
              "      <td>[('this', 'DT'), ('is', 'VBZ'), ('xxx', 'JJ')]</td>\n",
              "      <td>[('this', 'DT'), ('be', 'VB'), ('xxx', 'JJ')]</td>\n",
              "      <td>[('this', 'DT'), ('is', 'VBZ'), ('xxx', 'NN')]</td>\n",
              "      <td>[('this', 'DT'), ('be', 'VB'), ('xxx', 'NN')]</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>Operator 2 female Caller male</td>\n",
              "      <td>[0.34619140625, -0.07781982421875, 0.751953125...</td>\n",
              "      <td>[1.3330078125, -0.22119140625, -0.256591796875...</td>\n",
              "      <td>0.445610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Caller male</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>['i', 'need', 'somebody', 'here']</td>\n",
              "      <td>['i', 'need', 'somebody', 'here']</td>\n",
              "      <td>[('i', 'NNS'), ('need', 'VBP'), ('somebody', '...</td>\n",
              "      <td>[('i', 'NNS'), ('need', 'VBP'), ('somebody', '...</td>\n",
              "      <td>[('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...</td>\n",
              "      <td>[('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>Caller male Operator 2 female</td>\n",
              "      <td>[1.3330078125, -0.22119140625, -0.256591796875...</td>\n",
              "      <td>[1.45703125, -2.55078125, 0.83984375, -0.18359...</td>\n",
              "      <td>0.584490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>['sir', 'hold', 'on', 'one', 'second', 'okay',...</td>\n",
              "      <td>['sir', 'hold', 'on', 'one', 'second', 'okay',...</td>\n",
              "      <td>[('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...</td>\n",
              "      <td>[('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...</td>\n",
              "      <td>[('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...</td>\n",
              "      <td>[('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>Operator 2 female Operator female</td>\n",
              "      <td>[1.45703125, -2.55078125, 0.83984375, -0.18359...</td>\n",
              "      <td>[1.8779296875, -2.7109375, 1.240234375, 0.0852...</td>\n",
              "      <td>0.748699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>['okay', 'hello', 'does', 'it', 'look', 'like'...</td>\n",
              "      <td>['okay', 'hello', 'do', 'it', 'look', 'like', ...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'NN'), ('does', 'VB...</td>\n",
              "      <td>[('okay', 'NN'), ('hello', 'NN'), ('do', 'VBP'...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'UH'), ('does', 'VB...</td>\n",
              "      <td>[('okay', 'JJ'), ('hello', 'UH'), ('do', 'VBP'...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>honestly i couldn't i couldn't see anything i ...</td>\n",
              "      <td>Operator female Caller male</td>\n",
              "      <td>[1.8779296875, -2.7109375, 1.240234375, 0.0852...</td>\n",
              "      <td>[1.5244140625, -2.16015625, 1.0234375, 0.51464...</td>\n",
              "      <td>0.818535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         participant                                            content  \\\n",
              "0    Operator female                                  tuscon police xxx   \n",
              "1  Operator 2 female                                        this is xxx   \n",
              "2        Caller male                               i need somebody here   \n",
              "3  Operator 2 female   sir hold on one second okay he saw a patient ...   \n",
              "4    Operator female  okay hello does it look like he's been stabbed...   \n",
              "\n",
              "                                               token  \\\n",
              "0                        ['tucson', 'police', 'xxx']   \n",
              "1                              ['this', 'is', 'xxx']   \n",
              "2                  ['i', 'need', 'somebody', 'here']   \n",
              "3  ['sir', 'hold', 'on', 'one', 'second', 'okay',...   \n",
              "4  ['okay', 'hello', 'does', 'it', 'look', 'like'...   \n",
              "\n",
              "                                               lemma  \\\n",
              "0                        ['tucson', 'police', 'xxx']   \n",
              "1                              ['this', 'be', 'xxx']   \n",
              "2                  ['i', 'need', 'somebody', 'here']   \n",
              "3  ['sir', 'hold', 'on', 'one', 'second', 'okay',...   \n",
              "4  ['okay', 'hello', 'do', 'it', 'look', 'like', ...   \n",
              "\n",
              "                                        tagged_token  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1     [('this', 'DT'), ('is', 'VBZ'), ('xxx', 'JJ')]   \n",
              "2  [('i', 'NNS'), ('need', 'VBP'), ('somebody', '...   \n",
              "3  [('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...   \n",
              "4  [('okay', 'JJ'), ('hello', 'NN'), ('does', 'VB...   \n",
              "\n",
              "                                        tagged_lemma  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1      [('this', 'DT'), ('be', 'VB'), ('xxx', 'JJ')]   \n",
              "2  [('i', 'NNS'), ('need', 'VBP'), ('somebody', '...   \n",
              "3  [('sir', 'NN'), ('hold', 'NN'), ('on', 'IN'), ...   \n",
              "4  [('okay', 'NN'), ('hello', 'NN'), ('do', 'VBP'...   \n",
              "\n",
              "                                   tagged_stan_token  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...   \n",
              "1     [('this', 'DT'), ('is', 'VBZ'), ('xxx', 'NN')]   \n",
              "2  [('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...   \n",
              "3  [('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...   \n",
              "4  [('okay', 'JJ'), ('hello', 'UH'), ('does', 'VB...   \n",
              "\n",
              "                                   tagged_stan_lemma         file  \\\n",
              "0  [('tucson', 'NN'), ('police', 'NN'), ('xxx', '...  1-TUC_0.txt   \n",
              "1      [('this', 'DT'), ('be', 'VB'), ('xxx', 'NN')]  1-TUC_0.txt   \n",
              "2  [('i', 'LS'), ('need', 'MD'), ('somebody', 'NN...  1-TUC_0.txt   \n",
              "3  [('sir', 'NNP'), ('hold', 'VBP'), ('on', 'IN')...  1-TUC_0.txt   \n",
              "4  [('okay', 'JJ'), ('hello', 'UH'), ('do', 'VBP'...  1-TUC_0.txt   \n",
              "\n",
              "                                        Llama_utter1  \\\n",
              "0                                  tuscon police xxx   \n",
              "1                                        this is xxx   \n",
              "2                               i need somebody here   \n",
              "3   sir hold on one second okay he saw a patient ...   \n",
              "4  okay hello does it look like he's been stabbed...   \n",
              "\n",
              "                                        Llama_utter2  \\\n",
              "0                                        this is xxx   \n",
              "1                               i need somebody here   \n",
              "2   sir hold on one second okay he saw a patient ...   \n",
              "3  okay hello does it look like he's been stabbed...   \n",
              "4  honestly i couldn't i couldn't see anything i ...   \n",
              "\n",
              "                   Llama_utter_order  \\\n",
              "0  Operator female Operator 2 female   \n",
              "1      Operator 2 female Caller male   \n",
              "2      Caller male Operator 2 female   \n",
              "3  Operator 2 female Operator female   \n",
              "4        Operator female Caller male   \n",
              "\n",
              "                              Llama_utter1_embedding  \\\n",
              "0  [0.5087890625, -1.2724609375, 0.76220703125, 0...   \n",
              "1  [0.34619140625, -0.07781982421875, 0.751953125...   \n",
              "2  [1.3330078125, -0.22119140625, -0.256591796875...   \n",
              "3  [1.45703125, -2.55078125, 0.83984375, -0.18359...   \n",
              "4  [1.8779296875, -2.7109375, 1.240234375, 0.0852...   \n",
              "\n",
              "                              Llama_utter2_embedding  Llama_cosine_similarity  \n",
              "0  [0.34619140625, -0.07781982421875, 0.751953125...                 0.645413  \n",
              "1  [1.3330078125, -0.22119140625, -0.256591796875...                 0.445610  \n",
              "2  [1.45703125, -2.55078125, 0.83984375, -0.18359...                 0.584490  \n",
              "3  [1.8779296875, -2.7109375, 1.240234375, 0.0852...                 0.748699  \n",
              "4  [1.5244140625, -2.16015625, 1.0234375, 0.51464...                 0.818535  "
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "Llama_concatenated_df = concatenated_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End Llama 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merging Output\n",
        "the output would be a single spreadsheet with the cosine/ALIGN scores along four columns, corresponding to each method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df = pd.concat([W2V_concatenated_df, BERT_concatenated_df, GPT_concatenated_df, Llama_concatenated_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df = W2V_BERT_GPT_Llama_concatenated_df.loc[:, ~W2V_BERT_GPT_Llama_concatenated_df.columns.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['participant', 'content', 'token', 'lemma', 'tagged_token',\n",
              "       'tagged_lemma', 'tagged_stan_token', 'tagged_stan_lemma', 'file',\n",
              "       'W2V_content1', 'W2V_content2', 'W2V_token1', 'W2V_token2',\n",
              "       'W2V_lemma1', 'W2V_lemma2', 'W2V_tagged_token1', 'W2V_tagged_token2',\n",
              "       'W2V_tagged_lemma1', 'W2V_tagged_lemma2', 'W2V_tagged_stan_token1',\n",
              "       'W2V_tagged_stan_token2', 'W2V_tagged_stan_lemma1',\n",
              "       'W2V_tagged_stan_lemma2', 'W2V_utter_order', 'W2V_lemma1_sum_embedding',\n",
              "       'W2V_lemma2_sum_embedding', 'W2V_token1_sum_embedding',\n",
              "       'W2V_token2_sum_embedding', 'W2V_lemma_cosine_similarity',\n",
              "       'W2V_token_cosine_similarity', 'BERT_utter1', 'BERT_utter2',\n",
              "       'BERT_utter_order', 'BERT_utter1_embedding', 'BERT_utter2_embedding',\n",
              "       'BERT_cosine_similarity', 'GPT_utter1', 'GPT_utter2', 'GPT_utter_order',\n",
              "       'GPT_utter1_embedding', 'GPT_utter2_embedding', 'GPT_cosine_similarity',\n",
              "       'Llama_utter1', 'Llama_utter2', 'Llama_utter_order',\n",
              "       'Llama_utter1_embedding', 'Llama_utter2_embedding',\n",
              "       'Llama_cosine_similarity'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df = W2V_BERT_GPT_Llama_concatenated_df[['participant', 'content', 'token', 'lemma', 'tagged_token',\n",
        "       'tagged_lemma', 'tagged_stan_token', 'tagged_stan_lemma', 'file', 'W2V_lemma_cosine_similarity',\n",
        "       'W2V_token_cosine_similarity', 'BERT_cosine_similarity', 'GPT_cosine_similarity', 'Llama_cosine_similarity']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant</th>\n",
              "      <th>content</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tagged_token</th>\n",
              "      <th>tagged_lemma</th>\n",
              "      <th>tagged_stan_token</th>\n",
              "      <th>tagged_stan_lemma</th>\n",
              "      <th>file</th>\n",
              "      <th>W2V_lemma_cosine_similarity</th>\n",
              "      <th>W2V_token_cosine_similarity</th>\n",
              "      <th>BERT_cosine_similarity</th>\n",
              "      <th>GPT_cosine_similarity</th>\n",
              "      <th>Llama_cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>tuscon police xxx</td>\n",
              "      <td>[tucson, police, xxx]</td>\n",
              "      <td>[tucson, police, xxx]</td>\n",
              "      <td>[(tucson, NN), (police, NN), (xxx, NN)]</td>\n",
              "      <td>[(tucson, NN), (police, NN), (xxx, NN)]</td>\n",
              "      <td>[(tucson, NN), (police, NN), (xxx, NN)]</td>\n",
              "      <td>[(tucson, NN), (police, NN), (xxx, NN)]</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>0.718296</td>\n",
              "      <td>0.718296</td>\n",
              "      <td>0.624709</td>\n",
              "      <td>0.808507</td>\n",
              "      <td>0.645413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>this is xxx</td>\n",
              "      <td>[this, xxx]</td>\n",
              "      <td>[this, xxx]</td>\n",
              "      <td>[(this, DT), (is, VBZ), (xxx, JJ)]</td>\n",
              "      <td>[(this, DT), (be, VB), (xxx, JJ)]</td>\n",
              "      <td>[(this, DT), (is, VBZ), (xxx, NN)]</td>\n",
              "      <td>[(this, DT), (be, VB), (xxx, NN)]</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>0.239677</td>\n",
              "      <td>0.239677</td>\n",
              "      <td>0.537029</td>\n",
              "      <td>0.806422</td>\n",
              "      <td>0.445610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Caller male</td>\n",
              "      <td>i need somebody here</td>\n",
              "      <td>[need, somebody, here]</td>\n",
              "      <td>[need, somebody, here]</td>\n",
              "      <td>[(i, NNS), (need, VBP), (somebody, NN), (here,...</td>\n",
              "      <td>[(i, NNS), (need, VBP), (somebody, NN), (here,...</td>\n",
              "      <td>[(i, LS), (need, MD), (somebody, NN), (here, RB)]</td>\n",
              "      <td>[(i, LS), (need, MD), (somebody, NN), (here, RB)]</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>0.693040</td>\n",
              "      <td>0.669003</td>\n",
              "      <td>0.560584</td>\n",
              "      <td>0.757559</td>\n",
              "      <td>0.584490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Operator 2 female</td>\n",
              "      <td>sir hold on one second okay he saw a patient ...</td>\n",
              "      <td>[sir, hold, one, second, saw, patient, in, roa...</td>\n",
              "      <td>[sir, hold, one, second, saw, patient, in, roa...</td>\n",
              "      <td>[(sir, NN), (hold, NN), (on, IN), (one, CD), (...</td>\n",
              "      <td>[(sir, NN), (hold, NN), (on, IN), (one, CD), (...</td>\n",
              "      <td>[(sir, NNP), (hold, VBP), (on, IN), (one, CD),...</td>\n",
              "      <td>[(sir, NNP), (hold, VBP), (on, IN), (one, CD),...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>0.675061</td>\n",
              "      <td>0.564028</td>\n",
              "      <td>0.778168</td>\n",
              "      <td>0.774660</td>\n",
              "      <td>0.748699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Operator female</td>\n",
              "      <td>okay hello does it look like he's been stabbed...</td>\n",
              "      <td>[hello, look, like, or, shot, or]</td>\n",
              "      <td>[hello, look, like, or, shot, or, go]</td>\n",
              "      <td>[(okay, JJ), (hello, NN), (does, VBZ), (it, PR...</td>\n",
              "      <td>[(okay, NN), (hello, NN), (do, VBP), (it, PRP)...</td>\n",
              "      <td>[(okay, JJ), (hello, UH), (does, VBZ), (it, PR...</td>\n",
              "      <td>[(okay, JJ), (hello, UH), (do, VBP), (it, PRP)...</td>\n",
              "      <td>1-TUC_0.txt</td>\n",
              "      <td>0.703610</td>\n",
              "      <td>0.670530</td>\n",
              "      <td>0.770860</td>\n",
              "      <td>0.820494</td>\n",
              "      <td>0.818535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3312</th>\n",
              "      <td>operator female</td>\n",
              "      <td>how many people assaulted him do you know</td>\n",
              "      <td>[how, many, people, him, know]</td>\n",
              "      <td>[how, many, people, assault, him, know]</td>\n",
              "      <td>[(how, WRB), (many, JJ), (people, NNS), (assau...</td>\n",
              "      <td>[(how, WRB), (many, JJ), (people, NNS), (assau...</td>\n",
              "      <td>[(how, WRB), (many, JJ), (people, NNS), (assau...</td>\n",
              "      <td>[(how, WRB), (many, JJ), (people, NNS), (assau...</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>0.534133</td>\n",
              "      <td>0.537692</td>\n",
              "      <td>0.617103</td>\n",
              "      <td>0.721233</td>\n",
              "      <td>0.587910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3313</th>\n",
              "      <td>caller male</td>\n",
              "      <td>do not know i was at the front door for this n...</td>\n",
              "      <td>[know, at, front, door, for, this, now, they, ...</td>\n",
              "      <td>[know, at, front, door, for, this, now, they, ...</td>\n",
              "      <td>[(do, VB), (not, RB), (know, VB), (i, NN), (wa...</td>\n",
              "      <td>[(do, VB), (not, RB), (know, VB), (i, RB), (be...</td>\n",
              "      <td>[(do, VB), (not, RB), (know, VB), (i, FW), (wa...</td>\n",
              "      <td>[(do, VB), (not, RB), (know, VB), (i, FW), (be...</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>0.616518</td>\n",
              "      <td>0.622951</td>\n",
              "      <td>0.623789</td>\n",
              "      <td>0.755941</td>\n",
              "      <td>0.563802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3314</th>\n",
              "      <td>operator female</td>\n",
              "      <td>okay sir we'll get someone out as soon as we can</td>\n",
              "      <td>[sir, we, will, get, someone, out, as, soon, a...</td>\n",
              "      <td>[sir, we, will, get, someone, out, as, soon, w...</td>\n",
              "      <td>[(okay, JJ), (sir, NN), (we, PRP), (will, MD),...</td>\n",
              "      <td>[(okay, JJ), (sir, NN), (we, PRP), (will, MD),...</td>\n",
              "      <td>[(okay, JJ), (sir, NNP), (we, PRP), (will, MD)...</td>\n",
              "      <td>[(okay, JJ), (sir, NNP), (we, PRP), (will, MD)...</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>0.211888</td>\n",
              "      <td>0.202332</td>\n",
              "      <td>0.483212</td>\n",
              "      <td>0.768013</td>\n",
              "      <td>0.372024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3315</th>\n",
              "      <td>caller male</td>\n",
              "      <td>thank you</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>[(thank, NN), (you, PRP)]</td>\n",
              "      <td>[(thank, NN), (you, PRP)]</td>\n",
              "      <td>[(thank, VB), (you, PRP)]</td>\n",
              "      <td>[(thank, VB), (you, PRP)]</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>0.347395</td>\n",
              "      <td>0.347395</td>\n",
              "      <td>0.747736</td>\n",
              "      <td>0.904503</td>\n",
              "      <td>0.651379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3316</th>\n",
              "      <td>operator female</td>\n",
              "      <td>you're welcome</td>\n",
              "      <td>[welcome]</td>\n",
              "      <td>[welcome]</td>\n",
              "      <td>[(you, PRP), (are, VBP), (welcome, JJ)]</td>\n",
              "      <td>[(you, PRP), (be, VB), (welcome, JJ)]</td>\n",
              "      <td>[(you, PRP), (are, VBP), (welcome, JJ)]</td>\n",
              "      <td>[(you, PRP), (be, VB), (welcome, JJ)]</td>\n",
              "      <td>9-TUC_0.txt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3317 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            participant                                            content  \\\n",
              "0       Operator female                                  tuscon police xxx   \n",
              "1     Operator 2 female                                        this is xxx   \n",
              "2           Caller male                               i need somebody here   \n",
              "3     Operator 2 female   sir hold on one second okay he saw a patient ...   \n",
              "4       Operator female  okay hello does it look like he's been stabbed...   \n",
              "...                 ...                                                ...   \n",
              "3312    operator female          how many people assaulted him do you know   \n",
              "3313        caller male  do not know i was at the front door for this n...   \n",
              "3314    operator female   okay sir we'll get someone out as soon as we can   \n",
              "3315        caller male                                          thank you   \n",
              "3316    operator female                                     you're welcome   \n",
              "\n",
              "                                                  token  \\\n",
              "0                                 [tucson, police, xxx]   \n",
              "1                                           [this, xxx]   \n",
              "2                                [need, somebody, here]   \n",
              "3     [sir, hold, one, second, saw, patient, in, roa...   \n",
              "4                     [hello, look, like, or, shot, or]   \n",
              "...                                                 ...   \n",
              "3312                     [how, many, people, him, know]   \n",
              "3313  [know, at, front, door, for, this, now, they, ...   \n",
              "3314  [sir, we, will, get, someone, out, as, soon, a...   \n",
              "3315                                            [thank]   \n",
              "3316                                          [welcome]   \n",
              "\n",
              "                                                  lemma  \\\n",
              "0                                 [tucson, police, xxx]   \n",
              "1                                           [this, xxx]   \n",
              "2                                [need, somebody, here]   \n",
              "3     [sir, hold, one, second, saw, patient, in, roa...   \n",
              "4                 [hello, look, like, or, shot, or, go]   \n",
              "...                                                 ...   \n",
              "3312            [how, many, people, assault, him, know]   \n",
              "3313  [know, at, front, door, for, this, now, they, ...   \n",
              "3314  [sir, we, will, get, someone, out, as, soon, w...   \n",
              "3315                                            [thank]   \n",
              "3316                                          [welcome]   \n",
              "\n",
              "                                           tagged_token  \\\n",
              "0               [(tucson, NN), (police, NN), (xxx, NN)]   \n",
              "1                    [(this, DT), (is, VBZ), (xxx, JJ)]   \n",
              "2     [(i, NNS), (need, VBP), (somebody, NN), (here,...   \n",
              "3     [(sir, NN), (hold, NN), (on, IN), (one, CD), (...   \n",
              "4     [(okay, JJ), (hello, NN), (does, VBZ), (it, PR...   \n",
              "...                                                 ...   \n",
              "3312  [(how, WRB), (many, JJ), (people, NNS), (assau...   \n",
              "3313  [(do, VB), (not, RB), (know, VB), (i, NN), (wa...   \n",
              "3314  [(okay, JJ), (sir, NN), (we, PRP), (will, MD),...   \n",
              "3315                          [(thank, NN), (you, PRP)]   \n",
              "3316            [(you, PRP), (are, VBP), (welcome, JJ)]   \n",
              "\n",
              "                                           tagged_lemma  \\\n",
              "0               [(tucson, NN), (police, NN), (xxx, NN)]   \n",
              "1                     [(this, DT), (be, VB), (xxx, JJ)]   \n",
              "2     [(i, NNS), (need, VBP), (somebody, NN), (here,...   \n",
              "3     [(sir, NN), (hold, NN), (on, IN), (one, CD), (...   \n",
              "4     [(okay, NN), (hello, NN), (do, VBP), (it, PRP)...   \n",
              "...                                                 ...   \n",
              "3312  [(how, WRB), (many, JJ), (people, NNS), (assau...   \n",
              "3313  [(do, VB), (not, RB), (know, VB), (i, RB), (be...   \n",
              "3314  [(okay, JJ), (sir, NN), (we, PRP), (will, MD),...   \n",
              "3315                          [(thank, NN), (you, PRP)]   \n",
              "3316              [(you, PRP), (be, VB), (welcome, JJ)]   \n",
              "\n",
              "                                      tagged_stan_token  \\\n",
              "0               [(tucson, NN), (police, NN), (xxx, NN)]   \n",
              "1                    [(this, DT), (is, VBZ), (xxx, NN)]   \n",
              "2     [(i, LS), (need, MD), (somebody, NN), (here, RB)]   \n",
              "3     [(sir, NNP), (hold, VBP), (on, IN), (one, CD),...   \n",
              "4     [(okay, JJ), (hello, UH), (does, VBZ), (it, PR...   \n",
              "...                                                 ...   \n",
              "3312  [(how, WRB), (many, JJ), (people, NNS), (assau...   \n",
              "3313  [(do, VB), (not, RB), (know, VB), (i, FW), (wa...   \n",
              "3314  [(okay, JJ), (sir, NNP), (we, PRP), (will, MD)...   \n",
              "3315                          [(thank, VB), (you, PRP)]   \n",
              "3316            [(you, PRP), (are, VBP), (welcome, JJ)]   \n",
              "\n",
              "                                      tagged_stan_lemma         file  \\\n",
              "0               [(tucson, NN), (police, NN), (xxx, NN)]  1-TUC_0.txt   \n",
              "1                     [(this, DT), (be, VB), (xxx, NN)]  1-TUC_0.txt   \n",
              "2     [(i, LS), (need, MD), (somebody, NN), (here, RB)]  1-TUC_0.txt   \n",
              "3     [(sir, NNP), (hold, VBP), (on, IN), (one, CD),...  1-TUC_0.txt   \n",
              "4     [(okay, JJ), (hello, UH), (do, VBP), (it, PRP)...  1-TUC_0.txt   \n",
              "...                                                 ...          ...   \n",
              "3312  [(how, WRB), (many, JJ), (people, NNS), (assau...  9-TUC_0.txt   \n",
              "3313  [(do, VB), (not, RB), (know, VB), (i, FW), (be...  9-TUC_0.txt   \n",
              "3314  [(okay, JJ), (sir, NNP), (we, PRP), (will, MD)...  9-TUC_0.txt   \n",
              "3315                          [(thank, VB), (you, PRP)]  9-TUC_0.txt   \n",
              "3316              [(you, PRP), (be, VB), (welcome, JJ)]  9-TUC_0.txt   \n",
              "\n",
              "      W2V_lemma_cosine_similarity  W2V_token_cosine_similarity  \\\n",
              "0                        0.718296                     0.718296   \n",
              "1                        0.239677                     0.239677   \n",
              "2                        0.693040                     0.669003   \n",
              "3                        0.675061                     0.564028   \n",
              "4                        0.703610                     0.670530   \n",
              "...                           ...                          ...   \n",
              "3312                     0.534133                     0.537692   \n",
              "3313                     0.616518                     0.622951   \n",
              "3314                     0.211888                     0.202332   \n",
              "3315                     0.347395                     0.347395   \n",
              "3316                          NaN                          NaN   \n",
              "\n",
              "      BERT_cosine_similarity  GPT_cosine_similarity  Llama_cosine_similarity  \n",
              "0                   0.624709               0.808507                 0.645413  \n",
              "1                   0.537029               0.806422                 0.445610  \n",
              "2                   0.560584               0.757559                 0.584490  \n",
              "3                   0.778168               0.774660                 0.748699  \n",
              "4                   0.770860               0.820494                 0.818535  \n",
              "...                      ...                    ...                      ...  \n",
              "3312                0.617103               0.721233                 0.587910  \n",
              "3313                0.623789               0.755941                 0.563802  \n",
              "3314                0.483212               0.768013                 0.372024  \n",
              "3315                0.747736               0.904503                 0.651379  \n",
              "3316                     NaN                    NaN                      NaN  \n",
              "\n",
              "[3317 rows x 14 columns]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "395"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df[W2V_BERT_GPT_Llama_concatenated_df['W2V_lemma_cosine_similarity'] > 0.75].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "353"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df[W2V_BERT_GPT_Llama_concatenated_df['W2V_token_cosine_similarity'] > 0.75].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "481"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df[W2V_BERT_GPT_Llama_concatenated_df['BERT_cosine_similarity'] > 0.75].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2495"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df[W2V_BERT_GPT_Llama_concatenated_df['GPT_cosine_similarity'] > 0.75].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "830"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W2V_BERT_GPT_Llama_concatenated_df[W2V_BERT_GPT_Llama_concatenated_df['Llama_cosine_similarity'] > 0.75].shape[0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
