{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required environment setup\n",
    "\n",
    "For Local:\n",
    "```\n",
    "conda create -n gpt-bert-align python=3.10\n",
    "conda activate gpt-bert-align\n",
    "\n",
    "cat > requirements.txt <<EOL\n",
    "jupyter\n",
    "ipykernel\n",
    "ipywidgets\n",
    "openai\n",
    "torch\n",
    "transformers\n",
    "sentence-transformers\n",
    "scikit-learn\n",
    "nltk\n",
    "gensim\n",
    "matplotlib\n",
    "plotly\n",
    "pandas\n",
    "EOL\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "For Sol:\n",
    "```\n",
    "module load mamba/latest\n",
    "mamba create -n gpt-bert-align python=3.10\n",
    "source activate gpt-bert-align\n",
    "cat > requirements.txt\n",
    "pip install -r requirements.txt\n",
    "mkjupy gpt-bert-align\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "Next step is to probably integrate the below with the original ALIGN code (`troubleshootSemCosine`). A version that includes EVERYTHING with GPT and BERT added. Another version with just GPT and BERT, dropping word2vec. If I do this, I can rerun the R code with some confidence. \n",
    "\n",
    "Then, need to optimize code. I get the sense that the original ALIGN code is extremely slow. Need to use the groupBy sort of functionality to do this more effectively. Maybe create a version that just does this with GPT and BERT (maybe word2vec) for now. An ALIGN-CONCEPTUAL package. But need to figure out why I'm having such difficulty with GPT with groupBy. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[OpenAI documentation](https://platform.openai.com/docs/guides/embeddings/use-cases)\n",
    "\n",
    "[OpenAI cookbook sample code for embeddings](https://github.com/openai/openai-cookbook/blob/main/examples/Obtain_dataset.ipynb)\n",
    "\n",
    "[sentence-transformers; sbert documentation](https://www.sbert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os,re,math,csv,string,random,logging,glob,itertools,operator,sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import chain, combinations\n",
    "from pathlib import Path\n",
    "import json,pickle\n",
    "import time,datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import openai\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALIGN method for computing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filtered_vocab(concat_transcripts,\n",
    "                        output_file_directory,\n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1):\n",
    "    \n",
    "    # build vocabulary list from transcripts\n",
    "    data1 = pd.read_csv(concat_transcripts, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # get frequency count of all included words (as tokens) NOTE: previous default was lemmas\n",
    "    all_sentences = [re.sub('[^\\w\\s]+','',str(row)).split(' ') for row in list(data1['lemma'])]\n",
    "    all_words = list([a for b in all_sentences for a in b])\n",
    "    frequency = defaultdict(int)\n",
    "    for word in all_words:\n",
    "        frequency[word] += 1\n",
    "\n",
    "    ## start filtering process\n",
    "\n",
    "    # remove one-letter words (noise or extremely high frequency)\n",
    "    frequency_filt = {word: freq for word, freq in frequency.items() if len(word) > 1}\n",
    "    \n",
    "    # if desired, remove words that only occur more frequently than our cutoff (defined in occurrences)\n",
    "    frequency_filt = {word: freq for word, freq in frequency_filt.items() if freq > low_n_cutoff}\n",
    "\n",
    "    # if desired, remove high-frequency words (over user-defined SDs above mean AFTER removing one-letter words [which will impact SD])\n",
    "    if high_sd_cutoff is None:\n",
    "        filteredWords = [word for word in list(frequency_filt.keys())]\n",
    "    else:\n",
    "        getOut = np.mean(list(frequency_filt.values()))+(np.std(list(frequency_filt.values()))*(high_sd_cutoff))\n",
    "        # filteredWords = list({word: freq for word, freq in frequency_filt.items() if freq < getOut}.keys())\n",
    "        filteredWords = {word: freq for word, freq in frequency_filt.items() if freq < getOut}\n",
    "\n",
    "    ############ BONUS opertation: prints the frequency lists\n",
    "    vocabfreq_all = pd.DataFrame(list(frequency.items()), columns=[\"word\", \"count\"]).sort_values(by=['count'], ascending=False)\n",
    "    vocabfreq_filt = pd.DataFrame(list(filteredWords.items()), columns=[\"word\", \"count\"]).sort_values(by=['count'], ascending=False)\n",
    "    \n",
    "    vocabfreq_file = os.path.join(output_file_directory,'vocab_unfilt_freqs.txt')\n",
    "    vocabfreq_all.to_csv(vocabfreq_file, encoding='utf-8',index=False, sep='\\t')\n",
    "    \n",
    "    vocabfreq_filt_file = os.path.join(output_file_directory,'vocab_filt_freqs.txt')\n",
    "    vocabfreq_filt.to_csv(vocabfreq_filt_file, encoding='utf-8',index=False, sep='\\t')\n",
    "    ############\n",
    "\n",
    "    return list(frequency.keys()), list(filteredWords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v_trained(pretrained_input_file):\n",
    "\n",
    "    model = api.load(pretrained_input_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_utterance_string(tok_seq,vocablist,highDimModel):\n",
    "    \n",
    "    # Only consider the words that are in the vocablist after filtering for various criteria (e.g., only occur once, high frequency)\n",
    "    filter_vocablist = [word for word in tok_seq if word in vocablist]\n",
    "    \n",
    "    # Only consider the words that are in the pre-trained model vocabulary\n",
    "    filter_model = [word for word in filter_vocablist if highDimModel.has_index_for(word)]\n",
    "\n",
    "    return filter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_utterance_comp_norm(tok_seq,vocablist,highDimModel):    \n",
    "    \n",
    "    # Only consider the words that are in the vocablist after filtering for various criteria (e.g., only occur once, high frequency)\n",
    "    filter_tok_seq = [word for word in tok_seq if word in vocablist]\n",
    "        \n",
    "    # Retrieve the Word2Vec vectors for each word in the sentence; ignores any words not in the pre-trained model vocabulary\n",
    "    word_vectors = [highDimModel[word] for word in filter_tok_seq if highDimModel.has_index_for(word)]\n",
    "\n",
    "    # If no word vectors were retrieved, return None\n",
    "    if not word_vectors:\n",
    "        return None, None\n",
    "\n",
    "    # Aggregate the Word2Vec vectors using averaging (or sum, as originally done in ALIGN)\n",
    "    vector_avg = np.mean(word_vectors, axis=0)\n",
    "    vector_sum = np.sum(word_vectors, axis=0)\n",
    "    \n",
    "    return vector_avg, vector_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conceptual_alignment_w2v(sentence1, sentence2, vocablist, highDimModel):    \n",
    "    # Process the sentences\n",
    "    [vector1avg, vector1sum] = process_utterance_comp_norm(sentence1, vocablist, highDimModel)\n",
    "    [vector2avg, vector2sum] = process_utterance_comp_norm(sentence2, vocablist, highDimModel)\n",
    "\n",
    "    # If either vector is None (i.e., the corresponding sentence had no known words), return 0\n",
    "    if vector1sum is None or vector2sum is None:\n",
    "        return 0\n",
    "\n",
    "    # Normalize the aggregated vectors\n",
    "    vector1_norm = vector1sum / np.linalg.norm(vector1sum)\n",
    "    vector2_norm = vector2sum / np.linalg.norm(vector2sum)\n",
    "\n",
    "    # Calculate cosine similarity (this is equivalent to dot product between two vectors given these are normalized vectors)\n",
    "    similarity = cosine_similarity([vector1_norm], [vector2_norm])\n",
    "    \n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conceptual_alignment_GPT(content1, content2,\n",
    "                           highDimModel):\n",
    "\n",
    "    emb1 = get_embedding(content1, engine=highDimModel)\n",
    "    emb2 = get_embedding(content2, engine=highDimModel)\n",
    "    simGPT = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        \n",
    "    return simGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conceptual_alignment_BERT(content1, content2,\n",
    "                            highDimModel):\n",
    "\n",
    "    emb1 = highDimModel.encode(content1)\n",
    "    emb2 = highDimModel.encode(content2)\n",
    "    simBERT = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        \n",
    "    return simBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_multilevel_alignment(cond_info,\n",
    "                                   partnerA,tok1,lem1,content1,\n",
    "                                   partnerB,tok2,lem2,content2,\n",
    "                                   vocablist, \n",
    "                                   w2v_model_goog, w2v_model_twit, \n",
    "                                   bert_model,\n",
    "                                   gpt_model\n",
    "                                   ):\n",
    "\n",
    "    # create empty dictionaries\n",
    "    partner_direction = {}\n",
    "    condition_info = {}\n",
    "    semantic_W2V_goog = {}\n",
    "    semantic_W2V_twit = {}\n",
    "    utterance1_W2V = {}\n",
    "    utterance2_W2V = {}\n",
    "    # semantic_GPT = {}\n",
    "    semantic_BERT = {}\n",
    "    # utterance_length1 = {}\n",
    "    # utterance_length2 = {}\n",
    "\n",
    "    dictionaries_list = []\n",
    "\n",
    "    # calculate conceptual alignment: word2vec: Google\n",
    "    semantic_W2V_goog['semantic_W2V_goog'] = conceptual_alignment_w2v(lem1,lem2,vocablist,w2v_model_goog)\n",
    "    dictionaries_list.append(semantic_W2V_goog.copy())\n",
    "\n",
    "    # calculate conceptual alignment: word2vec: Twit  \n",
    "    semantic_W2V_twit['semantic_W2V_twit'] = conceptual_alignment_w2v(lem1,lem2,vocablist,w2v_model_twit)\n",
    "    dictionaries_list.append(semantic_W2V_twit.copy())\n",
    "\n",
    "    # return utterances being compared: word2vec (just going to use Google for now)\n",
    "    utterance1_W2V['utterance1_W2V'] = process_utterance_string(lem1,vocablist,w2v_model_goog)\n",
    "    dictionaries_list.append(utterance1_W2V.copy())\n",
    "    utterance2_W2V['utterance2_W2V'] = process_utterance_string(lem2,vocablist,w2v_model_goog)    \n",
    "    dictionaries_list.append(utterance2_W2V.copy())   \n",
    "\n",
    "    # calculate conceptual alignment: GPT    \n",
    "    # semantic_GPT['semantic_GPT'] = conceptual_alignment_GPT(content1,content2,gpt_model)\n",
    "    # dictionaries_list.append(semantic_GPT.copy())\n",
    "    \n",
    "    # # calculate conceptual alignment: BERT    \n",
    "    semantic_BERT['semantic_BERT'] = conceptual_alignment_BERT(content1,content2,bert_model)\n",
    "    dictionaries_list.append(semantic_BERT.copy())\n",
    "\n",
    "    # determine directionality of leading/following comparison;  Note: Partner B is the lagged partner, thus, B is following A\n",
    "    partner_direction['partner_direction'] = str(partnerA) + \">\" + str(partnerB)\n",
    "    dictionaries_list.append(partner_direction.copy())\n",
    "\n",
    "    # add number of tokens in each utterance \n",
    "    # NOTE: For semantic, this is incorrect as a lot of tokens are removed in the vocablist filtering step, can just do this in R\n",
    "    # utterance_length1['utterance_length1'] = len(lem1)\n",
    "    # dictionaries_list.append(utterance_length1.copy())\n",
    "\n",
    "    # utterance_length2['utterance_length2'] = len(lem2)\n",
    "    # dictionaries_list.append(utterance_length2.copy())\n",
    "\n",
    "    # add condition information\n",
    "    condition_info['condition_info'] = cond_info\n",
    "    dictionaries_list.append(condition_info.copy())\n",
    "\n",
    "    # return alignment scores\n",
    "    return dictionaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_by_turn_analysis(\n",
    "                    dataframe,\n",
    "                    delay,\n",
    "                    vocablist,\n",
    "                    w2v_model_goog,\n",
    "                    w2v_model_twit,\n",
    "                    bert_model,\n",
    "                    gpt_model\n",
    "                    ):\n",
    "\n",
    "    # prepare the data to the appropriate type\n",
    "    dataframe['token'] = dataframe['token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['lemma'] = dataframe['lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "\n",
    "    # create lagged version of the dataframe\n",
    "    df_original = dataframe.drop(dataframe.tail(delay).index,inplace=False)\n",
    "    df_lagged = dataframe.shift(-delay).drop(dataframe.tail(delay).index,inplace=False)\n",
    "\n",
    "    # cycle through each pair of turns\n",
    "    # aggregated_df = pd.DataFrame()\n",
    "    tmpfiles = list()\n",
    "    \n",
    "    for i in range(0,df_original.shape[0]):\n",
    "\n",
    "        # identify the condition for this dataframe\n",
    "        cond_info = dataframe['file'].unique()\n",
    "        if len(cond_info)==1:\n",
    "            cond_info = str(cond_info[0])\n",
    "\n",
    "        # break and flag error if we have more than 1 condition per dataframe\n",
    "        else:\n",
    "            raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "\n",
    "        # grab all of first participant's data\n",
    "        first_row = df_original.iloc[i]\n",
    "        first_partner = first_row['participant']\n",
    "        tok1=first_row['token']\n",
    "        lem1=first_row['lemma']\n",
    "        content1=first_row['content'] ## NOTE: to be used with chatGPT and BERT; but could be cleaned for conjunctions and misspelled words?\n",
    "\n",
    "        # grab all of lagged participant's data\n",
    "        lagged_row = df_lagged.iloc[i]\n",
    "        lagged_partner = lagged_row['participant']\n",
    "        tok2=lagged_row['token']\n",
    "        lem2=lagged_row['lemma']\n",
    "        content2=lagged_row['content'] ## NOTE: to be used with chatGPT and BERT; but could be cleaned for conjunctions and misspelled words?\n",
    "\n",
    "        # process multilevel alignment\n",
    "        dictionaries_list=return_multilevel_alignment(cond_info=cond_info,\n",
    "                                                         partnerA=first_partner,\n",
    "                                                         tok1=tok1,lem1=lem1,\n",
    "                                                         content1=content1,\n",
    "                                                         partnerB=lagged_partner,\n",
    "                                                         tok2=tok2,lem2=lem2,\n",
    "                                                         content2=content2,\n",
    "                                                         vocablist=vocablist,\n",
    "                                                         w2v_model_goog=w2v_model_goog,\n",
    "                                                         w2v_model_twit=w2v_model_twit,\n",
    "                                                         bert_model=bert_model,\n",
    "                                                         gpt_model=gpt_model)\n",
    "\n",
    "        # sort columns so they are in order, append data to existing structures\n",
    "        next_df_line = pd.DataFrame.from_dict(OrderedDict(k for num, i in enumerate(d for d in dictionaries_list) for k in sorted(i.items())),\n",
    "                               orient='index').transpose()\n",
    "            \n",
    "        # aggregated_df = aggregated_df.append(next_df_line) ## problematic. appending a dataframe to a dataframe. \n",
    "        tmpfiles.append(next_df_line)    \n",
    "    \n",
    "    # reformat turn information and add index\n",
    "    aggregated_df = pd.concat(tmpfiles)\n",
    "    aggregated_df = aggregated_df.reset_index(drop=True).reset_index().rename(columns={\"index\":\"time\"})\n",
    "\n",
    "    # give us our finished dataframe\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alignment(input_files,\n",
    "                        output_file_directory,\n",
    "                        delay,\n",
    "                        concat_transcripts,\n",
    "                        high_sd_cutoff,\n",
    "                        low_n_cutoff,\n",
    "                        model_input_file_w2v_twit,\n",
    "                        model_input_file_w2v_goog,\n",
    "                        model_id_bert,\n",
    "                        model_id_gpt\n",
    "                        ):\n",
    "\n",
    "    # get the various semantic models to be used throughout\n",
    "    # chatGPT\n",
    "    \n",
    "    # BERT\n",
    "    bert_model = st.SentenceTransformer(model_id_bert) \n",
    "    \n",
    "    # w2v_Google\n",
    "    w2v_model_goog = load_w2v_trained(\n",
    "                            pretrained_input_file=model_input_file_w2v_goog\n",
    "                            )        \n",
    "    \n",
    "    # w2v_Twitter\n",
    "    w2v_model_twit = load_w2v_trained(\n",
    "                            pretrained_input_file=model_input_file_w2v_twit        )      \n",
    "    \n",
    "    # for w2v, need the filtered vocabulary list to be used to identify content words\n",
    "    [unfiltered, filtered] = build_filtered_vocab(\n",
    "                                            concat_transcripts=concat_transcripts,\n",
    "                                            output_file_directory=output_file_directory,\n",
    "                                            high_sd_cutoff=high_sd_cutoff,\n",
    "                                            low_n_cutoff=low_n_cutoff\n",
    "                                            )\n",
    "    \n",
    "    # time to begin looping through individual conversations and generating values\n",
    "                                            \n",
    "    # grab the files in the list\n",
    "    file_list = glob.glob(input_files+\"/*.txt\")\n",
    "\n",
    "    # create containers for alignment values\n",
    "    tempT2T = list()\n",
    "    # tempC2C = list()\n",
    "\n",
    "    # cycle through each prepared file\n",
    "    for fileName in file_list:\n",
    "\n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 1:\n",
    "\n",
    "            # let us know which filename we're processing\n",
    "            print((\"Processing: \"+fileName))\n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=turn_by_turn_analysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         vocablist=filtered,\n",
    "                                         w2v_model_goog=w2v_model_goog,\n",
    "                                         w2v_model_twit=w2v_model_twit,\n",
    "                                         bert_model=bert_model,\n",
    "                                         gpt_model=model_id_gpt)\n",
    "            tempT2T.append(xT2T)\n",
    "\n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print((\"Invalid file: \"+fileName))\n",
    "\n",
    "    # update final dataframes\n",
    "    AlignmentT2T = pd.concat(tempT2T)\n",
    "    real_final_turn_df = AlignmentT2T.reset_index(drop=True)\n",
    "\n",
    "    # export the final files\n",
    "    now = datetime.datetime.now()\n",
    "    date_string = now.strftime(\"%Y-%m-%d_%H-%M\") \n",
    "    outfile = output_file_directory + f\"/semantic_output_\"+date_string+\".txt\"\n",
    "    # outfile = output_file_directory + f\"/{model_id_w2v}_{model_id_gpt}_{model_id_bert}_\"+date_string+\".txt\"    \n",
    "    real_final_turn_df.to_csv(outfile,                      \n",
    "                      encoding='utf-8', index=False, sep='\\t')\n",
    "\n",
    "    return real_final_turn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nickduran/Dropbox (ASU)/CPS-ALIGN/CPS-REV/ROUND2-split-level/tool_output/prepped_stan_small/ASU-T10_ExpBlock2-DolphinShow.txt\n",
      "Processing: /Users/nickduran/Dropbox (ASU)/CPS-ALIGN/CPS-REV/ROUND2-split-level/tool_output/prepped_stan_small/ASU-T10_ExpBlock1-Oneatatime.txt\n"
     ]
    }
   ],
   "source": [
    "# where are all the indiv txt files stored, and where to put the output generated from this analysis\n",
    "INPUT_FILES = \"/Users/nickduran/Dropbox (ASU)/CPS-ALIGN/CPS-REV/ROUND2-split-level/tool_output/prepped_stan_small/\"\n",
    "OUTPUT_FILES = \"/Users/nickduran/Desktop/GitProjects/align-linguistic-alignment/sandbox/transformers/outputs-semantic/\"\n",
    "\n",
    "# set standards for which words will be evaluated in word2vec analysis\n",
    "TRANSCRIPTS_CONCAT_VOCAB_FILE = \"/Users/nduran4/Dropbox (ASU)/Mac/Desktop/GitProjects/align-linguistic-alignment/sandbox/transformers/align_concatenated_dataframe.txt\"\n",
    "HIGH_SD_CUTOFF = None\n",
    "LOW_N_CUTOFF = 1\n",
    "\n",
    "# set standards to be used for real and surrogate\n",
    "MAXLAG = 1\n",
    "PAIR_TYPE = \"real\"\n",
    "\n",
    "# for loading in the gensim w2v models\n",
    "MODEL_w2v_google = 'word2vec-google-news-300'\n",
    "MODEL_w2v_twitter = 'glove-twitter-200'\n",
    "\n",
    "# for loading in the BERT and GPT models\n",
    "MODEL_BERT = 'all-mpnet-base-v2'\n",
    "\n",
    "MODEL_GPT = 'text-embedding-ada-002'\n",
    "openai.api_key = \"INSERT HERE\"\n",
    "\n",
    "turn_real = calculate_alignment(\n",
    "                        input_files=INPUT_FILES,\n",
    "                        output_file_directory=OUTPUT_FILES,\n",
    "                        delay=MAXLAG,\n",
    "                        concat_transcripts=TRANSCRIPTS_CONCAT_VOCAB_FILE,\n",
    "                        high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                        low_n_cutoff=LOW_N_CUTOFF,\n",
    "                        model_input_file_w2v_twit=MODEL_w2v_twitter,\n",
    "                        model_input_file_w2v_goog=MODEL_w2v_google,\n",
    "                        model_id_bert=MODEL_BERT,\n",
    "                        model_id_gpt=MODEL_GPT\n",
    "                        )      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform_openAIp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
