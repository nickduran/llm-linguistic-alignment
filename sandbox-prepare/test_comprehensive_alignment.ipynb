{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42890f8",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc181c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ndd697/Desktop/Github-Projects/llm-linguistic-alignment/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Import Required Libraries\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import ALIGN package components\n",
    "from align_test.alignment import LinguisticAlignment\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6a91ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created directory: ./test_alignment_results\n",
      "‚úì Created directory: ./test_baseline_results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Configuration - Set Your Data Directories\n",
    "# ============================================================\n",
    "\n",
    "# INPUT DIRECTORIES (from preprocessing notebook)\n",
    "# These should contain the preprocessed .txt files\n",
    "INPUT_DIR_BASIC = './test_output_basic'      # NLTK-only preprocessing\n",
    "INPUT_DIR_SPACY = './test_output_spacy'      # NLTK + spaCy preprocessing\n",
    "INPUT_DIR_STANFORD = './test_output_stanford' # NLTK + Stanford preprocessing (optional)\n",
    "\n",
    "# OUTPUT DIRECTORIES (for alignment results)\n",
    "OUTPUT_DIR_ALIGNMENT = './test_alignment_results'\n",
    "OUTPUT_DIR_BASELINE = './test_baseline_results'\n",
    "\n",
    "# Create output directories\n",
    "for dir_path in [OUTPUT_DIR_ALIGNMENT, OUTPUT_DIR_BASELINE]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"‚úì Created directory: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d9bc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying preprocessed input data...\n",
      "============================================================\n",
      "‚úì NLTK-only (REQUIRED): ./test_output_basic\n",
      "  Found 21 files\n",
      "‚úì spaCy-tagged (OPTIONAL): ./test_output_spacy\n",
      "  Found 21 files\n",
      "‚úì Stanford-tagged (OPTIONAL): ./test_output_stanford\n",
      "  Found 21 files\n",
      "\n",
      "‚úì Ready to test with 3 input types\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Verify Input Data Exists\n",
    "# ============================================================\n",
    "\n",
    "print(\"Verifying preprocessed input data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "input_dirs = {\n",
    "    \"NLTK-only (REQUIRED)\": INPUT_DIR_BASIC,\n",
    "    \"spaCy-tagged (OPTIONAL)\": INPUT_DIR_SPACY,\n",
    "    \"Stanford-tagged (OPTIONAL)\": INPUT_DIR_STANFORD\n",
    "}\n",
    "\n",
    "available_inputs = {}\n",
    "\n",
    "for label, path in input_dirs.items():\n",
    "    if os.path.exists(path):\n",
    "        files = [f for f in os.listdir(path) if f.endswith('.txt')]\n",
    "        print(f\"‚úì {label}: {path}\")\n",
    "        print(f\"  Found {len(files)} files\")\n",
    "        available_inputs[label] = path\n",
    "    else:\n",
    "        print(f\"‚úó {label}: {path} (not found)\")\n",
    "\n",
    "if \"NLTK-only (REQUIRED)\" not in available_inputs:\n",
    "    print(\"\\n‚ùå ERROR: Required NLTK-only preprocessing data not found!\")\n",
    "    print(\"Please run test_prepare_transcripts.ipynb first.\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Ready to test with {len(available_inputs)} input types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbc1e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting sample preprocessed file...\n",
      "============================================================\n",
      "\n",
      "Reading: time197-cond1.txt\n",
      "\n",
      "Columns: ['participant', 'content', 'token', 'lemma', 'tagged_token', 'tagged_lemma', 'file']\n",
      "Rows: 76\n",
      "\n",
      "First 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tagged_token</th>\n",
       "      <th>tagged_lemma</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cgv</td>\n",
       "      <td>that was fun</td>\n",
       "      <td>['that', 'was', 'fun']</td>\n",
       "      <td>['that', 'be', 'fun']</td>\n",
       "      <td>[('that', 'DT'), ('was', 'VBD'), ('fun', 'NN')]</td>\n",
       "      <td>[('that', 'DT'), ('be', 'VB'), ('fun', 'NN')]</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kid</td>\n",
       "      <td>dad you should have climbed the cliffs with us</td>\n",
       "      <td>['dad', 'you', 'should', 'have', 'climbed', 't...</td>\n",
       "      <td>['dad', 'you', 'should', 'have', 'climb', 'the...</td>\n",
       "      <td>[('dad', 'NN'), ('you', 'PRP'), ('should', 'MD...</td>\n",
       "      <td>[('dad', 'NN'), ('you', 'PRP'), ('should', 'MD...</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cgv</td>\n",
       "      <td>next time i will</td>\n",
       "      <td>['next', 'time', 'i', 'will']</td>\n",
       "      <td>['next', 'time', 'i', 'will']</td>\n",
       "      <td>[('next', 'JJ'), ('time', 'NN'), ('i', 'NN'), ...</td>\n",
       "      <td>[('next', 'JJ'), ('time', 'NN'), ('i', 'NN'), ...</td>\n",
       "      <td>time197-cond1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant                                         content  \\\n",
       "0         cgv                                    that was fun   \n",
       "1         kid  dad you should have climbed the cliffs with us   \n",
       "2         cgv                                next time i will   \n",
       "\n",
       "                                               token  \\\n",
       "0                             ['that', 'was', 'fun']   \n",
       "1  ['dad', 'you', 'should', 'have', 'climbed', 't...   \n",
       "2                      ['next', 'time', 'i', 'will']   \n",
       "\n",
       "                                               lemma  \\\n",
       "0                              ['that', 'be', 'fun']   \n",
       "1  ['dad', 'you', 'should', 'have', 'climb', 'the...   \n",
       "2                      ['next', 'time', 'i', 'will']   \n",
       "\n",
       "                                        tagged_token  \\\n",
       "0    [('that', 'DT'), ('was', 'VBD'), ('fun', 'NN')]   \n",
       "1  [('dad', 'NN'), ('you', 'PRP'), ('should', 'MD...   \n",
       "2  [('next', 'JJ'), ('time', 'NN'), ('i', 'NN'), ...   \n",
       "\n",
       "                                        tagged_lemma               file  \n",
       "0      [('that', 'DT'), ('be', 'VB'), ('fun', 'NN')]  time197-cond1.txt  \n",
       "1  [('dad', 'NN'), ('you', 'PRP'), ('should', 'MD...  time197-cond1.txt  \n",
       "2  [('next', 'JJ'), ('time', 'NN'), ('i', 'NN'), ...  time197-cond1.txt  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Inspect Sample Preprocessed File\n",
    "# ============================================================\n",
    "\n",
    "print(\"Inspecting sample preprocessed file...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load a sample file\n",
    "sample_file = [f for f in os.listdir(INPUT_DIR_BASIC) if f.endswith('.txt')][0]\n",
    "sample_path = os.path.join(INPUT_DIR_BASIC, sample_file)\n",
    "\n",
    "print(f\"\\nReading: {sample_file}\\n\")\n",
    "\n",
    "df_sample = pd.read_csv(sample_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(f\"Columns: {df_sample.columns.tolist()}\")\n",
    "print(f\"Rows: {len(df_sample)}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "df_sample.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650b717",
   "metadata": {},
   "source": [
    "## TEST 1: Lexical-Syntactic Alignment (NLTK Tags Only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daecba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing LexicalSyntacticAlignment analyzer...\n",
      "‚úì Analyzer initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lexical-syntactic analyzer\n",
    "print(\"\\nInitializing LexicalSyntacticAlignment analyzer...\")\n",
    "\n",
    "analyzer_lexsyn = LinguisticAlignment(\n",
    "    alignment_type=\"lexsyn\",\n",
    "    cache_dir=os.path.join(OUTPUT_DIR_ALIGNMENT, \"cache\")\n",
    ")\n",
    "\n",
    "print(\"‚úì Analyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8234dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running lexical-syntactic alignment analysis...\n",
      "Input folder: ./test_output_basic\n",
      "Output folder: ./test_alignment_results\n",
      "ANALYZE_FOLDER: Processing data from folder: ./test_output_basic with lag=1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LexicalSyntacticAlignment' object has no attribute 'analyze_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINPUT_DIR_BASIC\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR_ALIGNMENT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results_lexsyn_nltk = \u001b[43manalyzer_lexsyn\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINPUT_DIR_BASIC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR_ALIGNMENT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlag\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_ngram\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_duplicates\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_additional_tags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# NLTK tags only\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Alignment analysis complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUtterance pairs analyzed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results_lexsyn_nltk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github-Projects/llm-linguistic-alignment/src/align_test/alignment.py:93\u001b[39m, in \u001b[36mLinguisticAlignment.analyze_folder\u001b[39m\u001b[34m(self, folder_path, output_directory, file_pattern, lag, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m filtered_kwargs = \u001b[38;5;28mself\u001b[39m._filter_kwargs_for_analyzer(analyzer_type, kwargs)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Run analysis with filtered parameters \u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Important: Don't provide output_directory here to prevent duplicate files\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m analyzer_results = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_folder\u001b[49m(\n\u001b[32m     94\u001b[39m     folder_path=folder_path,\n\u001b[32m     95\u001b[39m     output_directory=\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Don't save intermediate results\u001b[39;00m\n\u001b[32m     96\u001b[39m     file_pattern=file_pattern,\n\u001b[32m     97\u001b[39m     **filtered_kwargs\n\u001b[32m     98\u001b[39m )\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m    101\u001b[39m results[analyzer_type] = analyzer_results\n",
      "\u001b[31mAttributeError\u001b[39m: 'LexicalSyntacticAlignment' object has no attribute 'analyze_folder'"
     ]
    }
   ],
   "source": [
    "# Run alignment analysis on NLTK-only preprocessed data\n",
    "print(\"\\nRunning lexical-syntactic alignment analysis...\")\n",
    "print(f\"Input folder: {INPUT_DIR_BASIC}\")\n",
    "print(f\"Output folder: {OUTPUT_DIR_ALIGNMENT}\")\n",
    "\n",
    "results_lexsyn_nltk = analyzer_lexsyn.analyze_folder(\n",
    "    folder_path=INPUT_DIR_BASIC,\n",
    "    output_directory=OUTPUT_DIR_ALIGNMENT,\n",
    "    lag=1,\n",
    "    max_ngram=2,\n",
    "    ignore_duplicates=True,\n",
    "    add_additional_tags=False  # NLTK tags only\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Alignment analysis complete!\")\n",
    "print(f\"Utterance pairs analyzed: {len(results_lexsyn_nltk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3dc7d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading alignment.py from: /Users/ndd697/Desktop/Github-Projects/llm-linguistic-alignment/src/align_test/alignment.py\n",
      "‚úó Using OLD alignment.py - needs to be replaced\n"
     ]
    }
   ],
   "source": [
    "# Check which version of alignment.py is being loaded\n",
    "import align_test.alignment as align_module\n",
    "print(f\"Loading alignment.py from: {align_module.__file__}\")\n",
    "\n",
    "# Check if it has the corrected code\n",
    "import inspect\n",
    "source = inspect.getsource(align_module.LinguisticAlignment.__init__)\n",
    "if 'add_additional_tags' in source:\n",
    "    print(\"‚úì Using UPDATED alignment.py\")\n",
    "else:\n",
    "    print(\"‚úó Using OLD alignment.py - needs to be replaced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf404da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZE_FOLDER: Processing data from folder: ./test_output_basic with lag=1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LexicalSyntacticAlignment' object has no attribute 'analyze_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     25\u001b[39m common_params = {\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlag\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# Number of turns to lag\u001b[39;00m\n\u001b[32m     27\u001b[39m }\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Analyze real conversations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m real_results = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINPUT_DIR_BASIC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR_ALIGNMENT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcommon_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfasttext_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlexsyn_params\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github-Projects/llm-linguistic-alignment/src/align_test/alignment.py:93\u001b[39m, in \u001b[36mLinguisticAlignment.analyze_folder\u001b[39m\u001b[34m(self, folder_path, output_directory, file_pattern, lag, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m filtered_kwargs = \u001b[38;5;28mself\u001b[39m._filter_kwargs_for_analyzer(analyzer_type, kwargs)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Run analysis with filtered parameters \u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Important: Don't provide output_directory here to prevent duplicate files\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m analyzer_results = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_folder\u001b[49m(\n\u001b[32m     94\u001b[39m     folder_path=folder_path,\n\u001b[32m     95\u001b[39m     output_directory=\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Don't save intermediate results\u001b[39;00m\n\u001b[32m     96\u001b[39m     file_pattern=file_pattern,\n\u001b[32m     97\u001b[39m     **filtered_kwargs\n\u001b[32m     98\u001b[39m )\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m    101\u001b[39m results[analyzer_type] = analyzer_results\n",
      "\u001b[31mAttributeError\u001b[39m: 'LexicalSyntacticAlignment' object has no attribute 'analyze_folder'"
     ]
    }
   ],
   "source": [
    "# from align_test.alignment import LinguisticAlignment\n",
    "\n",
    "# Initialize with multiple alignment types\n",
    "analyzer = LinguisticAlignment(\n",
    "    alignment_types=[\"lexsyn\"],  # Run one or multiple analyzers\n",
    "    cache_dir=os.path.join(OUTPUT_DIR_ALIGNMENT, \"cache\")\n",
    ")\n",
    "\n",
    "# Configure parameters for FastText\n",
    "fasttext_params = {\n",
    "    \"high_sd_cutoff\": 3,    # Filter out words with frequency > mean + 3*std\n",
    "    \"low_n_cutoff\": 2,      # Filter out words occurring < 2 times\n",
    "    \"save_vocab\": True      # Save vocabulary lists to output directory\n",
    "}\n",
    "\n",
    "# Configure parameters for Lexical/Syntactic analysis\n",
    "lexsyn_params = {\n",
    "    \"max_ngram\": 3,         # Maximum n-gram size\n",
    "    \"ignore_duplicates\": True,\n",
    "    \"add_additional_tags\": True,\n",
    "    \"additional_tagger_type\":'stanford'  # Include Stanford or Spacy POS tags if available\n",
    "}\n",
    "\n",
    "# Common parameters for all analyzers\n",
    "common_params = {\n",
    "    \"lag\": 1  # Number of turns to lag\n",
    "}\n",
    "\n",
    "# Analyze real conversations\n",
    "real_results = analyzer.analyze_folder(\n",
    "    folder_path=INPUT_DIR_BASIC,\n",
    "    output_directory=OUTPUT_DIR_ALIGNMENT,\n",
    "    **common_params,\n",
    "    **fasttext_params,\n",
    "    **lexsyn_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fee586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check if the file on disk actually has the new code\n",
    "import align_test.alignment as align_module\n",
    "\n",
    "# Get the file path\n",
    "file_path = align_module.__file__\n",
    "print(f\"Python is loading from: {file_path}\")\n",
    "\n",
    "# Read the file directly from disk\n",
    "with open(file_path, 'r') as f:\n",
    "    file_contents = f.read()\n",
    "\n",
    "# Check what's actually in the file on disk\n",
    "if 'add_additional_tags' in file_contents:\n",
    "    print(\"‚úì File ON DISK contains 'add_additional_tags'\")\n",
    "else:\n",
    "    print(\"‚úó File ON DISK does NOT contain 'add_additional_tags'\")\n",
    "\n",
    "# Check what Python is seeing in memory\n",
    "import inspect\n",
    "source = inspect.getsource(align_module.LinguisticAlignment.__init__)\n",
    "if 'add_additional_tags' in source:\n",
    "    print(\"‚úì Loaded IN MEMORY contains 'add_additional_tags'\")\n",
    "else:\n",
    "    print(\"‚úó Loaded IN MEMORY does NOT contain 'add_additional_tags'\")\n",
    "\n",
    "# Check for .pyc cache files\n",
    "import pathlib\n",
    "pyc_files = list(pathlib.Path(file_path).parent.glob('__pycache__/*.pyc'))\n",
    "print(f\"\\n.pyc cache files found: {len(pyc_files)}\")\n",
    "for pyc in pyc_files:\n",
    "    print(f\"  - {pyc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade73320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUCLEAR OPTION: Clear all Python cache\n",
    "import shutil\n",
    "import pathlib\n",
    "import align_test.alignment as align_module\n",
    "\n",
    "# Find the align_test directory\n",
    "align_test_dir = pathlib.Path(align_module.__file__).parent\n",
    "\n",
    "# Remove __pycache__ directory\n",
    "pycache_dir = align_test_dir / '__pycache__'\n",
    "if pycache_dir.exists():\n",
    "    print(f\"Removing cache: {pycache_dir}\")\n",
    "    shutil.rmtree(pycache_dir)\n",
    "    print(\"‚úì Cache removed\")\n",
    "\n",
    "# Also check parent __pycache__\n",
    "parent_pycache = align_test_dir.parent / '__pycache__'\n",
    "if parent_pycache.exists():\n",
    "    print(f\"Removing parent cache: {parent_pycache}\")\n",
    "    shutil.rmtree(parent_pycache)\n",
    "    print(\"‚úì Parent cache removed\")\n",
    "\n",
    "print(\"\\nüîÑ Now RESTART KERNEL and re-import!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
