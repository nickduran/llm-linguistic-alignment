{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Suite for Refactored `prepare_transcripts.py`\n",
    "\n",
    "This notebook tests the refactored preprocessing module with CHILDES sample data and verifies compatibility with alignment analysis scripts.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you've:\n",
    "1. Installed the package in editable mode: `pip install -e .`\n",
    "2. Installed spaCy: `pip install spacy` [NOTE: is this necessary?]\n",
    "3. Downloaded spaCy model: `python -m spacy download en_core_web_sm` [NOTE: is this necessary?]\n",
    "\n",
    "## Data Location\n",
    "\n",
    "Test files: `/Users/ndd697/Desktop/Github-Projects/llm-linguistic-alignment/src/align_test/data/CHILDES/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Import Libraries and Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Import the refactored preprocessing module\n",
    "from align_test.prepare_transcripts import prepare_transcripts\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION: Set your data directories\n",
    "# ============================================================\n",
    "\n",
    "# Input: CHILDES data directory\n",
    "CHILDES_DATA_DIR = '/Users/ndd697/Desktop/Github-Projects/llm-linguistic-alignment/src/align_test/data/CHILDES/'\n",
    "\n",
    "# Output: Test output directories\n",
    "OUTPUT_DIR_BASIC = './test_output_basic'\n",
    "OUTPUT_DIR_SPACY = './test_output_spacy'\n",
    "OUTPUT_DIR_STANFORD = './test_output_stanford'\n",
    "\n",
    "# Create output directories\n",
    "for dir_path in [OUTPUT_DIR_BASIC, OUTPUT_DIR_SPACY, OUTPUT_DIR_STANFORD]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"âœ“ Created directory: {dir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Verify Input Data\n",
    "\n",
    "Check that the CHILDES directory exists and contains our test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CHILDES directory exists\n",
    "print(f\"CHILDES Directory: {CHILDES_DATA_DIR}\")\n",
    "print(f\"Exists: {os.path.exists(CHILDES_DATA_DIR)}\")\n",
    "\n",
    "if not os.path.exists(CHILDES_DATA_DIR):\n",
    "    print(\"\\nâœ— Directory not found! Please update CHILDES_DATA_DIR above.\")\n",
    "else:\n",
    "    # List files in directory\n",
    "    files = [f for f in os.listdir(CHILDES_DATA_DIR) if f.endswith('.txt')]\n",
    "    print(f\"\\nâœ“ Found {len(files)} .txt files:\")\n",
    "    for f in files:\n",
    "        file_path = os.path.join(CHILDES_DATA_DIR, f)\n",
    "        size_kb = os.path.getsize(file_path) / 1024\n",
    "        print(f\"  - {f} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inspect Raw Input Files\n",
    "\n",
    "Let's look at the structure of the raw input files before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display a sample input file\n",
    "sample_file = os.path.join(CHILDES_DATA_DIR, 'time200-cond1.txt')\n",
    "\n",
    "print(f\"Reading: {os.path.basename(sample_file)}\\n\")\n",
    "\n",
    "raw_df = pd.read_csv(sample_file, sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(f\"Columns: {raw_df.columns.tolist()}\")\n",
    "print(f\"Rows: {len(raw_df)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample content\n",
    "print(\"Sample utterances:\\n\")\n",
    "for i in range(min(5, len(raw_df))):\n",
    "    print(f\"{raw_df['participant'].iloc[i]}: {raw_df['content'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TEST 1: Basic Preprocessing (NLTK Only)\n",
    "\n",
    "Test preprocessing with default NLTK POS tagger (fastest option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 1: Basic Preprocessing (NLTK only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run preprocessing with minimal options\n",
    "results_basic = prepare_transcripts(\n",
    "    input_files=CHILDES_DATA_DIR,\n",
    "    output_file_directory=OUTPUT_DIR_BASIC,\n",
    "    run_spell_check=False,  # Disable for faster testing\n",
    "    minwords=2,\n",
    "    add_additional_tags=False,  # NLTK only\n",
    "    input_as_directory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Preprocessing complete!\")\n",
    "print(f\"Total utterances processed: {len(results_basic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the output\n",
    "print(\"Output columns:\")\n",
    "for col in results_basic.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nDataFrame shape: {results_basic.shape}\")\n",
    "results_basic.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a single row in detail\n",
    "print(\"Sample processed utterance:\\n\")\n",
    "sample_row = results_basic.iloc[0]\n",
    "\n",
    "print(f\"Participant: {sample_row['participant']}\")\n",
    "print(f\"Content: {sample_row['content']}\")\n",
    "print(f\"\\nToken (string): {sample_row['token'][:100]}...\")\n",
    "print(f\"Type: {type(sample_row['token'])}\")\n",
    "\n",
    "# Parse and display\n",
    "tokens = ast.literal_eval(sample_row['token'])\n",
    "print(f\"\\nToken (parsed): {tokens}\")\n",
    "print(f\"Type after parsing: {type(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Output Format\n",
    "\n",
    "Check that the output format is compatible with alignment analysis scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one of the saved output files\n",
    "output_files = [f for f in os.listdir(OUTPUT_DIR_BASIC) \n",
    "                if f.endswith('.txt') and 'concatenated' not in f]\n",
    "\n",
    "print(f\"Output files created: {output_files}\")\n",
    "\n",
    "# Load the first file\n",
    "test_file_path = os.path.join(OUTPUT_DIR_BASIC, output_files[0])\n",
    "print(f\"\\nLoading: {output_files[0]}\")\n",
    "\n",
    "test_df = pd.read_csv(test_file_path, sep='\\t', encoding='utf-8')\n",
    "print(f\"Rows loaded: {len(test_df)}\")\n",
    "print(f\"Columns: {test_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Verify all columns are present\n",
    "required_cols = ['participant', 'content', 'token', 'lemma', 'tagged_token', 'tagged_lemma', 'file']\n",
    "\n",
    "print(\"Test 1: Required Columns\")\n",
    "print(\"-\" * 40)\n",
    "for col in required_cols:\n",
    "    present = col in test_df.columns\n",
    "    status = \"âœ“\" if present else \"âœ—\"\n",
    "    print(f\"{status} {col}\")\n",
    "\n",
    "all_present = all(col in test_df.columns for col in required_cols)\n",
    "print(f\"\\nResult: {'âœ“ PASSED' if all_present else 'âœ— FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Verify data types (should be strings)\n",
    "list_columns = ['token', 'lemma', 'tagged_token', 'tagged_lemma']\n",
    "\n",
    "print(\"Test 2: Data Types (should be strings)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "all_strings = True\n",
    "for col in list_columns:\n",
    "    if col in test_df.columns:\n",
    "        first_val = test_df[col].iloc[0]\n",
    "        is_string = isinstance(first_val, str)\n",
    "        status = \"âœ“\" if is_string else \"âœ—\"\n",
    "        print(f\"{status} {col}: {type(first_val).__name__}\")\n",
    "        if not is_string:\n",
    "            all_strings = False\n",
    "\n",
    "print(f\"\\nResult: {'âœ“ PASSED' if all_strings else 'âœ— FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Verify ast.literal_eval compatibility\n",
    "print(\"Test 3: ast.literal_eval Compatibility\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "all_parseable = True\n",
    "for col in list_columns:\n",
    "    if col in test_df.columns:\n",
    "        try:\n",
    "            parsed = ast.literal_eval(test_df[col].iloc[0])\n",
    "            print(f\"âœ“ {col}: Parses to {type(parsed).__name__}\")\n",
    "            print(f\"  Sample: {str(parsed)[:60]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— {col}: Parse failed - {e}\")\n",
    "            all_parseable = False\n",
    "\n",
    "print(f\"\\nResult: {'âœ“ PASSED' if all_parseable else 'âœ— FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Verify POS tag tuple format\n",
    "print(\"Test 4: POS Tag Format\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "correct_format = True\n",
    "for col in ['tagged_token', 'tagged_lemma']:\n",
    "    if col in test_df.columns:\n",
    "        try:\n",
    "            parsed = ast.literal_eval(test_df[col].iloc[0])\n",
    "            if parsed:\n",
    "                is_tuple = isinstance(parsed[0], tuple)\n",
    "                correct_length = len(parsed[0]) == 2 if is_tuple else False\n",
    "                \n",
    "                if is_tuple and correct_length:\n",
    "                    print(f\"âœ“ {col}: Correct format\")\n",
    "                    print(f\"  Sample: {parsed[0]} (word, POS)\")\n",
    "                else:\n",
    "                    print(f\"âœ— {col}: Incorrect format\")\n",
    "                    correct_format = False\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— {col}: Format check failed - {e}\")\n",
    "            correct_format = False\n",
    "\n",
    "print(f\"\\nResult: {'âœ“ PASSED' if correct_format else 'âœ— FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 1 SUMMARY: Basic Preprocessing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test1_passed = all_present and all_strings and all_parseable and correct_format\n",
    "\n",
    "if test1_passed:\n",
    "    print(\"\\nâœ“ TEST 1 PASSED: Basic preprocessing works correctly!\")\n",
    "    print(\"\\nOutput format is compatible with alignment analysis.\")\n",
    "else:\n",
    "    print(\"\\nâœ— TEST 1 FAILED: Some checks did not pass.\")\n",
    "    print(\"Please review the test results above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TEST 2: Preprocessing with spaCy\n",
    "\n",
    "Test preprocessing with spaCy POS tagger (100x faster than Stanford)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if spaCy is available\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"âœ“ spaCy is installed\")\n",
    "    print(\"Note: Model will be auto-downloaded by prepare_transcripts() if needed\")\n",
    "    spacy_available = True\n",
    "except ImportError:\n",
    "    print(\"âœ— spaCy not installed\")\n",
    "    print(\"Install with: pip install spacy\")\n",
    "    print(\"Will skip spaCy tests\")\n",
    "    spacy_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if spaCy is available\n",
    "if spacy_available:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEST 2: Preprocessing with spaCy\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run preprocessing with spaCy\n",
    "    results_spacy = prepare_transcripts(\n",
    "        input_files=CHILDES_DATA_DIR,\n",
    "        output_file_directory=OUTPUT_DIR_SPACY,\n",
    "        run_spell_check=False,\n",
    "        minwords=2,\n",
    "        add_additional_tags=True,\n",
    "        tagger_type='spacy',  # Use spaCy\n",
    "        input_as_directory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Preprocessing with spaCy complete!\")\n",
    "    print(f\"Total utterances processed: {len(results_spacy)}\")\n",
    "else:\n",
    "    print(\"\\nSkipping spaCy test (spaCy not available)\")\n",
    "    results_spacy = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 2 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine spaCy output (if available)\n",
    "if results_spacy is not None:\n",
    "    print(\"Output columns:\")\n",
    "    for col in results_spacy.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Check for spaCy-specific columns\n",
    "    has_spacy_cols = 'tagged_spacy_token' in results_spacy.columns and 'tagged_spacy_lemma' in results_spacy.columns\n",
    "    \n",
    "    if has_spacy_cols:\n",
    "        print(\"\\nâœ“ spaCy tagging columns present (tagged_spacy_token, tagged_spacy_lemma)\")\n",
    "        \n",
    "        # Show sample spaCy tags\n",
    "        sample_spacy_tag = ast.literal_eval(results_spacy['tagged_spacy_token'].iloc[0])\n",
    "        print(f\"\\nSample spaCy tags:\")\n",
    "        for i, (word, tag) in enumerate(sample_spacy_tag[:5]):\n",
    "            print(f\"  {i+1}. ('{word}', '{tag}')\")\n",
    "    else:\n",
    "        print(\"\\nâœ— spaCy tagging columns missing!\")\n",
    "    \n",
    "    results_spacy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 2 SUMMARY: spaCy Preprocessing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if results_spacy is not None:\n",
    "    test2_passed = has_spacy_cols and len(results_spacy) > 0\n",
    "    \n",
    "    if test2_passed:\n",
    "        print(\"\\nâœ“ TEST 2 PASSED: spaCy preprocessing works correctly!\")\n",
    "        print(\"\\nspaCy tags are being generated and stored properly.\")\n",
    "    else:\n",
    "        print(\"\\nâœ— TEST 2 FAILED: spaCy preprocessing had issues.\")\n",
    "else:\n",
    "    print(\"\\nâŠ˜ TEST 2 SKIPPED: spaCy not available\")\n",
    "    test2_passed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare NLTK tags vs spaCy tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NLTK tags vs spaCy tags for same utterance\n",
    "if results_spacy is not None:\n",
    "    print(\"Comparison: NLTK vs spaCy POS Tags\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sample_row = results_spacy.iloc[0]\n",
    "    \n",
    "    nltk_tags = ast.literal_eval(sample_row['tagged_token'])\n",
    "    spacy_tags = ast.literal_eval(sample_row['tagged_spacy_token'])\n",
    "    \n",
    "    print(f\"Utterance: {sample_row['content']}\\n\")\n",
    "    print(f\"{'Word':<15} {'NLTK Tag':<10} {'spaCy Tag':<10} {'Same?':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for (word_n, tag_n), (word_s, tag_s) in zip(nltk_tags, spacy_tags):\n",
    "        same = \"âœ“\" if tag_n == tag_s else \"âœ—\"\n",
    "        print(f\"{word_n:<15} {tag_n:<10} {tag_s:<10} {same:<10}\")\n",
    "    \n",
    "    # Calculate agreement\n",
    "    agreements = sum(1 for (_, t1), (_, t2) in zip(nltk_tags, spacy_tags) if t1 == t2)\n",
    "    total = len(nltk_tags)\n",
    "    agreement_pct = (agreements / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nAgreement: {agreements}/{total} ({agreement_pct:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NLTK tags vs spaCy tags across ALL utterances\n",
    "if results_spacy is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPREHENSIVE COMPARISON: NLTK vs spaCy POS Tags\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_agreements = 0\n",
    "    total_tokens = 0\n",
    "    per_utterance_agreements = []\n",
    "    \n",
    "    # Calculate agreement across all utterances\n",
    "    for idx in range(len(results_spacy)):\n",
    "        sample_row = results_spacy.iloc[idx]\n",
    "        \n",
    "        nltk_tags = ast.literal_eval(sample_row['tagged_token'])\n",
    "        spacy_tags = ast.literal_eval(sample_row['tagged_spacy_token'])\n",
    "        \n",
    "        if nltk_tags and spacy_tags and len(nltk_tags) == len(spacy_tags):\n",
    "            agreements = sum(1 for (_, t1), (_, t2) in zip(nltk_tags, spacy_tags) if t1 == t2)\n",
    "            total_agreements += agreements\n",
    "            total_tokens += len(nltk_tags)\n",
    "            \n",
    "            # Track per-utterance agreement\n",
    "            utterance_pct = (agreements / len(nltk_tags)) * 100\n",
    "            per_utterance_agreements.append(utterance_pct)\n",
    "    \n",
    "    # Overall statistics\n",
    "    overall_agreement_pct = (total_agreements / total_tokens * 100) if total_tokens > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š OVERALL STATISTICS:\")\n",
    "    print(f\"   Total tokens compared: {total_tokens}\")\n",
    "    print(f\"   Agreements: {total_agreements}\")\n",
    "    print(f\"   Disagreements: {total_tokens - total_agreements}\")\n",
    "    print(f\"   Overall Agreement: {overall_agreement_pct:.1f}%\")\n",
    "    \n",
    "    if per_utterance_agreements:\n",
    "        import numpy as np\n",
    "        print(f\"\\n   Per-utterance agreement:\")\n",
    "        print(f\"      Mean: {np.mean(per_utterance_agreements):.1f}%\")\n",
    "        print(f\"      Median: {np.median(per_utterance_agreements):.1f}%\")\n",
    "        print(f\"      Min: {np.min(per_utterance_agreements):.1f}%\")\n",
    "        print(f\"      Max: {np.max(per_utterance_agreements):.1f}%\")\n",
    "    \n",
    "    # Show detailed examples\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED EXAMPLES (First 3 utterances with >5 words)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    examples_shown = 0\n",
    "    for idx in range(len(results_spacy)):\n",
    "        if examples_shown >= 3:\n",
    "            break\n",
    "            \n",
    "        sample_row = results_spacy.iloc[idx]\n",
    "        nltk_tags = ast.literal_eval(sample_row['tagged_token'])\n",
    "        spacy_tags = ast.literal_eval(sample_row['tagged_spacy_token'])\n",
    "        \n",
    "        if len(nltk_tags) > 5 and len(spacy_tags) > 5:\n",
    "            examples_shown += 1\n",
    "            \n",
    "            print(f\"\\n--- Example {examples_shown} ---\")\n",
    "            print(f\"Source: {sample_row.get('file', 'unknown')}\")\n",
    "            print(f\"Participant: {sample_row.get('participant', 'unknown')}\")\n",
    "            print(f\"Utterance: {sample_row['content']}\\n\")\n",
    "            print(f\"{'Word':<15} {'NLTK':<10} {'spaCy':<10} {'Match':<8}\")\n",
    "            print(\"-\" * 48)\n",
    "            \n",
    "            agreements = 0\n",
    "            disagreements = []\n",
    "            \n",
    "            for (word_n, tag_n), (word_s, tag_s) in zip(nltk_tags, spacy_tags):\n",
    "                match = \"âœ“\" if tag_n == tag_s else \"âœ—\"\n",
    "                print(f\"{word_n:<15} {tag_n:<10} {tag_s:<10} {match:<8}\")\n",
    "                \n",
    "                if tag_n == tag_s:\n",
    "                    agreements += 1\n",
    "                else:\n",
    "                    disagreements.append((word_n, tag_n, tag_s))\n",
    "            \n",
    "            total = len(nltk_tags)\n",
    "            print(f\"\\nAgreement: {agreements}/{total} ({100*agreements/total:.1f}%)\")\n",
    "            \n",
    "            if disagreements:\n",
    "                print(f\"Disagreements: {len(disagreements)}\")\n",
    "                for word, nltk_tag, spacy_tag in disagreements[:3]:  # Show first 3\n",
    "                    print(f\"  â€¢ '{word}': NLTK={nltk_tag}, spaCy={spacy_tag}\")\n",
    "    \n",
    "    # Identify most common disagreements\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MOST COMMON TAG DISAGREEMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    disagreement_counts = {}\n",
    "    for idx in range(len(results_spacy)):\n",
    "        sample_row = results_spacy.iloc[idx]\n",
    "        nltk_tags = ast.literal_eval(sample_row['tagged_token'])\n",
    "        spacy_tags = ast.literal_eval(sample_row['tagged_spacy_token'])\n",
    "        \n",
    "        if nltk_tags and spacy_tags:\n",
    "            for (word, t1), (_, t2) in zip(nltk_tags, spacy_tags):\n",
    "                if t1 != t2:\n",
    "                    key = f\"NLTK:{t1} vs spaCy:{t2}\"\n",
    "                    if key not in disagreement_counts:\n",
    "                        disagreement_counts[key] = []\n",
    "                    disagreement_counts[key].append(word)\n",
    "    \n",
    "    # Show top 10 disagreements\n",
    "    if disagreement_counts:\n",
    "        sorted_disagreements = sorted(disagreement_counts.items(), \n",
    "                                     key=lambda x: len(x[1]), \n",
    "                                     reverse=True)\n",
    "        \n",
    "        print(\"\\nTop 10 tag disagreement patterns:\")\n",
    "        for i, (pattern, words) in enumerate(sorted_disagreements[:10], 1):\n",
    "            example_words = ', '.join(list(set(words))[:3])  # Show up to 3 unique examples\n",
    "            print(f\"{i:2}. {pattern:<30} (n={len(words):3}) Examples: {example_words}\")\n",
    "    else:\n",
    "        print(\"\\nâœ“ Perfect agreement! No disagreements found.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TEST 3: Preprocessing with Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Step 1: Check if Java is installed\n",
    "print(\"\\n1. Checking Java installation...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "java_available = False\n",
    "try:\n",
    "    result = subprocess.run(['java', '-version'], \n",
    "                          capture_output=True, \n",
    "                          text=True, \n",
    "                          timeout=5)\n",
    "    \n",
    "    # Check both stderr and stdout for Java version info\n",
    "    output = result.stderr + result.stdout\n",
    "    \n",
    "    # Java typically outputs to stderr, and should contain \"version\"\n",
    "    # Check return code AND output content\n",
    "    if result.returncode == 0 and ('version' in output.lower() or 'openjdk' in output.lower()):\n",
    "        # Extract version line (usually first line)\n",
    "        version_lines = [line for line in output.split('\\n') if line.strip()]\n",
    "        if version_lines:\n",
    "            java_version = version_lines[0]\n",
    "            # Double-check it's not an error message\n",
    "            if 'unable to locate' not in java_version.lower() and 'not found' not in java_version.lower():\n",
    "                print(f\"âœ“ Java is installed: {java_version}\")\n",
    "                java_available = True\n",
    "            else:\n",
    "                print(\"âœ— Java not found\")\n",
    "                print(f\"  Error: {java_version}\")\n",
    "    else:\n",
    "        print(\"âœ— Java not found or not working properly\")\n",
    "        if output.strip():\n",
    "            print(f\"  Output: {output.strip()[:100]}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"âœ— Java command not found\")\n",
    "    print(\"  Java is not installed or not in PATH\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"âœ— Java check timed out\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error checking Java: {e}\")\n",
    "\n",
    "if not java_available:\n",
    "    print(\"\\n  Stanford POS Tagger requires Java to run\")\n",
    "    print(\"  Install Java from:\")\n",
    "    print(\"    - macOS: https://www.java.com/en/download/\")\n",
    "    print(\"    - macOS (alternative): brew install openjdk\")\n",
    "    print(\"    - Linux: sudo apt-get install default-jdk\")\n",
    "    print(\"    - Windows: https://www.java.com/en/download/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Check for Stanford POS Tagger files\n",
    "print(\"\\n2. Checking Stanford POS Tagger files...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "stanford_available = False\n",
    "stanford_pos_path = None\n",
    "stanford_language_path = None\n",
    "\n",
    "if java_available:\n",
    "    # Common locations where users might have Stanford tagger\n",
    "    common_locations = [\n",
    "        os.path.expanduser(\"~/stanford-postagger\"),\n",
    "        os.path.expanduser(\"~/Downloads/stanford-postagger-full-2020-11-17\"),\n",
    "        \"/usr/local/stanford-postagger\",\n",
    "        \"./stanford-postagger\",\n",
    "    ]\n",
    "    \n",
    "    # Check if any common location exists\n",
    "    for location in common_locations:\n",
    "        if os.path.exists(location):\n",
    "            # Check for required files\n",
    "            jar_path = os.path.join(location, \"stanford-postagger.jar\")\n",
    "            model_path = os.path.join(location, \"models/english-left3words-distsim.tagger\")\n",
    "            \n",
    "            if os.path.exists(jar_path) and os.path.exists(model_path):\n",
    "                stanford_pos_path = location\n",
    "                stanford_language_path = \"models/english-left3words-distsim.tagger\"\n",
    "                stanford_available = True\n",
    "                print(f\"âœ“ Found Stanford tagger at: {location}\")\n",
    "                print(f\"  JAR: {jar_path}\")\n",
    "                print(f\"  Model: {model_path}\")\n",
    "                break\n",
    "    \n",
    "    if not stanford_available:\n",
    "        print(\"âœ— Stanford POS Tagger not found in common locations\")\n",
    "        print(\"\\nTo use Stanford tagger:\")\n",
    "        print(\"  1. Download from: https://nlp.stanford.edu/software/tagger.shtml#Download\")\n",
    "        print(\"  2. Extract to a known location\")\n",
    "        print(\"  3. Update the paths below\")\n",
    "        print(\"\\nCommon locations checked:\")\n",
    "        for loc in common_locations:\n",
    "            print(f\"  - {loc}\")\n",
    "else:\n",
    "    print(\"âŠ˜ Skipping (Java not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Manual path configuration (if not auto-detected)\n",
    "print(\"\\n3. Path Configuration\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if java_available and not stanford_available:\n",
    "    print(\"\\nâš ï¸  Stanford tagger not auto-detected.\")\n",
    "    print(\"If you have Stanford tagger installed, specify paths below:\")\n",
    "    print(\"\\nExample paths:\")\n",
    "    print(\"  stanford_pos_path = '/Users/yourname/stanford-postagger-full-2020-11-17'\")\n",
    "    print(\"  stanford_language_path = 'models/english-left3words-distsim.tagger'\")\n",
    "    \n",
    "    # Uncomment and update these lines if you have Stanford tagger installed:\n",
    "    # stanford_pos_path = \"/path/to/your/stanford-postagger\"\n",
    "    # stanford_language_path = \"models/english-left3words-distsim.tagger\"\n",
    "    # stanford_available = True\n",
    "    \n",
    "    if stanford_pos_path and stanford_language_path:\n",
    "        # Validate the paths\n",
    "        jar_path = os.path.join(stanford_pos_path, \"stanford-postagger.jar\")\n",
    "        model_path = os.path.join(stanford_pos_path, stanford_language_path)\n",
    "        \n",
    "        if os.path.exists(jar_path) and os.path.exists(model_path):\n",
    "            stanford_available = True\n",
    "            print(f\"âœ“ Manual configuration successful\")\n",
    "        else:\n",
    "            print(f\"âœ— Invalid paths:\")\n",
    "            if not os.path.exists(jar_path):\n",
    "                print(f\"  JAR not found: {jar_path}\")\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"  Model not found: {model_path}\")\n",
    "            stanford_available = False\n",
    "elif stanford_available:\n",
    "    print(f\"âœ“ Using auto-detected paths:\")\n",
    "    print(f\"  Base: {stanford_pos_path}\")\n",
    "    print(f\"  Model: {stanford_language_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MANUALLY ADD PATHS HERE IF NEEDED ##\n",
    "# Example manual configuration (uncomment and set your paths)\n",
    "stanford_pos_path = '/Users/ndd697/Desktop/Github-Projects/llm-linguistic-alignment/sandbox-prepare/stanford-postagger-full-2020-11-17'\n",
    "stanford_language_path = '/Users/ndd697/Desktop/Github-Projects/llm-linguistic-alignment/sandbox-prepare/stanford-postagger-full-2020-11-17/models/english-left3words-distsim.tagger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Run Stanford tagging (if available)\n",
    "print(\"\\n4. Test Results\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# RE-VALIDATE: Check if paths were manually set after Step 3\n",
    "if not stanford_available and 'stanford_pos_path' in locals() and 'stanford_language_path' in locals():\n",
    "    if stanford_pos_path is not None and stanford_language_path is not None:\n",
    "        print(\"\\nðŸ”„ Detected manually configured paths. Validating...\")\n",
    "        \n",
    "        # Normalize paths\n",
    "        stanford_pos_path = os.path.normpath(os.path.expanduser(stanford_pos_path))\n",
    "        stanford_language_path = os.path.normpath(stanford_language_path)\n",
    "        \n",
    "        # Check if stanford_language_path is absolute or relative\n",
    "        if os.path.isabs(stanford_language_path):\n",
    "            # It's an absolute path, use it directly\n",
    "            model_path = stanford_language_path\n",
    "        else:\n",
    "            # It's relative to stanford_pos_path\n",
    "            model_path = os.path.join(stanford_pos_path, stanford_language_path)\n",
    "        \n",
    "        jar_path = os.path.join(stanford_pos_path, \"stanford-postagger.jar\")\n",
    "        \n",
    "        print(f\"  Checking JAR: {jar_path}\")\n",
    "        print(f\"  Checking Model: {model_path}\")\n",
    "        \n",
    "        if os.path.exists(jar_path) and os.path.exists(model_path):\n",
    "            stanford_available = True\n",
    "            print(f\"  âœ“ Manual configuration validated!\")\n",
    "            print(f\"  âœ“ Found JAR: {os.path.basename(jar_path)}\")\n",
    "            print(f\"  âœ“ Found Model: {os.path.basename(model_path)}\")\n",
    "            \n",
    "            # Update stanford_language_path to be relative if it was given as absolute\n",
    "            if os.path.isabs(stanford_language_path):\n",
    "                # Convert to relative path from stanford_pos_path\n",
    "                try:\n",
    "                    stanford_language_path = os.path.relpath(model_path, stanford_pos_path)\n",
    "                    print(f\"  â„¹ï¸  Converted to relative path: {stanford_language_path}\")\n",
    "                except ValueError:\n",
    "                    # Can't make relative (e.g., different drives on Windows)\n",
    "                    print(f\"  â„¹ï¸  Using absolute model path\")\n",
    "        else:\n",
    "            print(f\"  âœ— Validation failed:\")\n",
    "            if not os.path.exists(jar_path):\n",
    "                print(f\"    - JAR not found: {jar_path}\")\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"    - Model not found: {model_path}\")\n",
    "            print(f\"\\n  ðŸ’¡ Tips:\")\n",
    "            print(f\"    - stanford_pos_path should point to the Stanford tagger directory\")\n",
    "            print(f\"    - stanford_language_path can be either:\")\n",
    "            print(f\"      â€¢ Relative: 'models/english-left3words-distsim.tagger'\")\n",
    "            print(f\"      â€¢ Absolute: '/full/path/to/models/english-left3words-distsim.tagger'\")\n",
    "\n",
    "# Now proceed with the test\n",
    "if not java_available:\n",
    "    print(\"\\nâŠ˜ TEST 3 SKIPPED: Java not installed\")\n",
    "    print(\"Stanford tagger requires Java to run.\")\n",
    "    stanford_test_passed = None\n",
    "    \n",
    "elif not stanford_available:\n",
    "    print(\"\\nâŠ˜ TEST 3 SKIPPED: Stanford tagger not configured\")\n",
    "    print(\"Stanford tagger files not found or paths not specified.\")\n",
    "    print(\"\\nðŸ’¡ To configure manually:\")\n",
    "    print(\"   1. Make sure Java is installed\")\n",
    "    print(\"   2. Download Stanford POS Tagger from:\")\n",
    "    print(\"      https://nlp.stanford.edu/software/tagger.shtml#Download\")\n",
    "    print(\"   3. In Step 3 above, set:\")\n",
    "    print(\"      stanford_pos_path = '/path/to/stanford-postagger-full-2020-11-17'\")\n",
    "    print(\"      stanford_language_path = 'models/english-left3words-distsim.tagger'\")\n",
    "    print(\"   4. Re-run this cell (Step 4)\")\n",
    "    stanford_test_passed = None\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâœ“ All prerequisites met. Running Stanford tagger test...\")\n",
    "    print(f\"\\nThis may take several minutes (Stanford tagger is ~100x slower than spaCy)\")\n",
    "    print(f\"Processing {len([f for f in os.listdir(CHILDES_DATA_DIR) if f.endswith('.txt')])} files...\")\n",
    "        \n",
    "    try:\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results_stanford = prepare_transcripts(\n",
    "            input_files=CHILDES_DATA_DIR,\n",
    "            output_file_directory=OUTPUT_DIR_STANFORD,\n",
    "            run_spell_check=False,\n",
    "            minwords=2,\n",
    "            add_additional_tags=True,\n",
    "            tagger_type='stanford',  # Use Stanford\n",
    "            stanford_pos_path=stanford_pos_path,\n",
    "            stanford_language_path=stanford_language_path,\n",
    "            stanford_batch_size=50,  # Process in batches for better performance\n",
    "            input_as_directory=True\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"\\nâœ“ Stanford preprocessing complete!\")\n",
    "        print(f\"  Time taken: {processing_time:.1f} seconds ({processing_time/60:.1f} minutes)\")\n",
    "        print(f\"  Total utterances processed: {len(results_stanford)}\")\n",
    "        \n",
    "        # Check if Stanford tags were actually created\n",
    "        sample_stanford_tags = ast.literal_eval(results_stanford['tagged_stan_token'].iloc[0])\n",
    "        if sample_stanford_tags:\n",
    "            print(f\"  âœ“ Stanford tags successfully generated\")\n",
    "            stanford_test_passed = True\n",
    "        else:\n",
    "            print(f\"  âœ— Stanford tags are empty\")\n",
    "            stanford_test_passed = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Stanford preprocessing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        stanford_test_passed = False\n",
    "        results_stanford = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare NLTK tags vs Stanford Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARISON: NLTK vs Stanford POS Tags\n",
    "# ============================================================\n",
    "if results_stanford is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPREHENSIVE COMPARISON: NLTK vs Stanford POS Tags\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_agreements = 0\n",
    "    total_tokens = 0\n",
    "    per_utterance_agreements = []\n",
    "    \n",
    "    # Calculate agreement across all utterances\n",
    "    for idx in range(len(results_stanford)):\n",
    "        sample_row = results_stanford.iloc[idx]\n",
    "        \n",
    "        nltk_tags = ast.literal_eval(sample_row['tagged_token'])\n",
    "        stanford_tags = ast.literal_eval(sample_row['tagged_stan_token'])\n",
    "        \n",
    "        if nltk_tags and stanford_tags and len(nltk_tags) == len(stanford_tags):\n",
    "            agreements = sum(1 for (_, t1), (_, t2) in zip(nltk_tags, stanford_tags) if t1 == t2)\n",
    "            total_agreements += agreements\n",
    "            total_tokens += len(nltk_tags)\n",
    "            \n",
    "            # Track per-utterance agreement\n",
    "            utterance_pct = (agreements / len(nltk_tags)) * 100\n",
    "            per_utterance_agreements.append(utterance_pct)\n",
    "    \n",
    "    # Overall statistics\n",
    "    overall_agreement_pct = (total_agreements / total_tokens * 100) if total_tokens > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š OVERALL STATISTICS:\")\n",
    "    print(f\"   Total tokens compared: {total_tokens}\")\n",
    "    print(f\"   Agreements: {total_agreements}\")\n",
    "    print(f\"   Disagreements: {total_tokens - total_agreements}\")\n",
    "    print(f\"   Overall Agreement: {overall_agreement_pct:.1f}%\")\n",
    "    \n",
    "    if per_utterance_agreements:\n",
    "        import numpy as np\n",
    "        print(f\"\\n   Per-utterance agreement:\")\n",
    "        print(f\"      Mean: {np.mean(per_utterance_agreements):.1f}%\")\n",
    "        print(f\"      Median: {np.median(per_utterance_agreements):.1f}%\")\n",
    "        print(f\"      Min: {np.min(per_utterance_agreements):.1f}%\")\n",
    "        print(f\"      Max: {np.max(per_utterance_agreements):.1f}%\")\n",
    "    \n",
    "    # Show detailed examples\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED EXAMPLES (First 3 utterances with >5 words)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    examples_shown = 0\n",
    "    for idx in range(len(results_stanford)):\n",
    "        if examples_shown >= 3:\n",
    "            break\n",
    "            \n",
    "        sample_row = results_stanford.iloc[idx]\n",
    "        nltk_tags = ast.literal_eval(sample_row['tagged_token'])\n",
    "        stanford_tags = ast.literal_eval(sample_row['tagged_stan_token'])\n",
    "        \n",
    "        if len(nltk_tags) > 5 and len(stanford_tags) > 5:\n",
    "            examples_shown += 1\n",
    "            \n",
    "            print(f\"\\n--- Example {examples_shown} ---\")\n",
    "            print(f\"Source: {sample_row.get('file', 'unknown')}\")\n",
    "            print(f\"Participant: {sample_row.get('participant', 'unknown')}\")\n",
    "            print(f\"Utterance: {sample_row['content']}\\n\")\n",
    "            print(f\"{'Word':<15} {'NLTK':<10} {'Stanford':<10} {'Match':<8}\")\n",
    "            print(\"-\" * 48)\n",
    "            \n",
    "            agreements = 0\n",
    "            disagreements = []\n",
    "            \n",
    "            for (word_n, tag_n), (word_s, tag_s) in zip(nltk_tags, stanford_tags):\n",
    "                match = \"âœ“\" if tag_n == tag_s else \"âœ—\"\n",
    "                print(f\"{word_n:<15} {tag_n:<10} {tag_s:<10} {match:<8}\")\n",
    "                \n",
    "                if tag_n == tag_s:\n",
    "                    agreements += 1\n",
    "                else:\n",
    "                    disagreements.append((word_n, tag_n, tag_s))\n",
    "            \n",
    "            total = len(nltk_tags)\n",
    "            print(f\"\\nAgreement: {agreements}/{total} ({100*agreements/total:.1f}%)\")\n",
    "            \n",
    "            if disagreements:\n",
    "                print(f\"Disagreements: {len(disagreements)}\")\n",
    "                for word, nltk_tag, stanford_tag in disagreements[:3]:\n",
    "                    print(f\"  â€¢ '{word}': NLTK={nltk_tag}, Stanford={stanford_tag}\")\n",
    "    \n",
    "    # Identify most common disagreements\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MOST COMMON TAG DISAGREEMENTS (NLTK vs Stanford)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    disagreement_counts = {}\n",
    "    for idx in range(len(results_stanford)):\n",
    "        sample_row = results_stanford.iloc[idx]\n",
    "        nltk_tags = ast.literal_eval(sample_row['tagged_token'])\n",
    "        stanford_tags = ast.literal_eval(sample_row['tagged_stan_token'])\n",
    "        \n",
    "        if nltk_tags and stanford_tags:\n",
    "            for (word, t1), (_, t2) in zip(nltk_tags, stanford_tags):\n",
    "                if t1 != t2:\n",
    "                    key = f\"NLTK:{t1} vs Stanford:{t2}\"\n",
    "                    if key not in disagreement_counts:\n",
    "                        disagreement_counts[key] = []\n",
    "                    disagreement_counts[key].append(word)\n",
    "    \n",
    "    # Show top 10 disagreements\n",
    "    if disagreement_counts:\n",
    "        sorted_disagreements = sorted(disagreement_counts.items(), \n",
    "                                     key=lambda x: len(x[1]), \n",
    "                                     reverse=True)\n",
    "        \n",
    "        print(\"\\nTop 10 tag disagreement patterns:\")\n",
    "        for i, (pattern, words) in enumerate(sorted_disagreements[:10], 1):\n",
    "            example_words = ', '.join(list(set(words))[:3])\n",
    "            print(f\"{i:2}. {pattern:<35} (n={len(words):3}) Examples: {example_words}\")\n",
    "    else:\n",
    "        print(\"\\nâœ“ Perfect agreement! No disagreements found.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare spaCy tags vs Stanford tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARISON: spaCy vs Stanford POS Tags\n",
    "# ============================================================\n",
    "if results_spacy is not None and results_stanford is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPREHENSIVE COMPARISON: spaCy vs Stanford POS Tags\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # First, verify we're comparing the same files\n",
    "    spacy_files = set(results_spacy['file'].unique())\n",
    "    stanford_files = set(results_stanford['file'].unique())\n",
    "    common_files = spacy_files & stanford_files\n",
    "    \n",
    "    print(f\"\\nFiles in spaCy results: {len(spacy_files)}\")\n",
    "    print(f\"Files in Stanford results: {len(stanford_files)}\")\n",
    "    print(f\"Files in common: {len(common_files)}\")\n",
    "    \n",
    "    if not common_files:\n",
    "        print(\"\\nâœ— No common files found! Cannot compare.\")\n",
    "    else:\n",
    "        total_agreements = 0\n",
    "        total_tokens = 0\n",
    "        per_utterance_agreements = []\n",
    "        \n",
    "        # Process each file that appears in both results\n",
    "        for file_name in sorted(common_files):\n",
    "            spacy_data = results_spacy[results_spacy['file'] == file_name].reset_index(drop=True)\n",
    "            stanford_data = results_stanford[results_stanford['file'] == file_name].reset_index(drop=True)\n",
    "            \n",
    "            # Compare row by row\n",
    "            for idx in range(min(len(spacy_data), len(stanford_data))):\n",
    "                spacy_tags = ast.literal_eval(spacy_data.iloc[idx]['tagged_spacy_token'])\n",
    "                stanford_tags = ast.literal_eval(stanford_data.iloc[idx]['tagged_stan_token'])\n",
    "                \n",
    "                if spacy_tags and stanford_tags and len(spacy_tags) == len(stanford_tags):\n",
    "                    agreements = sum(1 for (_, t1), (_, t2) in zip(spacy_tags, stanford_tags) if t1 == t2)\n",
    "                    total_agreements += agreements\n",
    "                    total_tokens += len(spacy_tags)\n",
    "                    \n",
    "                    # Track per-utterance agreement\n",
    "                    utterance_pct = (agreements / len(spacy_tags)) * 100\n",
    "                    per_utterance_agreements.append(utterance_pct)\n",
    "        \n",
    "        # Overall statistics\n",
    "        overall_agreement_pct = (total_agreements / total_tokens * 100) if total_tokens > 0 else 0\n",
    "        \n",
    "        print(f\"\\nðŸ“Š OVERALL STATISTICS:\")\n",
    "        print(f\"   Total tokens compared: {total_tokens}\")\n",
    "        print(f\"   Agreements: {total_agreements}\")\n",
    "        print(f\"   Disagreements: {total_tokens - total_agreements}\")\n",
    "        print(f\"   Overall Agreement: {overall_agreement_pct:.1f}%\")\n",
    "        \n",
    "        if per_utterance_agreements:\n",
    "            import numpy as np\n",
    "            print(f\"\\n   Per-utterance agreement:\")\n",
    "            print(f\"      Mean: {np.mean(per_utterance_agreements):.1f}%\")\n",
    "            print(f\"      Median: {np.median(per_utterance_agreements):.1f}%\")\n",
    "            print(f\"      Min: {np.min(per_utterance_agreements):.1f}%\")\n",
    "            print(f\"      Max: {np.max(per_utterance_agreements):.1f}%\")\n",
    "        \n",
    "        # Show detailed examples\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DETAILED EXAMPLES (First 3 utterances with >5 words)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        examples_shown = 0\n",
    "        for file_name in sorted(common_files):\n",
    "            if examples_shown >= 3:\n",
    "                break\n",
    "                \n",
    "            spacy_data = results_spacy[results_spacy['file'] == file_name].reset_index(drop=True)\n",
    "            stanford_data = results_stanford[results_stanford['file'] == file_name].reset_index(drop=True)\n",
    "            \n",
    "            for idx in range(min(len(spacy_data), len(stanford_data))):\n",
    "                if examples_shown >= 3:\n",
    "                    break\n",
    "                    \n",
    "                spacy_row = spacy_data.iloc[idx]\n",
    "                stanford_row = stanford_data.iloc[idx]\n",
    "                \n",
    "                spacy_tags = ast.literal_eval(spacy_row['tagged_spacy_token'])\n",
    "                stanford_tags = ast.literal_eval(stanford_row['tagged_stan_token'])\n",
    "                \n",
    "                if len(spacy_tags) > 5 and len(stanford_tags) > 5:\n",
    "                    examples_shown += 1\n",
    "                    \n",
    "                    print(f\"\\n--- Example {examples_shown} ---\")\n",
    "                    print(f\"Source: {spacy_row.get('file', 'unknown')}\")\n",
    "                    print(f\"Participant: {spacy_row.get('participant', 'unknown')}\")\n",
    "                    print(f\"Utterance: {spacy_row['content']}\\n\")\n",
    "                    print(f\"{'Word':<15} {'spaCy':<10} {'Stanford':<10} {'Match':<8}\")\n",
    "                    print(\"-\" * 48)\n",
    "                    \n",
    "                    agreements = 0\n",
    "                    disagreements = []\n",
    "                    \n",
    "                    for (word_s, tag_s), (word_st, tag_st) in zip(spacy_tags, stanford_tags):\n",
    "                        match = \"âœ“\" if tag_s == tag_st else \"âœ—\"\n",
    "                        print(f\"{word_s:<15} {tag_s:<10} {tag_st:<10} {match:<8}\")\n",
    "                        \n",
    "                        if tag_s == tag_st:\n",
    "                            agreements += 1\n",
    "                        else:\n",
    "                            disagreements.append((word_s, tag_s, tag_st))\n",
    "                    \n",
    "                    total = len(spacy_tags)\n",
    "                    print(f\"\\nAgreement: {agreements}/{total} ({100*agreements/total:.1f}%)\")\n",
    "                    \n",
    "                    if disagreements:\n",
    "                        print(f\"Disagreements: {len(disagreements)}\")\n",
    "                        for word, spacy_tag, stanford_tag in disagreements[:3]:\n",
    "                            print(f\"  â€¢ '{word}': spaCy={spacy_tag}, Stanford={stanford_tag}\")\n",
    "        \n",
    "        # Identify most common disagreements\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MOST COMMON TAG DISAGREEMENTS (spaCy vs Stanford)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        disagreement_counts = {}\n",
    "        for file_name in common_files:\n",
    "            spacy_data = results_spacy[results_spacy['file'] == file_name].reset_index(drop=True)\n",
    "            stanford_data = results_stanford[results_stanford['file'] == file_name].reset_index(drop=True)\n",
    "            \n",
    "            for idx in range(min(len(spacy_data), len(stanford_data))):\n",
    "                spacy_tags = ast.literal_eval(spacy_data.iloc[idx]['tagged_spacy_token'])\n",
    "                stanford_tags = ast.literal_eval(stanford_data.iloc[idx]['tagged_stan_token'])\n",
    "                \n",
    "                if spacy_tags and stanford_tags:\n",
    "                    for (word, t1), (_, t2) in zip(spacy_tags, stanford_tags):\n",
    "                        if t1 != t2:\n",
    "                            key = f\"spaCy:{t1} vs Stanford:{t2}\"\n",
    "                            if key not in disagreement_counts:\n",
    "                                disagreement_counts[key] = []\n",
    "                            disagreement_counts[key].append(word)\n",
    "        \n",
    "        # Show top 10 disagreements\n",
    "        if disagreement_counts:\n",
    "            sorted_disagreements = sorted(disagreement_counts.items(), \n",
    "                                         key=lambda x: len(x[1]), \n",
    "                                         reverse=True)\n",
    "            \n",
    "            print(\"\\nTop 10 tag disagreement patterns:\")\n",
    "            for i, (pattern, words) in enumerate(sorted_disagreements[:10], 1):\n",
    "                example_words = ', '.join(list(set(words))[:3])\n",
    "                print(f\"{i:2}. {pattern:<35} (n={len(words):3}) Examples: {example_words}\")\n",
    "        else:\n",
    "            print(\"\\nâœ“ Perfect agreement! No disagreements found.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TEST 3: Integration with Alignment Analysis\n",
    "\n",
    "Test that preprocessed files work correctly with the alignment analysis scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 3: Integration with Alignment Analysis\n",
      "============================================================\n",
      "\n",
      "Step 1: Initializing alignment analyzer...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ndd697/Desktop/Github-Projects/llm-linguistic-alignment/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malign_test\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malignment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinguisticAlignment\n\u001b[32m     11\u001b[39m analyzer = LinguisticAlignment(\n\u001b[32m     12\u001b[39m     alignment_type=\u001b[33m\"\u001b[39m\u001b[33mlexsyn\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Using lexical-syntactic alignment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     cache_dir=\u001b[43mos\u001b[49m.path.join(OUTPUT_DIR_ALIGNMENT, \u001b[33m\"\u001b[39m\u001b[33mcache\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Analyzer initialized (LexicalSyntacticAlignment)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 3: Integration with Alignment Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Import and initialize the alignment analyzer\n",
    "print(\"\\nStep 1: Initializing alignment analyzer...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from align_test.alignment import LinguisticAlignment\n",
    "\n",
    "analyzer = LinguisticAlignment(\n",
    "    alignment_type=\"lexsyn\",  # Using lexical-syntactic alignment\n",
    "    cache_dir=os.path.join(OUTPUT_DIR_ALIGNMENT, \"cache\")\n",
    ")\n",
    "\n",
    "print(\"âœ“ Analyzer initialized (LexicalSyntacticAlignment)\")\n",
    "\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize alignment analyzer\n",
    "print(\"Initializing alignment analyzer...\")\n",
    "\n",
    "analyzer = LinguisticAlignment(\n",
    "    alignment_type=\"lexsyn\",\n",
    "    cache_dir=os.path.join(OUTPUT_DIR_ALIGNMENT, \"cache\")\n",
    ")\n",
    "\n",
    "print(\"âœ“ Analyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run alignment analysis on preprocessed data\n",
    "print(\"\\nRunning alignment analysis...\")\n",
    "print(f\"Input folder: {OUTPUT_DIR_BASIC}\")\n",
    "print(f\"Output folder: {OUTPUT_DIR_ALIGNMENT}\")\n",
    "\n",
    "alignment_results = analyzer.analyze_folder(\n",
    "    folder_path=OUTPUT_DIR_BASIC,\n",
    "    output_directory=OUTPUT_DIR_ALIGNMENT,\n",
    "    lag=1,\n",
    "    max_ngram=2,\n",
    "    ignore_duplicates=True,\n",
    "    add_stanford_tags=False  # Using NLTK-only preprocessed data\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Alignment analysis complete!\")\n",
    "print(f\"Utterance pairs analyzed: {len(alignment_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine alignment results\n",
    "print(\"Alignment Results:\")\n",
    "print(f\"Shape: {alignment_results.shape}\")\n",
    "print(f\"\\nColumns: {alignment_results.columns.tolist()}\")\n",
    "\n",
    "alignment_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for expected alignment metrics\n",
    "expected_metrics = [\n",
    "    'lexical_tok1_cosine',\n",
    "    'lexical_lem1_cosine', \n",
    "    'pos_tok1_cosine',\n",
    "    'pos_lem1_cosine',\n",
    "    'lexical_master_cosine',\n",
    "    'syntactic_master_cosine'\n",
    "]\n",
    "\n",
    "print(\"Expected Alignment Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "found_metrics = []\n",
    "for metric in expected_metrics:\n",
    "    present = metric in alignment_results.columns\n",
    "    status = \"âœ“\" if present else \"âœ—\"\n",
    "    print(f\"{status} {metric}\")\n",
    "    if present:\n",
    "        found_metrics.append(metric)\n",
    "\n",
    "print(f\"\\nFound {len(found_metrics)}/{len(expected_metrics)} expected metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample alignment scores\n",
    "if found_metrics:\n",
    "    print(\"\\nSample Alignment Scores:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sample = alignment_results.iloc[0]\n",
    "    \n",
    "    print(f\"Source: {sample['source_file']}\")\n",
    "    print(f\"Participant: {sample['participant']}\")\n",
    "    print(f\"Content: {sample['content']}\")\n",
    "    print(f\"\\nAlignment Scores:\")\n",
    "    \n",
    "    for metric in found_metrics:\n",
    "        if metric in sample:\n",
    "            value = sample[metric]\n",
    "            print(f\"  {metric}: {value:.4f}\" if pd.notna(value) else f\"  {metric}: NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 3 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 3 SUMMARY: Alignment Integration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test3_passed = len(alignment_results) > 0 and len(found_metrics) >= 4\n",
    "\n",
    "if test3_passed:\n",
    "    print(\"\\nâœ“ TEST 3 PASSED: Integration with alignment analysis works!\")\n",
    "    print(\"\\nPreprocessed files are fully compatible with alignment analysis.\")\n",
    "    print(f\"Successfully analyzed {len(alignment_results)} utterance pairs.\")\n",
    "else:\n",
    "    print(\"\\nâœ— TEST 3 FAILED: Integration issues detected.\")\n",
    "    print(\"Please review the test results above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TEST 4: Check Output Files\n",
    "\n",
    "Verify that the saved output files on disk are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 4: Output Files on Disk\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check basic output directory\n",
    "print(f\"\\nBasic output directory: {OUTPUT_DIR_BASIC}\")\n",
    "basic_files = [f for f in os.listdir(OUTPUT_DIR_BASIC) if f.endswith('.txt')]\n",
    "print(f\"Files created: {len(basic_files)}\")\n",
    "for f in basic_files:\n",
    "    size_kb = os.path.getsize(os.path.join(OUTPUT_DIR_BASIC, f)) / 1024\n",
    "    print(f\"  - {f} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify a saved file\n",
    "if basic_files:\n",
    "    test_file = os.path.join(OUTPUT_DIR_BASIC, basic_files[0])\n",
    "    print(f\"\\nVerifying saved file: {basic_files[0]}\")\n",
    "    \n",
    "    # Load from disk\n",
    "    saved_df = pd.read_csv(test_file, sep='\\t', encoding='utf-8')\n",
    "    print(f\"âœ“ Loaded {len(saved_df)} rows from disk\")\n",
    "    \n",
    "    # Quick format check\n",
    "    token_str = saved_df['token'].iloc[0]\n",
    "    print(f\"\\nToken column type: {type(token_str)}\")\n",
    "    print(f\"Token value: {token_str[:80]}...\")\n",
    "    \n",
    "    # Parse check\n",
    "    try:\n",
    "        token_list = ast.literal_eval(token_str)\n",
    "        print(f\"âœ“ Successfully parsed to: {type(token_list).__name__}\")\n",
    "        print(f\"  Contents: {token_list}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Parse failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Summary\n",
    "\n",
    "Overall test results and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect results\n",
    "test_results = {\n",
    "    \"TEST 1: Basic Preprocessing (NLTK)\": test1_passed,\n",
    "    \"TEST 2: spaCy Integration\": test2_passed if test2_passed is not None else \"SKIPPED\",\n",
    "    \"TEST 3: Alignment Integration\": test3_passed\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for test_name, result in test_results.items():\n",
    "    if result == \"SKIPPED\":\n",
    "        print(f\"âŠ˜ {test_name}: SKIPPED\")\n",
    "    elif result:\n",
    "        print(f\"âœ“ {test_name}: PASSED\")\n",
    "    else:\n",
    "        print(f\"âœ— {test_name}: FAILED\")\n",
    "\n",
    "# Overall assessment\n",
    "passed_tests = [r for r in test_results.values() if r is True]\n",
    "failed_tests = [r for r in test_results.values() if r is False]\n",
    "\n",
    "print(f\"\\nResults: {len(passed_tests)} passed, {len(failed_tests)} failed\")\n",
    "\n",
    "if len(failed_tests) == 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ‰ ALL TESTS PASSED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nThe refactored prepare_transcripts.py is working correctly!\")\n",
    "    print(\"\\nYou can now:\")\n",
    "    print(\"  1. Use prepare_transcripts with your own data\")\n",
    "    print(\"  2. Run alignment analysis on preprocessed output\")\n",
    "    print(\"  3. Generate baseline comparisons with surrogates\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âš ï¸  SOME TESTS FAILED\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nPlease review the failed tests above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Quick Preprocessing Example\n",
    "\n",
    "Once tests pass, here's how to preprocess your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Preprocess with spaCy (recommended)\n",
    "# Uncomment and modify paths for your own data\n",
    "\n",
    "# from align_test.prepare_transcripts_refactored import prepare_transcripts\n",
    "\n",
    "# my_results = prepare_transcripts(\n",
    "#     input_files=\"/path/to/my/raw/transcripts\",\n",
    "#     output_file_directory=\"/path/to/my/preprocessed/output\",\n",
    "#     run_spell_check=True,\n",
    "#     minwords=2,\n",
    "#     add_stanford_tags=True,\n",
    "#     stanford_tagger_type='spacy',  # Recommended: fast and accurate\n",
    "#     save_concatenated_dataframe=True\n",
    "# )\n",
    "\n",
    "# print(f\"Preprocessed {len(my_results)} utterances!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Full Pipeline Example\n",
    "\n",
    "Complete workflow from raw data to alignment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete pipeline\n",
    "# Uncomment to run on your own data\n",
    "\n",
    "# # Step 1: Preprocess\n",
    "# preprocessed = prepare_transcripts(\n",
    "#     input_files=\"./my_raw_data\",\n",
    "#     output_file_directory=\"./my_preprocessed\",\n",
    "#     add_stanford_tags=True,\n",
    "#     stanford_tagger_type='spacy'\n",
    "# )\n",
    "\n",
    "# # Step 2: Analyze alignment\n",
    "# from align_test.alignment import LinguisticAlignment\n",
    "\n",
    "# analyzer = LinguisticAlignment(alignment_types=[\"lexsyn\", \"fasttext\"])\n",
    "# results = analyzer.analyze_folder(\n",
    "#     folder_path=\"./my_preprocessed\",\n",
    "#     output_directory=\"./my_results\",\n",
    "#     lag=1,\n",
    "#     max_ngram=2,\n",
    "#     add_stanford_tags=True  # Use spaCy tags from preprocessing\n",
    "# )\n",
    "\n",
    "# # Step 3: Generate baseline\n",
    "# baseline = analyzer.analyze_baseline(\n",
    "#     input_files=\"./my_preprocessed\",\n",
    "#     output_directory=\"./my_results\",\n",
    "#     lag=1,\n",
    "#     max_ngram=2,\n",
    "#     add_stanford_tags=True,\n",
    "#     id_separator=\"-\",\n",
    "#     condition_label=\"cond\",\n",
    "#     dyad_label=\"dyad\"\n",
    "# )\n",
    "\n",
    "# print(\"Complete pipeline finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
